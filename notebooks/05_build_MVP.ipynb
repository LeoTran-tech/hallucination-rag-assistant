{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d200c481",
   "metadata": {},
   "source": [
    "## This notebooks contains the code to build the MVP of our RAG assistant. It includes:\n",
    "- A search function that queries the ChromaDB collection for relevant chunks based on a question\n",
    "- A function to build a context string from the search hits, which will be used as input to the LLM\n",
    "- A function to call the local Ollama LLM with a prompt and get an answer\n",
    "The final part of the notebook calls the answer function with a sample question and prints the answer.\n",
    "We also have a compare_papers function that takes a question and a list of paper IDs, retrieves relevant chunks\n",
    "for each paper, builds a context, and prompts the LLM to compare the papers with respect to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3500d5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install pymupdf pandas tqdm tiktoken\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8774c651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\conda_envs\\hallucination-rag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 765.12it/s, Materializing param=pooler.dense.weight]                               \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: BAAI/bge-base-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Use the BGE model from BAAI, which is a strong open-source embedding model \n",
    "# that converts text into vector embeddings. These embeddings can be used for\n",
    "# tasks like semantic search, etc. We will use this model later to convert\n",
    "# our text chunks into embeddings\n",
    "embed_model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "model = SentenceTransformer(embed_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db399fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "CHROMA_DIR = DATA_DIR / \"chroma_db\"\n",
    "client = chromadb.PersistentClient(path=str(CHROMA_DIR), settings=Settings(anonymized_telemetry=False))\n",
    "\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"hallucination_faithfulness_chunks\",\n",
    "    metadata={\"embedding_model\": embed_model_name}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "149ba2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, k=5, where=None, *, return_text_preview_chars=700):\n",
    "    q_emb = model.encode([query], normalize_embeddings=True).tolist()\n",
    "\n",
    "    res = collection.query(\n",
    "        query_embeddings=q_emb,\n",
    "        n_results=k,\n",
    "        where=where\n",
    "    )\n",
    "\n",
    "    hits = []\n",
    "    n = min(k, len(res[\"ids\"][0]))\n",
    "\n",
    "    for i in range(n):\n",
    "        meta = res[\"metadatas\"][0][i] or {}\n",
    "        doc  = res[\"documents\"][0][i] or \"\"\n",
    "        dist = res[\"distances\"][0][i]\n",
    "\n",
    "        hits.append({\n",
    "            \"rank\": i + 1,\n",
    "            \"distance\": float(dist),\n",
    "            \"paper_id\": meta.get(\"paper_id\"),\n",
    "            \"year\": meta.get(\"year\"),\n",
    "            \"page\": meta.get(\"page\"),\n",
    "            \"title\": meta.get(\"title\", \"\"),\n",
    "            \"source_file\": meta.get(\"source_file\", \"\"),\n",
    "            \"text\": doc,\n",
    "            \"text_preview\": doc[:return_text_preview_chars].strip()\n",
    "        })\n",
    "\n",
    "    return hits\n",
    "\n",
    "def build_context(hits, max_chars=6000):\n",
    "    blocks = []\n",
    "    total = 0\n",
    "    for h in hits:\n",
    "        block = f\"[{h['paper_id']} p.{h['page']}] {h['title']}\\n{h['text'].strip()}\\n\"\n",
    "        if total + len(block) > max_chars:\n",
    "            break\n",
    "        blocks.append(block)\n",
    "        total += len(block)\n",
    "    return \"\\n\\n\".join(blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5e14e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "OLLAMA_MODEL = \"qwen2.5:7b-instruct\"\n",
    "\n",
    "def call_llm(prompt: str, model: str = OLLAMA_MODEL, temperature: float = 0.2, max_tokens: int = 700) -> str:\n",
    "    \"\"\"\n",
    "    Call local Ollama model. No API key. Free (runs on your machine).\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"num_predict\": max_tokens,\n",
    "        }\n",
    "    }\n",
    "    r = requests.post(OLLAMA_URL, json=payload, timeout=300)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    return data.get(\"response\", \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f36d82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_citations(hits, top_n=5):\n",
    "    # tạo citations gọn: [paper_id p.page] file\n",
    "    cits = []\n",
    "    for h in hits[:top_n]:\n",
    "        cits.append({\n",
    "            \"paper_id\": h.get(\"paper_id\"),\n",
    "            \"page\": h.get(\"page\"),\n",
    "            \"title\": h.get(\"title\"),\n",
    "            \"source_file\": h.get(\"source_file\"),\n",
    "            \"distance\": h.get(\"distance\"),\n",
    "        })\n",
    "    return cits\n",
    "\n",
    "\n",
    "def answer(question, *, k=8, where=None, max_ctx_chars=8000):\n",
    "    hits = search(question, k=k, where=where)\n",
    "    context = build_context(hits, max_chars=max_ctx_chars)  # nếu hàm bạn có param khác thì sửa tên\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a research assistant specialized in LLM hallucination & faithfulness.\n",
    "Answer the QUESTION using ONLY the CONTEXT.\n",
    "If the context is insufficient, say \"I don't know from the provided papers.\"\n",
    "\n",
    "You must cite sources inline like: [paper_id p.page]\n",
    "At the end, output a bullet list \"Sources\" with unique citations.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    llm_text = call_llm(prompt)\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"answer\": llm_text,\n",
    "        \"hits\": hits,\n",
    "        \"citations\": make_citations(hits, top_n=5),\n",
    "        \"context_chars\": len(context),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a2f38d",
   "metadata": {},
   "source": [
    "## The same query is used twice to compare the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421db4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness or groundedness is defined as a multi-party property where a faithful explanation enables a listener model to come to the same conclusion as the speaker without access to the speaker’s answer. Specifically, faithfulness is framed in terms of reasoning executability: a reasoning chain is considered faithful if it can be executed by a similarly capable listener to recover the same conclusion, without access to the speaker's final answer.\n",
      "\n",
      "Sources:\n",
      "- [2602.16154v1 p.3]\n",
      "- [2602.16154v1 p.9]\n"
     ]
    }
   ],
   "source": [
    "res = answer(\"How do papers define faithfulness or groundedness?\", k=10)\n",
    "print(res[\"answer\"])\n",
    "\n",
    "# 2m16s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24bbb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers define faithfulness or groundedness as follows:\n",
      "\n",
      "- Faithfulness is a multi-party property where a faithful explanation enables a listener model to come to the same conclusion as the speaker without access to the speaker’s answer. [2602.16154v1 p.1]\n",
      "- Faithfulness can be measured by training models to produce reasoning traces that other models can effectively execute across multiple truncation points, encouraging the generation of responses executable by listener models using soft execution. [2602.16154v1 p.3]\n",
      "\n",
      "Sources:\n",
      "- [2602.16154v1 p.1]\n",
      "- [2602.16154v1 p.3]\n"
     ]
    }
   ],
   "source": [
    "res = answer(\"How do papers define faithfulness or groundedness?\", k=10)\n",
    "print(res[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e29337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_papers(question, paper_ids, *, k_per_paper=4):\n",
    "    all_hits = []\n",
    "    for pid in paper_ids:\n",
    "        hits = search(question, k=k_per_paper, where={\"paper_id\": pid})\n",
    "        all_hits.extend(hits)\n",
    "\n",
    "    # sort theo distance \n",
    "    all_hits = sorted(all_hits, key=lambda x: x[\"distance\"])\n",
    "    context = build_context(all_hits, max_chars=9000)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Compare the papers with respect to the QUESTION.\n",
    "Use ONLY the CONTEXT. Cite inline [paper_id p.page].\n",
    "Output:\n",
    "- Summary table (paper_id -> key points)\n",
    "- Agreement / disagreement\n",
    "- Practical takeaway\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    llm_text = call_llm(prompt)\n",
    "    return {\"question\": question, \"paper_ids\": paper_ids, \"answer\": llm_text, \"hits\": all_hits}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58aabf2",
   "metadata": {},
   "source": [
    "## Compare the 2 papers with a question twice to compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e2f651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Summary Table\n",
      "\n",
      "| Paper ID | Key Points |\n",
      "| --- | --- |\n",
      "| 2602.16154v1 p.3 | - Defines faithfulness as the degree to which a listener model reaches the same answer as the original speaker reasoning model.<br>- Uses multi-listener soft execution for training, where truncated reasoning chains are provided to multiple listeners to compute matching rewards. |\n",
      "| 2602.16154v1 p.6 | - Evaluates faithfulness using AOC metrics across different methods and shows REMUL balances faithfulness and correctness.<br>- Faithfulness-only training improves AOC, while hint-optimized models perform poorly. |\n",
      "| 2602.16154v1 p.1 | - Argues that faithfulness is a multi-party property enabling listeners to reach the same conclusion as speakers without access to answers.<br>- Proposes REMUL for balancing faithfulness and correctness. |\n",
      "\n",
      "### Agreement / Disagreement\n",
      "\n",
      "- **Agreement**: Both papers agree on defining faithfulness in terms of listener models reaching the same conclusions as speaker models.\n",
      "- **Disagreement**: The second paper (2602.16154v1 p.6) provides a more detailed evaluation and comparison of different training methods, showing that REMUL balances both faithfulness and correctness better than other approaches.\n",
      "\n",
      "### Practical Takeaway\n",
      "\n",
      "- **Faithfulness Definition**: Faithfulness should be understood as the ability of listener models to reach the same conclusion as speaker models without access to their answers.\n",
      "- **Training Methodology**: Multi-listener soft execution (REMUL) is effective in balancing faithfulness and correctness, providing a robust approach for training reasoning models. This method ensures that models are not only correct but also faithful to the intended reasoning process.\n",
      "\n",
      "By adopting REMUL or similar methodologies, developers can ensure that AI systems provide both accurate and interpretable outputs, enhancing user trust and understanding.\n"
     ]
    }
   ],
   "source": [
    "cmp_res = compare_papers(\n",
    "    \"How is faithfulness defined and measured?\",\n",
    "    paper_ids=[\"2602.16154v1\", \"2602.14529v1\"]\n",
    ")\n",
    "print(cmp_res[\"answer\"])\n",
    "\n",
    "# 4m55s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4869c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Summary Table\n",
      "\n",
      "| Paper ID | Key Points |\n",
      "| --- | --- |\n",
      "| 2602.16154v1 p.3 | Faithfulness is defined as the degree to which a listener model reaches the same answer as the original speaker reasoning model. The faithfulness reward is computed by comparing the answers of multiple listeners with the original answer. |\n",
      "| 2602.16154v1 p.6 | REMUL trains models for both faithfulness and correctness simultaneously, using truncated CoT answering and adding mistakes to measure faithfulness. Faithfulness-only training improves AOC metrics, while hint-optimized models perform worse on these metrics. |\n",
      "| 2602.16154v1 p.1 | Faithfulness is a multi-party property where the speaker's reasoning must be executable by a listener without access to the answer. REMUL uses soft execution and multiple listeners for training. |\n",
      "\n",
      "### Agreement / Disagreement\n",
      "\n",
      "- **Agreement**: Both papers agree that faithfulness involves ensuring the listener model reaches the same conclusion as the speaker, even when only part of the reasoning chain is provided.\n",
      "- **Disagreement**: The first paper (2602.16154v1 p.3) provides a more detailed definition and computation method for faithfulness, while the second paper (2602.16154v1 p.6) focuses on the metrics used to measure it.\n",
      "\n",
      "### Practical Takeaway\n",
      "\n",
      "- **Definition of Faithfulness**: Faithfulness is crucial in ensuring that AI models provide explanations or reasoning chains that are understandable and executable by other models, even when parts of the chain are truncated.\n",
      "- **Training Methods**: Training methods like REMUL (2602.16154v1 p.3) can balance faithfulness with correctness, while metrics like AOC and hint usage help in measuring these properties effectively.\n",
      "\n",
      "By focusing on both the definition and practical training methods for faithfulness, researchers can ensure that AI models provide clear and executable reasoning chains, enhancing their overall utility and reliability.\n"
     ]
    }
   ],
   "source": [
    "cmp_res = compare_papers(\n",
    "    \"How is faithfulness defined and measured?\",\n",
    "    paper_ids=[\"2602.16154v1\", \"2602.14529v1\"]\n",
    ")\n",
    "print(cmp_res[\"answer\"])\n",
    "\n",
    "# 1m16.4s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
