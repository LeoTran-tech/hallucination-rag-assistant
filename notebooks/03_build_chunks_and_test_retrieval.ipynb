{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35f9f300",
   "metadata": {},
   "source": [
    "## I. Build “chunk dataset” (PDF → chunks.jsonl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fb6d013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>first_author</th>\n",
       "      <th>authors</th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "      <th>categories</th>\n",
       "      <th>summary</th>\n",
       "      <th>pdf_url</th>\n",
       "      <th>arxiv_url</th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2602.16704v1</td>\n",
       "      <td>Reinforced Fast Weights with Next-Sequence Pre...</td>\n",
       "      <td>2026</td>\n",
       "      <td>Hee Seung Hwang</td>\n",
       "      <td>Hee Seung Hwang, Xindi Wu, Sanghyuk Chun, Olga...</td>\n",
       "      <td>2026-02-18T18:53:18+00:00</td>\n",
       "      <td>2026-02-18T18:53:18+00:00</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>Fast weight architectures offer a promising al...</td>\n",
       "      <td>https://arxiv.org/pdf/2602.16704v1</td>\n",
       "      <td>http://arxiv.org/abs/2602.16704v1</td>\n",
       "      <td>2026_hee-seung-hwang_reinforced-fast-weights-w...</td>\n",
       "      <td>data\\raw\\papers\\2026_hee-seung-hwang_reinforce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2602.16671v1</td>\n",
       "      <td>SPARC: Scenario Planning and Reasoning for Aut...</td>\n",
       "      <td>2026</td>\n",
       "      <td>Jaid Monwar Chowdhury</td>\n",
       "      <td>Jaid Monwar Chowdhury, Chi-An Fu, Reyhaneh Jab...</td>\n",
       "      <td>2026-02-18T18:09:03+00:00</td>\n",
       "      <td>2026-02-18T18:09:03+00:00</td>\n",
       "      <td>cs.SE, cs.AI</td>\n",
       "      <td>Automated unit test generation for C remains a...</td>\n",
       "      <td>https://arxiv.org/pdf/2602.16671v1</td>\n",
       "      <td>http://arxiv.org/abs/2602.16671v1</td>\n",
       "      <td>2026_jaid-monwar-chowdhury_sparc-scenario-plan...</td>\n",
       "      <td>data\\raw\\papers\\2026_jaid-monwar-chowdhury_spa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       paper_id                                              title  year  \\\n",
       "0  2602.16704v1  Reinforced Fast Weights with Next-Sequence Pre...  2026   \n",
       "1  2602.16671v1  SPARC: Scenario Planning and Reasoning for Aut...  2026   \n",
       "\n",
       "            first_author                                            authors  \\\n",
       "0        Hee Seung Hwang  Hee Seung Hwang, Xindi Wu, Sanghyuk Chun, Olga...   \n",
       "1  Jaid Monwar Chowdhury  Jaid Monwar Chowdhury, Chi-An Fu, Reyhaneh Jab...   \n",
       "\n",
       "                   published                    updated    categories  \\\n",
       "0  2026-02-18T18:53:18+00:00  2026-02-18T18:53:18+00:00         cs.CL   \n",
       "1  2026-02-18T18:09:03+00:00  2026-02-18T18:09:03+00:00  cs.SE, cs.AI   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Fast weight architectures offer a promising al...   \n",
       "1  Automated unit test generation for C remains a...   \n",
       "\n",
       "                              pdf_url                          arxiv_url  \\\n",
       "0  https://arxiv.org/pdf/2602.16704v1  http://arxiv.org/abs/2602.16704v1   \n",
       "1  https://arxiv.org/pdf/2602.16671v1  http://arxiv.org/abs/2602.16671v1   \n",
       "\n",
       "                                           file_name  \\\n",
       "0  2026_hee-seung-hwang_reinforced-fast-weights-w...   \n",
       "1  2026_jaid-monwar-chowdhury_sparc-scenario-plan...   \n",
       "\n",
       "                                           file_path  \n",
       "0  data\\raw\\papers\\2026_hee-seung-hwang_reinforce...  \n",
       "1  data\\raw\\papers\\2026_jaid-monwar-chowdhury_spa...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip -q install pymupdf pandas tqdm tiktoken\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import fitz # fitz is the PyMuPDF library for PDF processing\n",
    "import json\n",
    "import re\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "\n",
    "PDF_DIR = Path(\"../data/raw/papers\")\n",
    "META_CSV = Path(\"../data/metadata/papers.csv\")\n",
    "\n",
    "# create the output directory if not exists\n",
    "OUT_DIR = Path(\"../data/processed\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(META_CSV)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab93f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block defines funcitons to clean text and extract text from\n",
    "# PDF files\n",
    "\n",
    "# Function takes a string and performs several cleaning operations\n",
    "def clean_text(s: str) -> str:\n",
    "    # replace null character by space\n",
    "    s = s.replace(\"\\x00\", \" \")\n",
    "\n",
    "    # delete whitespace before \\n, e.g. \"word \\n\" -> \"word\\n\"\n",
    "    s = re.sub(r\"\\s+\\n\", \"\\n\", s)\n",
    "\n",
    "    # if there are 3+ empty lines, replace them with 2 empty lines\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "\n",
    "    # replace long whitespace with a single whitespace\n",
    "    s = re.sub(r\"[ \\t]{2,}\", \" \", s)\n",
    "\n",
    "    # remove beginning and ending whitespaces\n",
    "    return s.strip()\n",
    "\n",
    "# Function to read a PDF and return the cleaned text of each page\n",
    "def pdf_pages_text(pdf_path: Path):\n",
    "    doc = fitz.open(pdf_path) # read the pdf file \n",
    "    for i, page in enumerate(doc):\n",
    "\n",
    "        # retrieve plain text from the page\n",
    "        text = page.get_text(\"text\")\n",
    "\n",
    "        # return page number starting from 1 and clean the text\n",
    "        yield (i+1, clean_text(text)) # yeild is a generator\n",
    "    doc.close()\n",
    "\n",
    "import string\n",
    "\n",
    "import re\n",
    "\n",
    "REF_HDR = re.compile(r\"^\\s*(references|bibliography)\\s*$\", re.IGNORECASE | re.MULTILINE)\n",
    "\n",
    "# Function to check if a page is low signal based on heuristics\n",
    "def looks_like_prompt_block(text: str) -> bool:\n",
    "    t = text.lower()\n",
    "    patterns = [\n",
    "        \"site:github.com\", \"output format\", \"query constraints\",\n",
    "        \"follow initial rules\", \"================\", \"strict\",\n",
    "        \"do not\", \"must:\"\n",
    "    ]\n",
    "    hits = sum(p in t for p in patterns)\n",
    "    return hits >= 2\n",
    "\n",
    "# Function to check if a page is a valid chunk of text based on heuristics\n",
    "def is_valid_chunk(text: str, min_chars=200):\n",
    "    if len(text) < min_chars:\n",
    "        return False\n",
    "\n",
    "    letters = sum(c.isalpha() for c in text)\n",
    "    digits  = sum(c.isdigit() for c in text)\n",
    "    if letters / max(len(text), 1) < 0.45:\n",
    "        return False\n",
    "    if digits / max(len(text), 1) > 0.35:\n",
    "        return False\n",
    "    if looks_like_prompt_block(text):\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# Function to check if a page looks like a prompt based on heuristics\n",
    "def looks_like_prompt(text: str) -> bool:\n",
    "    t = text.lower()\n",
    "    bad = [\"site:github.com\", \"output format\", \"query constraints\", \"follow initial rules\", \"do not\", \"must:\"]\n",
    "    hits = sum(b in t for b in bad)\n",
    "    return hits >= 2\n",
    "\n",
    "# Function to check if a page is low signal, which means it likely\n",
    "# contains acknowledgements or impact statements rather than main content\n",
    "def is_low_signal_page(page_text: str) -> bool:\n",
    "    head = page_text[:1200].lower()\n",
    "    return (\"impact statement\" in head) or (\"acknowledgements\" in head)\n",
    "\n",
    "import re\n",
    "\n",
    "REF_HEADER = re.compile(r\"^\\s*(references|bibliography)\\s*$\", re.IGNORECASE | re.MULTILINE)\n",
    "\n",
    "# Function to check if a page is a references page based on heuristics\n",
    "\n",
    "def is_references_page(page_text: str) -> bool:\n",
    "    # check early part of page for header\n",
    "    head = page_text[:1200].lower()\n",
    "    return \"references\" in head.splitlines()[:20] or bool(REF_HEADER.search(page_text[:1200]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612bd919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken # a tokenizer library from OpenAI\n",
    "\n",
    "# tokenizer for gpt-4, gpt-4o\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Function to chunk text into pieces of a certain token size \n",
    "# with some overlap\n",
    "def chunk_by_tokens(text: str, chunk_size=450, overlap=80):\n",
    "    \n",
    "    # encode the text into tokens\n",
    "    tokens = enc.encode(\n",
    "        text,\n",
    "        disallowed_special=()  # allow all special tokens as normal text\n",
    "    )\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    # loop to create chunks of tokens with specified size and overlap\n",
    "    while start < len(tokens):\n",
    "        end = min(start + chunk_size, len(tokens))\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        chunk_text = enc.decode(chunk_tokens).strip()\n",
    "        if chunk_text:\n",
    "            chunks.append(chunk_text)\n",
    "        if end == len(tokens):\n",
    "            break\n",
    "        start = max(0, end - overlap)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afefdbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:19<00:00,  6.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote chunks: 6319\n",
      "Saved: ..\\data\\processed\\chunks.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define where the output JSONL file will be saved\n",
    "OUT_JSONL = OUT_DIR / \"chunks.jsonl\"\n",
    "# JSONL (JSON Lines) is chosen instead of JSON because:\n",
    "# - with JSONL, it's not required to load all data into RAM\n",
    "# - easier to add a new chunk, while with JSON, you need to \n",
    "# load the whole file, add the chunk, then write back the whole file\n",
    "# - JSONL is more robust to errors, e.g. if the process is interrupted \n",
    "# in the middle of writing, the already written lines are still valid JSON\n",
    "# objects, while with JSON, the whole file would be corrupted\n",
    "# JSON is better for hierarchical data, while JSONL is better for flat data \n",
    "# like our chunks.\n",
    "\n",
    "written = 0\n",
    "with OUT_JSONL.open(\"w\", encoding=\"utf-8\") as f:\n",
    "\n",
    "    # Loop through each papers in metada/papers.csv (120 in total)\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "\n",
    "        # For each paper, read the corresponding PDF file\n",
    "        pdf_path = PDF_DIR / row[\"file_name\"]\n",
    "        if not pdf_path.exists():\n",
    "            continue\n",
    "        \n",
    "        # Extract metadata\n",
    "        paper_id = row[\"paper_id\"]\n",
    "        title = row.get(\"title\", \"\")\n",
    "        year = int(row.get(\"year\", 0))\n",
    "        source_file = pdf_path.name\n",
    "\n",
    "        # For each page in the PDF\n",
    "        for page_num, page_text in pdf_pages_text(pdf_path):\n",
    "            if is_low_signal_page(page_text):\n",
    "                continue\n",
    "            if is_references_page(page_text):\n",
    "                continue\n",
    "\n",
    "            if len(page_text) < 200:  # skip tiny pages\n",
    "                continue\n",
    "\n",
    "            # Split the page text into chunks of ~450 tokens\n",
    "            # Each chunk will repeat the last 80 tokens of the previous\n",
    "            # chunk, to help maintain context across chunks\n",
    "            # page_text is the cleaned text of the page, which is a string,\n",
    "            # this means overlap only applies to the text within a page, not across pages\n",
    "            page_chunks = chunk_by_tokens(page_text, chunk_size=450, overlap=80)\n",
    "\n",
    "            # For each chunk\n",
    "            for j, ch in enumerate(page_chunks):\n",
    "                if not is_valid_chunk(ch):\n",
    "                    continue\n",
    "\n",
    "                # Create a record, which is a dictionary containing the chunk \n",
    "                # text and its metadata\n",
    "                chunk_id = f\"{paper_id}_p{page_num:02d}_c{j:03d}\"\n",
    "\n",
    "                if looks_like_prompt(ch):\n",
    "                    continue\n",
    "\n",
    "                rec = {\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"paper_id\": paper_id,\n",
    "                    \"title\": title,\n",
    "                    \"year\": year,\n",
    "                    \"page\": page_num,\n",
    "                    \"text\": ch,\n",
    "                    \"source_file\": source_file,\n",
    "                }\n",
    "\n",
    "                # Write the record as a JSON line in the output file\n",
    "                f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "                written += 1\n",
    "\n",
    "print(\"Wrote chunks:\", written)\n",
    "print(\"Saved:\", OUT_JSONL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56009a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2602.15909v1_p14_c001 14\n",
      "amil W´ojcicki, and Benjamin Shannon. The importance of phase in speech\n",
      "enhancement. speech communication, 53(4):465–494, 2011.\n",
      "Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and\n",
      "Quoc V Le. Specaugment: A simple data augmentation method for automatic speech recognition.\n",
      "arXiv preprint arXiv:1904.08779, 2019.\n",
      "William Peebles and Saining Xie. Scalable diffusion m \n",
      "---\n",
      "\n",
      "2602.16660v1_p04_c000 4\n",
      "Published as a conference paper at ICLR 2026\n",
      "representations (c.f., Section 3.1). The overall framework, including the linear extractor used to ob-\n",
      "tain representations from hidden states, as well as the joint optimization objective, is subsequently\n",
      "elucidated in Section 3.2.\n",
      "3.1\n",
      "REGULATING MULTILINGUAL CONSISTENCY WITH SINGULAR ANALYSIS\n",
      "Representation consistency in queries. Recent studies have d \n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show 2 random chunks\n",
    "import random, itertools\n",
    "lines = OUT_JSONL.read_text(encoding=\"utf-8\").splitlines()\n",
    "for line in random.sample(lines, 2):\n",
    "    obj = json.loads(line)\n",
    "    print(obj[\"chunk_id\"], obj[\"page\"])\n",
    "    print(obj[\"text\"][:400], \"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95e3143e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 6319\n",
      "Digit-heavy (>30% digits): 3 (0.05%)\n",
      "Letter-light (<50% letters): 143 (2.26%)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "CHUNKS = Path(\"../data/processed/chunks.jsonl\")\n",
    "\n",
    "def noise_score(text: str):\n",
    "    n = max(len(text), 1)\n",
    "    letters = sum(c.isalpha() for c in text) / n\n",
    "    digits  = sum(c.isdigit() for c in text) / n\n",
    "    return letters, digits\n",
    "\n",
    "total = 0\n",
    "digit_heavy = 0\n",
    "letter_light = 0\n",
    "\n",
    "with CHUNKS.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        total += 1\n",
    "        obj = json.loads(line)\n",
    "        letters, digits = noise_score(obj[\"text\"])\n",
    "        if digits > 0.30:      # nhiều số\n",
    "            digit_heavy += 1\n",
    "        if letters < 0.50:     # ít chữ\n",
    "            letter_light += 1\n",
    "\n",
    "print(\"Total chunks:\", total)\n",
    "print(\"Digit-heavy (>30% digits):\", digit_heavy, f\"({digit_heavy/total:.2%})\")\n",
    "print(\"Letter-light (<50% letters):\", letter_light, f\"({letter_light/total:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "effff215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks: 6319\n",
      "min/median/p90/max: 202 1488 2096 3035\n",
      "too short (<200 chars): 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "CHUNKS = Path(\"../data/processed/chunks.jsonl\")\n",
    "lens = []\n",
    "\n",
    "with CHUNKS.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        lens.append(len(obj[\"text\"]))\n",
    "\n",
    "arr = np.array(lens)\n",
    "print(\"chunks:\", len(arr))\n",
    "print(\"min/median/p90/max:\", arr.min(), int(np.median(arr)), int(np.percentile(arr, 90)), arr.max())\n",
    "print(\"too short (<200 chars):\", (arr < 200).sum(), f\"({(arr<200).mean():.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60f749d",
   "metadata": {},
   "source": [
    "## II. Build Vector Index (Chroma) + test retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f361e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install chromadb sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a3c7665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6319,\n",
       " dict_keys(['chunk_id', 'paper_id', 'title', 'year', 'page', 'text', 'source_file']))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "CHUNKS_PATH = Path(\"../data/processed/chunks.jsonl\")\n",
    "\n",
    "# Function to load a JSONL file and return a list of JSON objects\n",
    "def load_jsonl(path, limit=None):\n",
    "\n",
    "    # Define an empty list to store the JSON objects\n",
    "    items = []\n",
    "\n",
    "    # Open the JSONL file and read it line by line\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "\n",
    "            # Parse the line as a JSON object and append it\n",
    "            items.append(json.loads(line))\n",
    "\n",
    "            # If a limit is specified, stop after reading that many lines\n",
    "            if limit and (i+1) >= limit:\n",
    "                break\n",
    "    return items\n",
    "\n",
    "chunks = load_jsonl(CHUNKS_PATH, limit=20000)  # test 20k chunks\n",
    "len(chunks), chunks[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ccf7635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\conda_envs\\hallucination-rag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 886.68it/s, Materializing param=pooler.dense.weight]                               \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: BAAI/bge-base-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Use the BGE model from BAAI, which is a strong open-source embedding model \n",
    "# that converts text into vector embeddings. These embeddings can be used for\n",
    "# tasks like semantic search, etc. We will use this model later to convert\n",
    "# our text chunks into embeddings\n",
    "embed_model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "model = SentenceTransformer(embed_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c622944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "CHROMA_DIR = DATA_DIR / \"chroma_db\"\n",
    "client = chromadb.PersistentClient(path=str(CHROMA_DIR), settings=Settings(anonymized_telemetry=False))\n",
    "\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"hallucination_faithfulness_chunks\",\n",
    "    metadata={\"embedding_model\": embed_model_name}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bba61ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [41:01<00:00, 98.46s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted: 6319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Function that takes an iterable and yields it in batches of size n\n",
    "def batch(iterable, n=256):\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i:i+n]\n",
    "\n",
    "# Prepare the data for upserting into ChromaDB. We need three lists:\n",
    "# - docs: the text of each chunk\n",
    "# - ids: a unique ID for each chunk, which we will use the \"chunk_id\"\n",
    "# - metas: a dictionary of metadata for each chunk\n",
    "docs = [c[\"text\"] for c in chunks]\n",
    "ids = [c[\"chunk_id\"] for c in chunks]\n",
    "metas = [\n",
    "    {\n",
    "        \"paper_id\": c[\"paper_id\"],\n",
    "        \"title\": c.get(\"title\", \"\"),\n",
    "        \"year\": int(c.get(\"year\", 0)),\n",
    "        \"page\": int(c.get(\"page\", 0)),\n",
    "        \"source_file\": c.get(\"source_file\", \"\")\n",
    "    }\n",
    "    for c in chunks\n",
    "]\n",
    "\n",
    "# Upsert in batches\n",
    "for b_docs, b_ids, b_metas in tqdm(list(zip(batch(docs), batch(ids), batch(metas))), total=(len(docs)+255)//256):\n",
    "    emb = model.encode(b_docs, normalize_embeddings=True).tolist()\n",
    "    collection.upsert(\n",
    "        ids=b_ids,\n",
    "        documents=b_docs,\n",
    "        metadatas=b_metas,\n",
    "        embeddings=emb\n",
    "    )\n",
    "\n",
    "print(\"Inserted:\", collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a31b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform a search query on the ChromaDB collection.\n",
    "def search(query, k=5, where=None):\n",
    "    \n",
    "    # Encode the query into an embedding\n",
    "    q_emb = model.encode([query], normalize_embeddings=True).tolist()\n",
    "    \n",
    "    # Query the collection for the top k most similar chunks to the \n",
    "    # query embedding,\n",
    "    res = collection.query(\n",
    "        query_embeddings=q_emb,\n",
    "        n_results=k,\n",
    "        where=where\n",
    "    )\n",
    "\n",
    "    # Loop through the search results and print the metadata and \n",
    "    # text of each chunk\n",
    "    for i in range(min(k, len(res[\"ids\"][0]))):\n",
    "        meta = res[\"metadatas\"][0][i]\n",
    "        doc  = res[\"documents\"][0][i]\n",
    "        dist = res[\"distances\"][0][i]\n",
    "\n",
    "        print(\"\\n\" + \"=\"*90)\n",
    "        print(f\"#{i+1} | dist={dist:.4f} | paper_id={meta.get('paper_id')} | year={meta.get('year')} | page={meta.get('page')}\")\n",
    "        title = meta.get(\"title\", \"\")\n",
    "        if title:\n",
    "            print(\"TITLE:\", title[:140])\n",
    "        print(\"FILE:\", meta.get(\"source_file\", \"\"))\n",
    "        print(\"-\"*90)\n",
    "        print(doc[:700].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b06c82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "##############################\n",
      "QUERY: What is hallucination in large language models?\n",
      "\n",
      "==========================================================================================\n",
      "#1 | dist=0.3085 | paper_id=2602.14259v1 | year=2026 | page=7\n",
      "TITLE: Detecting LLM Hallucinations via Embedding Cluster Geometry: A Three-Type Taxonomy with Measurable Signatures\n",
      "FILE: 2026_matic-korun_detecting-llm-hallucinations-via-embedding-cluster-geometry-_2602.142591.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "prerequisites, but hallucination detection in prac-\n",
      "tice operates on contextual hidden states, which\n",
      "may exhibit different geometry. Second, the cross-\n",
      "model survey covers 11 models but no models\n",
      "above 1.5B parameters; extending to larger mod-\n",
      "els requires GPU resources. Third, the detection\n",
      "architecture is proposed but not yet benchmarked\n",
      "against established datasets such as HaluEval (Li\n",
      "et al., 2023). Fourth, the α statistic is limited by\n",
      "the number of clusters containing ≥2 co-clustered\n",
      "antonym pairs from our curated set; sample sizes\n",
      "range from 3 to 11 clusters per model, and means\n",
      "computed on the smaller samples are less stable.\n",
      "Finally, the k = 40 cluster count is a hyperparam-\n",
      "eter wh\n",
      "\n",
      "==========================================================================================\n",
      "#2 | dist=0.3160 | paper_id=2602.14778v2 | year=2026 | page=11\n",
      "TITLE: A Geometric Analysis of Small-sized Language Model Hallucinations\n",
      "FILE: 2026_emanuele-ricco_a-geometric-analysis-of-small-sized-language-model-hallucina_2602.147782.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "A Geometric Analysis of Small-sized Language Model Hallucinations\n",
      "large language models. arXiv preprint arXiv:2511.12116,\n",
      "2025.\n",
      "Qi, X., Panda, A., Lyu, K., Ma, X., Roy, S., Beirami, A.,\n",
      "Mittal, P., and Henderson, P. Safety alignment should be\n",
      "made more than just a few tokens deep. In Yue, Y., Garg,\n",
      "A., Peng, N., Sha, F., and Yu, R. (eds.), International\n",
      "Conference on Representation Learning, volume 2025,\n",
      "pp. 54911–54941, 2025.\n",
      "Reimers, N. and Gurevych, I. Sentence-BERT: Sentence\n",
      "embeddings using Siamese BERT-networks.\n",
      "In Inui,\n",
      "K., Jiang, J., Ng, V., and Wan, X. (eds.), Proceed-\n",
      "ings of the 2019 Conference on Empirical Methods\n",
      "in Natural Language Processing and the 9th Interna-\n",
      "tional Joint C\n",
      "\n",
      "==========================================================================================\n",
      "#3 | dist=0.3162 | paper_id=2602.14778v2 | year=2026 | page=11\n",
      "TITLE: A Geometric Analysis of Small-sized Language Model Hallucinations\n",
      "FILE: 2026_emanuele-ricco_a-geometric-analysis-of-small-sized-language-model-hallucina_2602.147782.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "W., Wang, C., Ai, Q., Hu, Y., Wu, Z., Zhou, Y., and\n",
      "Liu, Y. Unsupervised real-time hallucination detection\n",
      "based on the internal states of large language models. In\n",
      "Findings of the 62nd Annual Meeting of the Association\n",
      "for Computational Linguistics, ACL 2024, pp. 14379–\n",
      "14391. Association for Computational Linguistics (ACL),\n",
      "2024.\n",
      "Sun, Y., Sheng, D., Zhou, Z., and Wu, Y. Ai hallucina-\n",
      "tion: towards a comprehensive classification of distorted\n",
      "information in artificial intelligence-generated content.\n",
      "Humanities and Social Sciences Communications, 11(1):\n",
      "1–14, 2024.\n",
      "Tonmoy, S. T. I., Zaman, S. M., Jain, V., Rani, A., Rawte,\n",
      "V., Chadha, A., and Das, A. A comprehensive survey\n",
      "of hallucination mi\n",
      "\n",
      "==========================================================================================\n",
      "#4 | dist=0.3280 | paper_id=2602.14778v2 | year=2026 | page=11\n",
      "TITLE: A Geometric Analysis of Small-sized Language Model Hallucinations\n",
      "FILE: 2026_emanuele-ricco_a-geometric-analysis-of-small-sized-language-model-hallucina_2602.147782.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "ankanhalli, M.\n",
      "Hallucination is\n",
      "inevitable: An innate limitation of large language models.\n",
      "arXiv preprint arXiv:2401.11817, 2024.\n",
      "Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z.,\n",
      "Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., Zhang, H.,\n",
      "Gonzalez, J. E., and Stoica, I. Judging llm-as-a-judge\n",
      "with mt-bench and chatbot arena. In Oh, A., Naumann,\n",
      "T., Globerson, A., Saenko, K., Hardt, M., and Levine, S.\n",
      "(eds.), Advances in Neural Information Processing Sys-\n",
      "tems, volume 36, pp. 46595–46623. Curran Associates,\n",
      "Inc., 2023.\n",
      "Zhu, Z., Liao, Y., Chen, Z., Wang, Y., Guan, Y., Wang, Y.,\n",
      "and Wang, Y. Evolvebench: A comprehensive benchmark\n",
      "for assessing temporal awareness in llms on evolving\n",
      "knowle\n",
      "\n",
      "==========================================================================================\n",
      "#5 | dist=0.3295 | paper_id=2602.14419v1 | year=2026 | page=8\n",
      "TITLE: WavePhaseNet: A DFT-Based Method for Constructing Semantic Conceptual Hierarchy Structures (SCHS)\n",
      "FILE: 2026_kiyotaka-kasubuchi_wavephasenet-a-dft-based-method-for-constructing-semantic-co_2602.144191.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "Fourier transforms,” it ex-\n",
      "tends this idea toward the explicit construction of semantic hierarchical structure and the suppression of\n",
      "hallucination.\n",
      "10\n",
      "Conclusion\n",
      "This paper formulates hallucination in large language models (LLMs) as an inevitable phenomenon arising\n",
      "from their underlying mathematical structure, and presents a theoretical framework to suppress this issue\n",
      "by introducing a DFT-based Semantic Concept Hierarchy Structure (SCHS) into the embedding space.\n",
      "In particular, the demonstration that the reduction from 24,576 dimensions to 3,000 dimensions is justified\n",
      "by cumulative energy analysis based on Zipf’s law and the 1/f spectrum constitutes the core contribution of\n",
      "this proposal\n",
      "\n",
      "\n",
      "##############################\n",
      "QUERY: How do papers define faithfulness or groundedness?\n",
      "\n",
      "==========================================================================================\n",
      "#1 | dist=0.7170 | paper_id=2602.16154v1 | year=2026 | page=1\n",
      "TITLE: Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution\n",
      "FILE: 2026_nithin-sivakumaran_balancing-faithfulness-and-performance-in-reasoning-via-mult_2602.161541.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "’s rea-\n",
      "soning (Lambert et al., 2024). Indeed, past work has found\n",
      "that outcome-only RL can lead to unfaithful or uninter-\n",
      "pretable outputs (Guo et al., 2025; Kirchner et al., 2024;\n",
      "Jose, 2025). Unlike correctness, which can be objectively\n",
      "defined, we argue that faithfulness is a multi-party prop-\n",
      "erty: given a speaker and a listener, a faithful explanation is\n",
      "one which enables the listener model to come to the same\n",
      "conclusion as the speaker, without access to the speaker’s\n",
      "answer. For example, in Figure 1(A), we see that when\n",
      "we truncate a reasoning chain from a speaker model to ex-\n",
      "clude its final few steps and its answer, listener models use\n",
      "the same reasoning prefix to come to different\n",
      "\n",
      "==========================================================================================\n",
      "#2 | dist=0.7434 | paper_id=2602.16154v1 | year=2026 | page=3\n",
      "TITLE: Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution\n",
      "FILE: 2026_nithin-sivakumaran_balancing-faithfulness-and-performance-in-reasoning-via-mult_2602.161541.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "the remaining reasoning steps\n",
      "to reach a potentially different answer a′. We define our\n",
      "faithfulness reward as I[L(x, t1,m) = S(x, t)] (or, equiva-\n",
      "lently, I[a′ = a]) i.e. the degree to which a listener model L\n",
      "reaches the same answer as the original speaker reasoning\n",
      "model S. This process is illustrated in Figure 1, where the\n",
      "speaker model’s partial chain is passed to multiple listeners.\n",
      "As illustrated in Figure 2 (top), REMUL uses cross-model\n",
      "execution as a training signal. We generate an initial re-\n",
      "sponse to input x using S and generate reasoning trace t and\n",
      "final answer a. We then truncate t to 25%, 50%, and 75% of\n",
      "its length (split on newlines) to get T ′ = {t1, t2, t3}. Then,\n",
      "given a\n",
      "\n",
      "==========================================================================================\n",
      "#3 | dist=0.7479 | paper_id=2602.16154v1 | year=2026 | page=9\n",
      "TITLE: Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution\n",
      "FILE: 2026_nithin-sivakumaran_balancing-faithfulness-and-performance-in-reasoning-via-mult_2602.161541.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "faithfulness, has speakers exchange truncated reasoning\n",
      "prefixes rather than complete messages, and incorporates\n",
      "correctness signals as a separate training procedure.\n",
      "6. Conclusion\n",
      "We proposed REMUL, a multi-party training framework to\n",
      "improve faithfulness in reasoning models. By incentivizing\n",
      "the speaker model to produce reasoning traces that listener\n",
      "models can effectively execute across multiple truncation\n",
      "points, we encourage the policy model to generate responses\n",
      "that are executable by listener models using soft execution,\n",
      "i.e., completing a reasoning prefix. A key finding of our\n",
      "work is the importance of decoupling the training strategies\n",
      "for faithfulness and accuracy, allowing us to i\n",
      "\n",
      "==========================================================================================\n",
      "#4 | dist=0.7588 | paper_id=2602.16162v1 | year=2026 | page=2\n",
      "TITLE: LLMs Exhibit Significantly Lower Uncertainty in Creative Writing Than Professional Writers\n",
      "FILE: 2026_peiqi-sui_llms-exhibit-significantly-lower-uncertainty-in-creative-wri_2602.161621.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "ground truth? Yes. Across all models,\n",
      "datasets, and metrics, we observe a consistent human–model\n",
      "uncertainty gap. Current post-training alignment methods\n",
      "widen this disparity: instruct and thinking models exhibit\n",
      "significantly larger gaps than their base counterparts.\n",
      "R2: Is this gap unique to creative writing? It is present\n",
      "across other domains but amplified in creative writing, espe-\n",
      "cially on context-dependent uncertainty metrics.\n",
      "R3: Is uncertainty correlated with writing quality? Yes.\n",
      "We find a robust correlation between higher uncertainty met-\n",
      "rics and automated quality scores, with evidence of possible\n",
      "“sweet spots” (inverted U-curve) where optimal writing qual-\n",
      "ity exists at high ent\n",
      "\n",
      "==========================================================================================\n",
      "#5 | dist=0.7625 | paper_id=2602.16154v1 | year=2026 | page=2\n",
      "TITLE: Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution\n",
      "FILE: 2026_nithin-sivakumaran_balancing-faithfulness-and-performance-in-reasoning-via-mult_2602.161541.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "Millan et al., 2025), we find that\n",
      "increasing faithfulness alone comes at a cost to correct-\n",
      "ness. To offset this, we incorporate correctness training into\n",
      "REMUL. Finding that jointly rewarding correctness and\n",
      "faithfulness via GRPO tends to lead to competing rewards\n",
      "and suboptimal performance, we introduce a mixed method\n",
      "which optimizes for faithfulness via RL and optimizes for\n",
      "correctness using supervised finetuning (SFT) with a LoRA\n",
      "(Hu et al., 2021) adapter, shown in Figure 2 (bottom).\n",
      "After training on data from BIG-Bench Hard (BBH; Suzgun\n",
      "et al., 2023), we evaluate REMUL on challenging reason-\n",
      "ing problems from BIG-Bench Extra Hard (Kazemi et al.,\n",
      "2025), logic puzzles from ZebraLogicBen\n",
      "\n",
      "\n",
      "##############################\n",
      "QUERY: What metrics are used to measure factuality or faithfulness?\n",
      "\n",
      "==========================================================================================\n",
      "#1 | dist=0.5201 | paper_id=2602.13964v2 | year=2026 | page=2\n",
      "TITLE: HLE-Verified: A Systematic Verification and Structured Revision of Humanity's Last Exam\n",
      "FILE: 2026_weiqi-zhai_hle-verified-a-systematic-verification-and-structured-revisi_2602.139642.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "or reference answers can disproportionately influence aggregate metrics. Recent independent audits\n",
      "and community-led examinations have raised concerns that HLE contains a non-trivial number of noisy\n",
      "items, including ambiguous or underspecified problem statements, incorrect reference answers, and\n",
      "inconsistencies between rationales and final answers (White, 2025; lhl, 2026; red, 2025). These analyses\n",
      "suggest that a portion of measured model performance on HLE may reflect annotation artifacts rather\n",
      "than genuine capability differences.\n",
      "This issue highlights a broader methodological question: how should high-difficulty benchmarks be\n",
      "systematically verified and maintained after deployment? While\n",
      "\n",
      "==========================================================================================\n",
      "#2 | dist=0.5218 | paper_id=2602.14778v2 | year=2026 | page=2\n",
      "TITLE: A Geometric Analysis of Small-sized Language Model Hallucinations\n",
      "FILE: 2026_emanuele-ricco_a-geometric-analysis-of-small-sized-language-model-hallucina_2602.147782.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "150 generations for 200 prompts across 10 small-sized\n",
      "LLMs) to support structural analyses of hallucinations.\n",
      "Roadmap\n",
      "The next section reviews related work; Section\n",
      "3 introduces the adopted methodology; Section 4 reports the\n",
      "achieved results, while their discussion is in Section 5; and,\n",
      "finally, Section 6 sports some concluding remarks.\n",
      "2. Related Work\n",
      "Small-sized LLMs (7–30B) offer practical advantages in\n",
      "terms of efficiency, deployability, and accessibility, enabling\n",
      "fast inference and domain-specific customization. However,\n",
      "these benefits raise important concerns about reliability and\n",
      "trustworthiness, particularly regarding hallucinations, which\n",
      "motivate the need for systematic analysis a\n",
      "\n",
      "==========================================================================================\n",
      "#3 | dist=0.5308 | paper_id=2602.16154v1 | year=2026 | page=6\n",
      "TITLE: Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution\n",
      "FILE: 2026_nithin-sivakumaran_balancing-faithfulness-and-performance-in-reasoning-via-mult_2602.161541.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution\n",
      "Table 2. Qwen3-14B AOC metrics for measuring faithfulness using truncated CoT and after adding mistakes to the CoT, following\n",
      "Lanham et al. (2023).\n",
      "Method\n",
      "Truncated CoT Answering\n",
      "Adding Mistake\n",
      "BBEH\n",
      "ZLB\n",
      "MuSR\n",
      "FOLIO\n",
      "BBEH\n",
      "ZLB\n",
      "MuSR\n",
      "FOLIO\n",
      "Original\n",
      "0.580\n",
      "0.520\n",
      "0.332\n",
      "0.284\n",
      "0.621\n",
      "0.793\n",
      "0.708\n",
      "0.667\n",
      "MAT-Steer\n",
      "0.644\n",
      "0.527\n",
      "0.336\n",
      "0.320\n",
      "0.649\n",
      "0.808\n",
      "0.722\n",
      "0.674\n",
      "Faithfulness Only\n",
      "0.665\n",
      "0.587\n",
      "0.330\n",
      "0.350\n",
      "0.672\n",
      "0.838\n",
      "0.731\n",
      "0.714\n",
      "Correctness Only\n",
      "0.518\n",
      "0.474\n",
      "0.317\n",
      "0.247\n",
      "0.598\n",
      "0.778\n",
      "0.694\n",
      "0.648\n",
      "Balanced Rewards\n",
      "0.574\n",
      "0.512\n",
      "0.328\n",
      "0.299\n",
      "0.614\n",
      "0.790\n",
      "0.712\n",
      "0.671\n",
      "Hint Optimized\n",
      "0.503\n",
      "0.481\n",
      "0.304\n",
      "0.268\n",
      "0.587\n",
      "0.771\n",
      "0.688\n",
      "0.641\n",
      "REMUL\n",
      "\n",
      "==========================================================================================\n",
      "#4 | dist=0.5483 | paper_id=2602.16154v1 | year=2026 | page=6\n",
      "TITLE: Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution\n",
      "FILE: 2026_nithin-sivakumaran_balancing-faithfulness-and-performance-in-reasoning-via-mult_2602.161541.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "this seems to be a local improve-\n",
      "ment without transferable improvements on other metrics:\n",
      "Table 2 indicates that AOC with Truncated CoT Answer-\n",
      "ing decreases when models are optimized for hint usage,\n",
      "with the hint usage-optimized model obtaining the worst\n",
      "AOC for half the datasets. This holds true for the AOC with\n",
      "Adding Mistake, where hint-optimized models obtain the\n",
      "lowest scores on all benchmarks.\n",
      "In contrast, faithfulness-only training and REMUL’s im-\n",
      "provements to hint usage in Table 1 transfer to Table 2,\n",
      "suggesting that multi-party listener training may do more\n",
      "to address unfaithfulness at its core, whereas optimizing\n",
      "for hint usage merely addresses one symptom. Similarly,\n",
      "Table 1 sh\n",
      "\n",
      "==========================================================================================\n",
      "#5 | dist=0.5495 | paper_id=2602.14812v1 | year=2026 | page=5\n",
      "TITLE: Physical Commonsense Reasoning for Lower-Resourced Languages and Dialects: a Study on Basque\n",
      "FILE: 2026_jaione-bengoetxea_physical-commonsense-reasoning-for-lower-resourced-languages_2602.148121.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "each task is evaluated\n",
      "conditionally on the success of the previous one,\n",
      "forming a crescendo of increasingly demanding\n",
      "reasoning requirements. Specifically, only the cor-\n",
      "rectly solved instances from one level are used as\n",
      "input to the next. Accordingly, we adopt three com-\n",
      "plementary metrics for the three evaluated tasks:\n",
      "• Accuracy: Quantifies the proportion of the\n",
      "correctly identified plausible and implausible\n",
      "stories. This metric will be used in the story\n",
      "classification task.\n",
      "• Consistency: Measures the proportion of the\n",
      "correctly identified plausible sentences and the\n",
      "conflicting sentence in the implausible stories.\n",
      "The aim of this measure is to check models’\n",
      "consistency when recognizing\n",
      "\n",
      "\n",
      "##############################\n",
      "QUERY: How does retrieval (RAG) help reduce hallucination?\n",
      "\n",
      "==========================================================================================\n",
      "#1 | dist=0.3460 | paper_id=2602.14612v1 | year=2026 | page=1\n",
      "TITLE: LongAudio-RAG: Event-Grounded Question Answering over Multi-Hour Long Audio\n",
      "FILE: 2026_naveen-vakada_longaudio-rag-event-grounded-question-answering-over-multi-h_2602.146121.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "downstream\n",
      "reasoning. Third, open-ended generation over long\n",
      "logs is prone to hallucination unless responses are\n",
      "explicitly grounded in verifiable evidence.\n",
      "Retrieval-augmented generation (RAG) reduces\n",
      "hallucinations by grounding model outputs in re-\n",
      "trieved evidence (Lewis et al., 2020; Gupta et al.,\n",
      "2024).\n",
      "Although widely used for knowledge-\n",
      "intensive QA and summarization, RAG perfor-\n",
      "mance is tightly coupled to retrieval quality, es-\n",
      "pecially for ambiguous or underspecified queries\n",
      "(Karpukhin et al., 2020; Gupta et al., 2024). Re-\n",
      "arXiv:2602.14612v1 [eess.AS] 16 Feb 2026\n",
      "\n",
      "==========================================================================================\n",
      "#2 | dist=0.5823 | paper_id=2602.14374v1 | year=2026 | page=1\n",
      "TITLE: Differentially Private Retrieval-Augmented Generation\n",
      "FILE: 2026_tingting-tang_differentially-private-retrieval-augmented-generation_2602.143741.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "Differentially Private Retrieval-Augmented Generation\n",
      "Tingting Tang\n",
      "University of Southern California\n",
      "Los Angeles, California, USA\n",
      "tangting@usc.edu\n",
      "James Flemings\n",
      "University of Southern California\n",
      "Los Angeles, California, USA\n",
      "jamesf17@usc.edu\n",
      "Yongqin Wang\n",
      "University of Southern California\n",
      "Los Angeles, California, USA\n",
      "yongqin@usc.edu\n",
      "Murali Annavaram\n",
      "University of Southern California\n",
      "Los Angeles, California, USA\n",
      "annavara@usc.edu\n",
      "Abstract\n",
      "Retrieval-augmented generation (RAG) is a widely used framework\n",
      "for reducing hallucinations in large language models (LLMs) on\n",
      "domain-specific tasks by retrieving relevant documents from a data-\n",
      "base to support accurate responses. However, when the database\n",
      "c\n",
      "\n",
      "==========================================================================================\n",
      "#3 | dist=0.5885 | paper_id=2602.14612v1 | year=2026 | page=1\n",
      "TITLE: LongAudio-RAG: Event-Grounded Question Answering over Multi-Hour Long Audio\n",
      "FILE: 2026_naveen-vakada_longaudio-rag-event-grounded-question-answering-over-multi-h_2602.146121.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "a GPU-backed server. This architecture enables\n",
      "low-latency event extraction at the edge and\n",
      "high-quality language reasoning in the cloud.\n",
      "Experiments show that structured, event-level\n",
      "retrieval significantly improves accuracy com-\n",
      "pared to vanilla Retrieval-Augmented Genera-\n",
      "tion (RAG) or text-to-SQL approaches.\n",
      "1\n",
      "Introduction\n",
      "Long-duration audio streams are now common not\n",
      "only in industrial settings such as machine monitor-\n",
      "ing and safety logging but also in homes through\n",
      "smart assistants, baby monitors, and security sys-\n",
      "tems. These recordings span many hours, making\n",
      "manual review impractical and motivating systems\n",
      "Figure 1: Chat Example for LongAudio-RAG\n",
      "that can answer natural-language q\n",
      "\n",
      "==========================================================================================\n",
      "#4 | dist=0.5886 | paper_id=2602.15509v1 | year=2026 | page=4\n",
      "TITLE: Fine-Refine: Iterative Fine-grained Refinement for Mitigating Dialogue Hallucination\n",
      "FILE: 2026_xiangyan-chen_fine-refine-iterative-fine-grained-refinement-for-mitigating_2602.155091.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      ". We adopt Llama-2-7B as the back-\n",
      "bone model, consistent with prior work.\n",
      "• Self-RAG is a text-based retrieval-augmented generation\n",
      "approach that learns reflection/critique signals to decide\n",
      "when to retrieve and to assess whether the generated re-\n",
      "sponse is supported by the retrieved evidence. We adopt\n",
      "Llama-2-7B as the backbone, following prior work\n",
      "Experimental Setup\n",
      "To ensure a fair comparison, we control the decoding con-\n",
      "figuration (e.g., max tokens and sampling strategy) across\n",
      "\n",
      "==========================================================================================\n",
      "#5 | dist=0.6107 | paper_id=2602.14259v1 | year=2026 | page=7\n",
      "TITLE: Detecting LLM Hallucinations via Embedding Cluster Geometry: A Three-Type Taxonomy with Measurable Signatures\n",
      "FILE: 2026_matic-korun_detecting-llm-hallucinations-via-embedding-cluster-geometry-_2602.142591.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "prerequisites, but hallucination detection in prac-\n",
      "tice operates on contextual hidden states, which\n",
      "may exhibit different geometry. Second, the cross-\n",
      "model survey covers 11 models but no models\n",
      "above 1.5B parameters; extending to larger mod-\n",
      "els requires GPU resources. Third, the detection\n",
      "architecture is proposed but not yet benchmarked\n",
      "against established datasets such as HaluEval (Li\n",
      "et al., 2023). Fourth, the α statistic is limited by\n",
      "the number of clusters containing ≥2 co-clustered\n",
      "antonym pairs from our curated set; sample sizes\n",
      "range from 3 to 11 clusters per model, and means\n",
      "computed on the smaller samples are less stable.\n",
      "Finally, the k = 40 cluster count is a hyperparam-\n",
      "eter wh\n",
      "\n",
      "\n",
      "##############################\n",
      "QUERY: What are common causes of hallucination?\n",
      "\n",
      "==========================================================================================\n",
      "#1 | dist=0.6538 | paper_id=2602.14529v1 | year=2026 | page=2\n",
      "TITLE: Disentangling Deception and Hallucination Failures in LLMs\n",
      "FILE: 2026_haolang-lu_disentangling-deception-and-hallucination-failures-in-llms_2602.145291.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "Prior Perspectives\n",
      "Hallucination.\n",
      "Hallucination in QA tasks is commonly\n",
      "studied as a failure arising from missing or unreliable knowl-\n",
      "edge (Wen et al., 2024). In most prior work (Huang et al.,\n",
      "2025a), incorrect or unsupported outputs are treated as evi-\n",
      "dence that the model does not possess the relevant informa-\n",
      "tion. Early studies primarily focus on output-level factual\n",
      "errors, evaluating hallucination through correctness (Seo\n",
      "& Lim, 2025; Oh et al., 2024) or faithfulness (Tang et al.,\n",
      "2025) in question answering and generation tasks. Subse-\n",
      "quent work incorporates behavioral signals, such as incon-\n",
      "sistency across prompts (Farquhar et al., 2024; Chen et al.,\n",
      "2024a), sensitivity to paraphr\n",
      "\n",
      "==========================================================================================\n",
      "#2 | dist=0.6703 | paper_id=2602.14529v1 | year=2026 | page=8\n",
      "TITLE: Disentangling Deception and Hallucination Failures in LLMs\n",
      "FILE: 2026_haolang-lu_disentangling-deception-and-hallucination-failures-in-llms_2602.145291.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "to steer hallucinated outputs back to the\n",
      "correct regime are entirely unsuccessful, with flip rates ap-\n",
      "proaching zero across all entities. This sharp asymmetry\n",
      "provides direct causal evidence that hallucination and decep-\n",
      "tion are governed by distinct internal mechanisms. In par-\n",
      "ticular, when K = 0, manipulating behavior expression B\n",
      "alone is insufficient to recover correct outputs, indicating\n",
      "that hallucination induced by knowledge absence cannot be\n",
      "reversed without introducing new knowledge.\n",
      "Takeaway. Under fixed knowledge (K = 1), behavior\n",
      "expression B constitutes a controllable internal degree\n",
      "of freedom, and that deceptive behaviors can be induced\n",
      "or reversed through targeted manipula\n",
      "\n",
      "==========================================================================================\n",
      "#3 | dist=0.7002 | paper_id=2602.14529v1 | year=2026 | page=14\n",
      "TITLE: Disentangling Deception and Hallucination Failures in LLMs\n",
      "FILE: 2026_haolang-lu_disentangling-deception-and-hallucination-failures-in-llms_2602.145291.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "Disentangling Deception and Hallucination Failures in LLMs\n",
      "Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X.,\n",
      "Efrat, A., Yu, P., Yu, L., et al. Lima: Less is more for\n",
      "alignment. Advances in Neural Information Processing\n",
      "Systems, 36:55006–55021, 2023.\n",
      "14\n",
      "\n",
      "==========================================================================================\n",
      "#4 | dist=0.7177 | paper_id=2602.14259v1 | year=2026 | page=7\n",
      "TITLE: Detecting LLM Hallucinations via Embedding Cluster Geometry: A Three-Type Taxonomy with Measurable Signatures\n",
      "FILE: 2026_matic-korun_detecting-llm-hallucinations-via-embedding-cluster-geometry-_2602.142591.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "prerequisites, but hallucination detection in prac-\n",
      "tice operates on contextual hidden states, which\n",
      "may exhibit different geometry. Second, the cross-\n",
      "model survey covers 11 models but no models\n",
      "above 1.5B parameters; extending to larger mod-\n",
      "els requires GPU resources. Third, the detection\n",
      "architecture is proposed but not yet benchmarked\n",
      "against established datasets such as HaluEval (Li\n",
      "et al., 2023). Fourth, the α statistic is limited by\n",
      "the number of clusters containing ≥2 co-clustered\n",
      "antonym pairs from our curated set; sample sizes\n",
      "range from 3 to 11 clusters per model, and means\n",
      "computed on the smaller samples are less stable.\n",
      "Finally, the k = 40 cluster count is a hyperparam-\n",
      "eter wh\n",
      "\n",
      "==========================================================================================\n",
      "#5 | dist=0.7214 | paper_id=2602.14529v1 | year=2026 | page=1\n",
      "TITLE: Disentangling Deception and Hallucination Failures in LLMs\n",
      "FILE: 2026_haolang-lu_disentangling-deception-and-hallucination-failures-in-llms_2602.145291.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "the relevant knowledge present?)\n",
      "Figure 1. External and Internal Views of LLM Outputs. We\n",
      "characterize model failures, including hallucination and deception,\n",
      "through two latent factors: Knowledge Existence and Behavioral\n",
      "Expression, which together provide an internal, mechanism-level\n",
      "account of how different failure modes arise.\n",
      "making processes. Consequently, understanding how and\n",
      "why these failures arise has become a foundational require-\n",
      "ment for the reliable deployment of LLMs.\n",
      "Are all model failures hallucinations? Most studies (Wang\n",
      "et al., 2025; Shorinwa et al., 2025) implicitly treats failures\n",
      "in LLM interaction as hallucinations. From this perspec-\n",
      "tive, hallucination is typically d\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"What is hallucination in large language models?\",\n",
    "    \"How do papers define faithfulness or groundedness?\",\n",
    "    \"What metrics are used to measure factuality or faithfulness?\",\n",
    "    \"How does retrieval (RAG) help reduce hallucination?\",\n",
    "    \"What are common causes of hallucination?\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(\"\\n\\n\" + \"#\"*30)\n",
    "    print(\"QUERY:\", q)\n",
    "    search(q, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b84f95ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2602.14419v1 3\n",
      "Thus, the entire model resides in\n",
      "Lp(Ω, F, P)\n",
      "(6)\n",
      "The norm of this Lp space corresponds to the Lebesgue integral representing the expectation of the\n",
      "probability distribution\n",
      "∥f∥p =\n",
      "\u0012Z\n",
      "Ω\n",
      "|f(ω)|p dP(ω)\n",
      "\u00131/p\n",
      "(7)\n",
      "This expression indicates that the model optimizes not pointwise truth values, but plausible average\n",
      "optimality over distributions.\n",
      "2.3\n",
      "Norms and the Minkowski Inequality\n",
      "In Lp spaces, the Minkowski inequality\n",
      "∥f + g∥p ≤∥f∥p + ∥g∥p\n",
      "(8)\n",
      "holds. This implies that different semantic components, hypotheses, and contexts can be linearly combined\n",
      "in the embedding space, while the norm remains stable (i.e., no gradient explosion).\n",
      "This property allows semantic superposition, which is a source of the flexibility of generative AI.\n",
      "On the other hand, the combination of this linearity and incompleteness inherently contains a structure\n",
      "that can generate non-existent propositions (erroneous outputs).\n",
      "3\n",
      "Mathematical Definition of Hallucination\n",
      "3.1\n",
      "Definition\n",
      "Definition 1 (Hallucination). Let T denote the truth set, and let S denote the support of the training\n",
      "distribution. A hallucination occurs when\n",
      "S ̸⊆T\n",
      "(9)\n",
      "In other words, when there exists a point x ∈S such that\n",
      "P(x) > 0\n",
      "but\n",
      "x /∈T\n",
      "(10)\n",
      "the model generates a statement x with positive probability even though it is not true.\n",
      "The embedding space V functions as a conditional expectation over the σ-algebra Ft, but when this is\n",
      "not isomorphic to T,\n",
      "E[f | Ft] ̸∼= f|T\n",
      "(11)\n",
      "holds. That is, the conditional expectation does not correctly project onto the truth manifold.\n",
      "3.2\n",
      "Impossibility of Isomorphism and Incompleteness\n",
      "Theorem 2 (Structural Inevitability of Hallucination). For the embedding space V and the\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "keywords = [\n",
    "    \"we define hallucination\",\n",
    "    \"hallucination refers to\",\n",
    "    \"hallucination occurs when\",\n",
    "    \"hallucination can be understood\",\n",
    "    \"hallucination denotes\",\n",
    "]\n",
    "\n",
    "with open(\"../data/processed/chunks.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        text = item[\"text\"].lower()\n",
    "        if any(k in text for k in keywords):\n",
    "            print(item[\"paper_id\"], item[\"page\"])\n",
    "            print(item[\"text\"])\n",
    "            print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d88127d",
   "metadata": {},
   "source": [
    "The query \"what is hallucination...\" doesn't return the definition of hallucination. However, by scanning chunks.jsonl and match lines with keywords, 1 formal definition of hallucination is found. The solutions include:\n",
    "- Find 5-10 papers about hallucination, benchmark papers, or foundational papers\n",
    "- Create index for chunks. For example, chunks that contain terms like \"we define\", \"refers to\", \"definition\" can be saved to \"definition_index\".\n",
    "- MMR(Diversity Control) to avoid top-k from 1 paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10218a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter: year >= 2024\n",
      "\n",
      "==========================================================================================\n",
      "#1 | dist=0.5143 | paper_id=2602.16154v1 | year=2026 | page=6\n",
      "TITLE: Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution\n",
      "FILE: 2026_nithin-sivakumaran_balancing-faithfulness-and-performance-in-reasoning-via-mult_2602.161541.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution\n",
      "Table 2. Qwen3-14B AOC metrics for measuring faithfulness using truncated CoT and after adding mistakes to the CoT, following\n",
      "Lanham et al. (2023).\n",
      "Method\n",
      "Truncated CoT Answering\n",
      "Adding Mistake\n",
      "BBEH\n",
      "ZLB\n",
      "MuSR\n",
      "FOLIO\n",
      "BBEH\n",
      "ZLB\n",
      "MuSR\n",
      "FOLIO\n",
      "Original\n",
      "0.580\n",
      "0.520\n",
      "0.332\n",
      "0.284\n",
      "0.621\n",
      "0.793\n",
      "0.708\n",
      "0.667\n",
      "MAT-Steer\n",
      "0.644\n",
      "0.527\n",
      "0.336\n",
      "0.320\n",
      "0.649\n",
      "0.808\n",
      "0.722\n",
      "0.674\n",
      "Faithfulness Only\n",
      "0.665\n",
      "0.587\n",
      "0.330\n",
      "0.350\n",
      "0.672\n",
      "0.838\n",
      "0.731\n",
      "0.714\n",
      "Correctness Only\n",
      "0.518\n",
      "0.474\n",
      "0.317\n",
      "0.247\n",
      "0.598\n",
      "0.778\n",
      "0.694\n",
      "0.648\n",
      "Balanced Rewards\n",
      "0.574\n",
      "0.512\n",
      "0.328\n",
      "0.299\n",
      "0.614\n",
      "0.790\n",
      "0.712\n",
      "0.671\n",
      "Hint Optimized\n",
      "0.503\n",
      "0.481\n",
      "0.304\n",
      "0.268\n",
      "0.587\n",
      "0.771\n",
      "0.688\n",
      "0.641\n",
      "REMUL\n",
      "\n",
      "==========================================================================================\n",
      "#2 | dist=0.5559 | paper_id=2602.16038v1 | year=2026 | page=6\n",
      "TITLE: Heuristic Search as Language-Guided Program Optimization\n",
      "FILE: 2026_mingxin-yu_heuristic-search-as-language-guided-program-optimization_2602.160381.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "0.514±0.002\n",
      "0.532±0.023\n",
      "EoH-I\n",
      "0.838±0.005\n",
      "0.521±0.042\n",
      "0.947±0.004\n",
      "0.888±0.035\n",
      "0.629±0.002\n",
      "0.678±0.001\n",
      "0.610±0.031\n",
      "0.450±0.076\n",
      "ReEvo-I\n",
      "0.859±0.042\n",
      "0.636±0.065\n",
      "0.912±0.038\n",
      "0.882±0.032\n",
      "0.631±0.001\n",
      "0.681±0.003\n",
      "0.597±0.110\n",
      "0.542±0.054\n",
      "LLM-LNS-I\n",
      "0.836±0.002\n",
      "0.487±0.032\n",
      "0.936±0.035\n",
      "0.916±0.026\n",
      "0.629±0.002\n",
      "0.682±0.002\n",
      "0.560±0.031\n",
      "0.515±0.081\n",
      "Ours\n",
      "0.882±0.031\n",
      "0.808±0.002\n",
      "0.953±0.003\n",
      "0.918±0.006\n",
      "0.732±0.061\n",
      "0.713±0.013\n",
      "0.783±0.045\n",
      "0.636±0.060\n",
      "• End-to-End Generation: For HeuriGen, we follow\n",
      "its original protocol, where the LLM directly produces\n",
      "the complete executable code without a predefined\n",
      "algorithm skeleton.\n",
      "These protocols allow us to isolate the performance contri-\n",
      "butions of different algorith\n",
      "\n",
      "==========================================================================================\n",
      "#3 | dist=0.5786 | paper_id=2602.16154v1 | year=2026 | page=10\n",
      "TITLE: Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution\n",
      "FILE: 2026_nithin-sivakumaran_balancing-faithfulness-and-performance-in-reasoning-via-mult_2602.161541.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution\n",
      "token pruning. In ES-FoMo III: 3rd Workshop on Efficient\n",
      "Systems for Foundation Models, 2025.\n",
      "Creswell, A. and Shanahan, M. Faithful reasoning using\n",
      "large language models. arXiv preprint arXiv:2208.14271,\n",
      "2022.\n",
      "Eisenstein, J., Aghajani, R., Fisch, A., Dua, D., Huot, F.,\n",
      "Lapata, M., Zayats, V., and Berant, J. Don’t lie to your\n",
      "friends: Learning what you know from collaborative self-\n",
      "play. arXiv preprint arXiv:2503.14481, 2025.\n",
      "Emmons, S., Zimmermann, R. S., Elson, D. K., and Shah,\n",
      "R. A pragmatic way to measure chain-of-thought moni-\n",
      "torability. arXiv preprint arXiv:2510.23966, 2025.\n",
      "Fang, G., Ma, X., and Wan\n",
      "\n",
      "==========================================================================================\n",
      "#4 | dist=0.5824 | paper_id=2602.16154v1 | year=2026 | page=6\n",
      "TITLE: Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution\n",
      "FILE: 2026_nithin-sivakumaran_balancing-faithfulness-and-performance-in-reasoning-via-mult_2602.161541.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "this seems to be a local improve-\n",
      "ment without transferable improvements on other metrics:\n",
      "Table 2 indicates that AOC with Truncated CoT Answer-\n",
      "ing decreases when models are optimized for hint usage,\n",
      "with the hint usage-optimized model obtaining the worst\n",
      "AOC for half the datasets. This holds true for the AOC with\n",
      "Adding Mistake, where hint-optimized models obtain the\n",
      "lowest scores on all benchmarks.\n",
      "In contrast, faithfulness-only training and REMUL’s im-\n",
      "provements to hint usage in Table 1 transfer to Table 2,\n",
      "suggesting that multi-party listener training may do more\n",
      "to address unfaithfulness at its core, whereas optimizing\n",
      "for hint usage merely addresses one symptom. Similarly,\n",
      "Table 1 sh\n",
      "\n",
      "==========================================================================================\n",
      "#5 | dist=0.6004 | paper_id=2602.16038v1 | year=2026 | page=6\n",
      "TITLE: Heuristic Search as Language-Guided Program Optimization\n",
      "FILE: 2026_mingxin-yu_heuristic-search-as-language-guided-program-optimization_2602.160381.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      ".\n",
      "Following (Chen et al., 2025), we adopt the\n",
      "Quality-Yield Index (QYI) as the primary metric for eval-\n",
      "uating performance. QYI provides a unified measure of\n",
      "a heuristic’s robustness (feasibility) and optimality. It is\n",
      "defined as the harmonic mean of Quality and Yield:\n",
      "QYI = 2 · Quality · Yield\n",
      "Quality + Yield\n",
      "(8)\n",
      "where the components are calculated as:\n",
      "Quality = 1\n",
      "ˆK\n",
      "ˆ\n",
      "K\n",
      "X\n",
      "k=1\n",
      "min\n",
      "\u0012\n",
      "1, c∗\n",
      "k\n",
      "ck\n",
      "\u0013\n",
      ",\n",
      "Yield =\n",
      "ˆK\n",
      "K\n",
      "(9)\n",
      "Here, K is the total number of instances, ˆK is the number of\n",
      "instances for which a feasible solution was found, and ck and\n",
      "c∗\n",
      "k represent the costs of the LLM-generated solution and\n",
      "the expert or optimal solution for instance k, respectively.\n",
      "While QYI is used for final evaluatio\n"
     ]
    }
   ],
   "source": [
    "print(\"Filter: year >= 2024\")\n",
    "search(\"faithfulness metric\", k=5, where={\"year\": {\"$gte\": 2024}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad1441a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter: paper_id == 2602.16660v1\n",
      "\n",
      "==========================================================================================\n",
      "#1 | dist=0.5244 | paper_id=2602.16660v1 | year=2026 | page=18\n",
      "TITLE: Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment\n",
      "FILE: 2026_yuyan-bu_align-once-benefit-multilingually-enforcing-multilingual-con_2602.166601.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "PO leverages the reward gap in a dominant language as a high-quality supervision signal to guide\n",
      "multilingual safety alignment. Specifically, it minimizes the discrepancy in reward gaps across\n",
      "different languages to enable effective transfer of alignment signals. We reproduce MPO using the\n",
      "same PKU-SafeRLHF dataset as in our main experiments 11. The only deviation from the original\n",
      "setup lies in the data source: instead of using their original dataset, we adopt the prompt, chosen,\n",
      "and rejected fields from PKU-SafeRLHF, and translate both prompts and responses into the target\n",
      "languages to construct the training data for MPO.\n",
      "D.4\n",
      "EVALUATION PROMPT\n",
      "For safety evaluation on both PKU-SafeRLHF and\n",
      "\n",
      "==========================================================================================\n",
      "#2 | dist=0.6619 | paper_id=2602.16660v1 | year=2026 | page=12\n",
      "TITLE: Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment\n",
      "FILE: 2026_yuyan-bu_align-once-benefit-multilingually-enforcing-multilingual-con_2602.166601.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "2025b.\n",
      "Yu Meng, Mengzhou Xia, and Danqi Chen.\n",
      "Simpo: Simple preference optimization with a\n",
      "reference-free reward. Advances in Neural Information Processing Systems, 37:124198–124235,\n",
      "2024.\n",
      "Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le\n",
      "Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual gen-\n",
      "eralization through multitask finetuning. arXiv preprint arXiv:2211.01786, 2022.\n",
      "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\n",
      "Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to fol-\n",
      "low instructions with human feedback. Advances in neural inform\n",
      "\n",
      "==========================================================================================\n",
      "#3 | dist=0.6626 | paper_id=2602.16660v1 | year=2026 | page=6\n",
      "TITLE: Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment\n",
      "FILE: 2026_yuyan-bu_align-once-benefit-multilingually-enforcing-multilingual-con_2602.166601.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "(DPO) (Rafailov et al., 2023), simple preference optimization (SimPO) (Meng et al., 2024), and\n",
      "odds-ratio preference optimization (ORPO) (Hong et al., 2024).\n",
      "Baselines. We also compare our approach against representative multilingual alignment baselines\n",
      "MPO (Zhao et al., 2025) and SDRRL (Zhang et al., 2024b). MPO is a representative algorithm-\n",
      "design-based multilingual safety alignment method that leverages the reward gap in a dominant\n",
      "language for high-quality supervision. SDRRL is a representative data-augmentation-based method\n",
      "that constructs supervision signals for knowledge distillation by pairing model-generated responses\n",
      "in resource-rich languages with their corresponding target-langu\n",
      "\n",
      "==========================================================================================\n",
      "#4 | dist=0.6758 | paper_id=2602.16660v1 | year=2026 | page=16\n",
      "TITLE: Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment\n",
      "FILE: 2026_yuyan-bu_align-once-benefit-multilingually-enforcing-multilingual-con_2602.166601.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "σiuiv⊤\n",
      "i\n",
      "2\n",
      "F\n",
      "=\n",
      "k\n",
      "X\n",
      "i=2\n",
      "σ2\n",
      "i .\n",
      "(9)\n",
      "Since:\n",
      "∥Z∥2\n",
      "F =\n",
      "k\n",
      "X\n",
      "i=1\n",
      "∥z(i)∥2 = k =\n",
      "r\n",
      "X\n",
      "i=1\n",
      "σ2\n",
      "i\n",
      "(10)\n",
      "we get the identity\n",
      "min\n",
      "rank(˜Z)=1\n",
      "∥Z −˜Z∥2\n",
      "F = k −σ2\n",
      "1\n",
      "⇐⇒\n",
      "max σ1.\n",
      "(11)\n",
      "Considering that a naive objective max σ1 is unbounded and ignores the need to suppress competing\n",
      "directions. To obtain a stable and differentiable formulation, we cast the problem into a normalized\n",
      "objective by applying a softmax over all singular values. In this way, σ1 is encouraged to dominate\n",
      "while competing directions are simultaneously suppressed. Concretely, we maximize the probability\n",
      "of σ1 under the softmax distribution, leading to the multilingual consistency loss:\n",
      "Lcons = −1\n",
      "N\n",
      "N\n",
      "X\n",
      "n=1\n",
      "log\n",
      "exp(σ(n)\n",
      "1\n",
      "/τ)\n",
      "Pk\n",
      "j=1 exp(σ(n)\n",
      "j\n",
      "\n",
      "==========================================================================================\n",
      "#5 | dist=0.6842 | paper_id=2602.16660v1 | year=2026 | page=3\n",
      "TITLE: Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment\n",
      "FILE: 2026_yuyan-bu_align-once-benefit-multilingually-enforcing-multilingual-con_2602.166601.pdf\n",
      "------------------------------------------------------------------------------------------\n",
      "(m−1)\n",
      "P\n",
      "i<j I\n",
      "\u0002\n",
      "M(q(li)) = M(q(lj))\n",
      "\u0003\n",
      ", where M(q(li)) denotes the\n",
      "model’s response to the query translated into language li, and I is the indicator function. A higher\n",
      "PAG(q) indicates stronger response consistency across languages for the same underlying prompt.\n",
      "In the context of safety alignment, for example, π indicates whether an answer is classified as safe or\n",
      "unsafe. Multilingual consistency then requires that, given semantically equivalent safety-sensitive\n",
      "prompts in different languages, the model should exhibit the same behavior (e.g., consistently refus-\n",
      "ing unsafe requests or consistently providing safe responses).\n",
      "Logits-based alignment. A dominant line of work in post-training al\n"
     ]
    }
   ],
   "source": [
    "paper_id_example = \"2602.16660v1\"\n",
    "print(f\"Filter: paper_id == {paper_id_example}\")\n",
    "search(\"reward gap supervision signal\", k=5, where={\"paper_id\": paper_id_example})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d23210",
   "metadata": {},
   "source": [
    "MMR just return more papers, it doesn't guarantee that the paper containing the definition of \"hallucination\" is returned."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
