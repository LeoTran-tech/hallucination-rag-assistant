paper_id,title,year,first_author,authors,published,updated,categories,summary,pdf_url,arxiv_url,file_name,file_path
2602.16704v1,Reinforced Fast Weights with Next-Sequence Prediction,2026,Hee Seung Hwang,"Hee Seung Hwang, Xindi Wu, Sanghyuk Chun, Olga Russakovsky",2026-02-18T18:53:18+00:00,2026-02-18T18:53:18+00:00,cs.CL,"Fast weight architectures offer a promising alternative to attention-based transformers for long-context modeling by maintaining constant memory overhead regardless of context length. However, their potential is limited by the next-token prediction (NTP) training paradigm. NTP optimizes single-token predictions and ignores semantic coherence across multiple tokens following a prefix. Consequently, fast weight models, which dynamically update their parameters to store contextual information, learn suboptimal representations that fail to capture long-range dependencies. We introduce REFINE (Reinforced Fast weIghts with Next sEquence prediction), a reinforcement learning framework that trains fast weight models under the next-sequence prediction (NSP) objective. REFINE selects informative token positions based on prediction entropy, generates multi-token rollouts, assigns self-supervised sequence-level rewards, and optimizes the model with group relative policy optimization (GRPO). REFINE is applicable throughout the training lifecycle of pre-trained language models: mid-training, post-training, and test-time training. Our experiments on LaCT-760M and DeltaNet-1.3B demonstrate that REFINE consistently outperforms supervised fine-tuning with NTP across needle-in-a-haystack retrieval, long-context question answering, and diverse tasks in LongBench. REFINE provides an effective and versatile framework for improving long-context modeling in fast weight architectures.",https://arxiv.org/pdf/2602.16704v1,http://arxiv.org/abs/2602.16704v1,2026_hee-seung-hwang_reinforced-fast-weights-with-next-sequence-prediction_2602.167041.pdf,data\raw\papers\2026_hee-seung-hwang_reinforced-fast-weights-with-next-sequence-prediction_2602.167041.pdf
2602.16671v1,SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation,2026,Jaid Monwar Chowdhury,"Jaid Monwar Chowdhury, Chi-An Fu, Reyhaneh Jabbarvand",2026-02-18T18:09:03+00:00,2026-02-18T18:09:03+00:00,"cs.SE, cs.AI","Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases.",https://arxiv.org/pdf/2602.16671v1,http://arxiv.org/abs/2602.16671v1,2026_jaid-monwar-chowdhury_sparc-scenario-planning-and-reasoning-for-automated-c-unit-t_2602.166711.pdf,data\raw\papers\2026_jaid-monwar-chowdhury_sparc-scenario-planning-and-reasoning-for-automated-c-unit-t_2602.166711.pdf
2602.16660v1,"Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment",2026,Yuyan Bu,"Yuyan Bu, Xiaohao Liu, ZhaoXing Ren, Yaodong Yang, Juntao Dai",2026-02-18T18:01:23+00:00,2026-02-18T18:01:23+00:00,"cs.CL, cs.AI, cs.LG","The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the target language or through pairwise alignment with high-resource languages, which limits scalability. In this work, we propose a resource-efficient method for improving multilingual safety alignment. We introduce a plug-and-play Multi-Lingual Consistency (MLC) loss that can be integrated into existing monolingual alignment pipelines. By improving collinearity between multilingual representation vectors, our method encourages directional consistency at the multilingual semantic level in a single update. This allows simultaneous alignment across multiple languages using only multilingual prompt variants without requiring additional response-level supervision in low-resource languages. We validate the proposed method across different model architectures and alignment paradigms, and demonstrate its effectiveness in enhancing multilingual safety with limited impact on general model utility. Further evaluation across languages and tasks indicates improved cross-lingual generalization, suggesting the proposed approach as a practical solution for multilingual consistency alignment under limited supervision.",https://arxiv.org/pdf/2602.16660v1,http://arxiv.org/abs/2602.16660v1,2026_yuyan-bu_align-once-benefit-multilingually-enforcing-multilingual-con_2602.166601.pdf,data\raw\papers\2026_yuyan-bu_align-once-benefit-multilingually-enforcing-multilingual-con_2602.166601.pdf
2602.16653v1,Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments,2026,Yangjie Xu,"Yangjie Xu, Lujun Li, Lama Sleem, Niccolo Gentile, Yewei Song, Yiqun Wang, Siming Ji, Wenbo Wu, Radu State",2026-02-18T17:52:17+00:00,2026-02-18T17:52:17+00:00,cs.AI,"Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on public APIs is infeasible due to data-security and budget constraints requirements, and where SLMs often show limited generalization in highly customized scenarios. This work introduces a formal mathematical definition of the Agent Skill process, followed by a systematic evaluation of language models of varying sizes across multiple use cases. The evaluation encompasses two open-source tasks and a real-world insurance claims data set. The results show that tiny models struggle with reliable skill selection, while moderately sized SLMs (approximately 12B - 30B) parameters) benefit substantially from the Agent Skill approach. Moreover, code-specialized variants at around 80B parameters achieve performance comparable to closed-source baselines while improving GPU efficiency. Collectively, these findings provide a comprehensive and nuanced characterization of the capabilities and constraints of the framework, while providing actionable insights for the effective deployment of Agent Skills in SLM-centered environments.",https://arxiv.org/pdf/2602.16653v1,http://arxiv.org/abs/2602.16653v1,2026_yangjie-xu_agent-skill-framework-perspectives-on-the-potential-of-small_2602.166531.pdf,data\raw\papers\2026_yangjie-xu_agent-skill-framework-perspectives-on-the-potential-of-small_2602.166531.pdf
2602.16610v1,Who can we trust? LLM-as-a-jury for Comparative Assessment,2026,Mengjie Qian,"Mengjie Qian, Guangzhi Sun, Mark J. F. Gales, Kate M. Knill",2026-02-18T17:04:02+00:00,2026-02-18T17:04:02+00:00,"cs.CL, cs.AI, cs.LG","Large language models (LLMs) are increasingly applied as automatic evaluators for natural language generation assessment often using pairwise comparative judgements. Existing approaches typically rely on single judges or aggregate multiple judges assuming equal reliability. In practice, LLM judges vary substantially in performance across tasks and aspects, and their judgment probabilities may be biased and inconsistent. Furthermore, human-labelled supervision for judge calibration may be unavailable. We first empirically demonstrate that inconsistencies in LLM comparison probabilities exist and show that it limits the effectiveness of direct probability-based ranking. To address this, we study the LLM-as-a-jury setting and propose BT-sigma, a judge-aware extension of the Bradley-Terry model that introduces a discriminator parameter for each judge to jointly infer item rankings and judge reliability from pairwise comparisons alone. Experiments on benchmark NLG evaluation datasets show that BT-sigma consistently outperforms averaging-based aggregation methods, and that the learned discriminator strongly correlates with independent measures of the cycle consistency of LLM judgments. Further analysis reveals that BT-sigma can be interpreted as an unsupervised calibration mechanism that improves aggregation by modelling judge reliability.",https://arxiv.org/pdf/2602.16610v1,http://arxiv.org/abs/2602.16610v1,2026_mengjie-qian_who-can-we-trust-llm-as-a-jury-for-comparative-assessment_2602.166101.pdf,data\raw\papers\2026_mengjie-qian_who-can-we-trust-llm-as-a-jury-for-comparative-assessment_2602.166101.pdf
2602.16590v1,A Contrastive Learning Framework Empowered by Attention-based Feature Adaptation for Street-View Image Classification,2026,Qi You,"Qi You, Yitai Cheng, Zichao Zeng, James Haworth",2026-02-18T16:41:32+00:00,2026-02-18T16:41:32+00:00,"cs.CV, cs.AI, cs.LG","Street-view image attribute classification is a vital downstream task of image classification, enabling applications such as autonomous driving, urban analytics, and high-definition map construction. It remains computationally demanding whether training from scratch, initialising from pre-trained weights, or fine-tuning large models. Although pre-trained vision-language models such as CLIP offer rich image representations, existing adaptation or fine-tuning methods often rely on their global image embeddings, limiting their ability to capture fine-grained, localised attributes essential in complex, cluttered street scenes. To address this, we propose CLIP-MHAdapter, a variant of the current lightweight CLIP adaptation paradigm that appends a bottleneck MLP equipped with multi-head self-attention operating on patch tokens to model inter-patch dependencies. With approximately 1.4 million trainable parameters, CLIP-MHAdapter achieves superior or competitive accuracy across eight attribute classification tasks on the Global StreetScapes dataset, attaining new state-of-the-art results while maintaining low computational cost. The code is available at https://github.com/SpaceTimeLab/CLIP-MHAdapter.",https://arxiv.org/pdf/2602.16590v1,http://arxiv.org/abs/2602.16590v1,2026_qi-you_a-contrastive-learning-framework-empowered-by-attention-base_2602.165901.pdf,data\raw\papers\2026_qi-you_a-contrastive-learning-framework-empowered-by-attention-base_2602.165901.pdf
2602.16467v1,IndicEval: A Bilingual Indian Educational Evaluation Framework for Large Language Models,2026,Saurabh Bharti,"Saurabh Bharti, Gaurav Azad, Abhinaw Jagtap, Nachiket Tapas",2026-02-18T13:55:57+00:00,2026-02-18T13:55:57+00:00,"cs.CL, cs.AI","The rapid advancement of large language models (LLMs) necessitates evaluation frameworks that reflect real-world academic rigor and multilingual complexity. This paper introduces IndicEval, a scalable benchmarking platform designed to assess LLM performance using authentic high-stakes examination questions from UPSC, JEE, and NEET across STEM and humanities domains in both English and Hindi. Unlike synthetic benchmarks, IndicEval grounds evaluation in real examination standards, enabling realistic measurement of reasoning, domain knowledge, and bilingual adaptability. The framework automates assessment using Zero-Shot, Few-Shot, and Chain-of-Thought (CoT) prompting strategies and supports modular integration of new models and languages. Experiments conducted on Gemini 2.0 Flash, GPT-4, Claude, and LLaMA 3-70B reveal three major findings. First, CoT prompting consistently improves reasoning accuracy, with substantial gains across subjects and languages. Second, significant cross-model performance disparities persist, particularly in high-complexity examinations. Third, multilingual degradation remains a critical challenge, with marked accuracy drops in Hindi compared to English, especially under Zero-Shot conditions. These results highlight persistent gaps in bilingual reasoning and domain transfer. Overall, IndicEval provides a practice-oriented, extensible foundation for rigorous, equitable evaluation of LLMs in multilingual educational settings and offers actionable insights for improving reasoning robustness and language adaptability.",https://arxiv.org/pdf/2602.16467v1,http://arxiv.org/abs/2602.16467v1,2026_saurabh-bharti_indiceval-a-bilingual-indian-educational-evaluation-framewor_2602.164671.pdf,data\raw\papers\2026_saurabh-bharti_indiceval-a-bilingual-indian-educational-evaluation-framewor_2602.164671.pdf
2602.16438v1,Intra-Fairness Dynamics: The Bias Spillover Effect in Targeted LLM Alignment,2026,Eva Paraschou,"Eva Paraschou, Line Harder Clemmensen, Sneha Das",2026-02-18T13:19:11+00:00,2026-02-18T13:19:11+00:00,"cs.LG, cs.AI","Conventional large language model (LLM) fairness alignment largely focuses on mitigating bias along single sensitive attributes, overlooking fairness as an inherently multidimensional and context-specific value. This approach risks creating systems that achieve narrow fairness metrics while exacerbating disparities along untargeted attributes, a phenomenon known as bias spillover. While extensively studied in machine learning, bias spillover remains critically underexplored in LLM alignment. In this work, we investigate how targeted gender alignment affects fairness across nine sensitive attributes in three state-of-the-art LLMs (Mistral 7B, Llama 3.1 8B, Qwen 2.5 7B). Using Direct Preference Optimization and the BBQ benchmark, we evaluate fairness under ambiguous and disambiguous contexts. Our findings reveal noticeable bias spillover: while aggregate results show improvements, context-aware analysis exposes significant degradations in ambiguous contexts, particularly for physical appearance ($p< 0.001$ across all models), sexual orientation, and disability status. We demonstrate that improving fairness along one attribute can inadvertently worsen disparities in others under uncertainty, highlighting the necessity of context-aware, multi-attribute fairness evaluation frameworks.",https://arxiv.org/pdf/2602.16438v1,http://arxiv.org/abs/2602.16438v1,2026_eva-paraschou_intra-fairness-dynamics-the-bias-spillover-effect-in-targete_2602.164381.pdf,data\raw\papers\2026_eva-paraschou_intra-fairness-dynamics-the-bias-spillover-effect-in-targete_2602.164381.pdf
2602.16430v1,Designing Production-Scale OCR for India: Multilingual and Domain-Specific Systems,2026,Ali Faraz,"Ali Faraz, Raja Kolla, Ashish Kulkarni, Shubham Agarwal",2026-02-18T13:03:05+00:00,2026-02-18T13:03:05+00:00,"cs.CV, cs.AI","Designing Optical Character Recognition (OCR) systems for India requires balancing linguistic diversity, document heterogeneity, and deployment constraints. In this paper, we study two training strategies for building multilingual OCR systems with Vision-Language Models through the Chitrapathak series. We first follow a popular multimodal approach, pairing a generic vision encoder with a strong multilingual language model and training the system end-to-end for OCR. Alternatively, we explore fine-tuning an existing OCR model, despite not being trained for the target languages. Through extensive evaluation on multilingual Indic OCR benchmarks and deployment-oriented metrics, we find that the second strategy consistently achieves better accuracy-latency trade-offs. Chitrapathak-2 achieves 3-6x speedup over its predecessor with being state-of-the-art (SOTA) in Telugu (6.69 char ANLS) and second best in the rest. In addition, we present Parichay, an independent OCR model series designed specifically for 9 Indian government documents to extract structured key fields, achieving 89.8% Exact Match score with a faster inference. Together, these systems achieve SOTA performance and provide practical guidance for building production-scale OCR pipelines in the Indian context.",https://arxiv.org/pdf/2602.16430v1,http://arxiv.org/abs/2602.16430v1,2026_ali-faraz_designing-production-scale-ocr-for-india-multilingual-and-do_2602.164301.pdf,data\raw\papers\2026_ali-faraz_designing-production-scale-ocr-for-india-multilingual-and-do_2602.164301.pdf
2602.16424v1,Verifiable Semantics for Agent-to-Agent Communication,2026,Philipp Schoenegger,"Philipp Schoenegger, Matt Carlson, Chris Schneider, Chris Daly",2026-02-18T12:55:58+00:00,2026-02-18T12:55:58+00:00,"cs.AI, cs.MA","Multiagent AI systems require consistent communication, but we lack methods to verify that agents share the same understanding of the terms used. Natural language is interpretable but vulnerable to semantic drift, while learned protocols are efficient but opaque. We propose a certification protocol based on the stimulus-meaning model, where agents are tested on shared observable events and terms are certified if empirical disagreement falls below a statistical threshold. In this protocol, agents restricting their reasoning to certified terms (""core-guarded reasoning"") achieve provably bounded disagreement. We also outline mechanisms for detecting drift (recertification) and recovering shared vocabulary (renegotiation). In simulations with varying degrees of semantic divergence, core-guarding reduces disagreement by 72-96%. In a validation with fine-tuned language models, disagreement is reduced by 51%. Our framework provides a first step towards verifiable agent-to-agent communication.",https://arxiv.org/pdf/2602.16424v1,http://arxiv.org/abs/2602.16424v1,2026_philipp-schoenegger_verifiable-semantics-for-agent-to-agent-communication_2602.164241.pdf,data\raw\papers\2026_philipp-schoenegger_verifiable-semantics-for-agent-to-agent-communication_2602.164241.pdf
2602.16379v1,Label-Consistent Data Generation for Aspect-Based Sentiment Analysis Using LLM Agents,2026,Mohammad H. A. Monfared,"Mohammad H. A. Monfared, Lucie Flek, Akbar Karimi",2026-02-18T11:38:11+00:00,2026-02-18T11:38:11+00:00,cs.CL,"We propose an agentic data augmentation method for Aspect-Based Sentiment Analysis (ABSA) that uses iterative generation and verification to produce high quality synthetic training examples. To isolate the effect of agentic structure, we also develop a closely matched prompting-based baseline using the same model and instructions. Both methods are evaluated across three ABSA subtasks (Aspect Term Extraction (ATE), Aspect Sentiment Classification (ATSC), and Aspect Sentiment Pair Extraction (ASPE)), four SemEval datasets, and two encoder-decoder models: T5-Base and Tk-Instruct. Our results show that the agentic augmentation outperforms raw prompting in label preservation of the augmented data, especially when the tasks require aspect term generation. In addition, when combined with real data, agentic augmentation provides higher gains, consistently outperforming prompting-based generation. These benefits are most pronounced for T5-Base, while the more heavily pretrained Tk-Instruct exhibits smaller improvements. As a result, augmented data helps T5-Base achieve comparable performance with its counterpart.",https://arxiv.org/pdf/2602.16379v1,http://arxiv.org/abs/2602.16379v1,2026_mohammad-h-a-monfared_label-consistent-data-generation-for-aspect-based-sentiment-_2602.163791.pdf,data\raw\papers\2026_mohammad-h-a-monfared_label-consistent-data-generation-for-aspect-based-sentiment-_2602.163791.pdf
2602.16346v1,"Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents",2026,Nivya Talokar,"Nivya Talokar, Ayush K Tarun, Murari Mandal, Maksym Andriushchenko, Antoine Bosselut",2026-02-18T10:31:19+00:00,2026-02-18T10:31:19+00:00,"cs.CL, cs.LG","LLM-based agents execute real-world workflows via tools and memory. These affordances enable ill-intended adversaries to also use these agents to carry out complex misuse scenarios. Existing agent misuse benchmarks largely test single-prompt instructions, leaving a gap in measuring how agents end up helping with harmful or illegal tasks over multiple turns. We introduce STING (Sequential Testing of Illicit N-step Goal execution), an automated red-teaming framework that constructs a step-by-step illicit plan grounded in a benign persona and iteratively probes a target agent with adaptive follow-ups, using judge agents to track phase completion. We further introduce an analysis framework that models multi-turn red-teaming as a time-to-first-jailbreak random variable, enabling analysis tools like discovery curves, hazard-ratio attribution by attack language, and a new metric: Restricted Mean Jailbreak Discovery. Across AgentHarm scenarios, STING yields substantially higher illicit-task completion than single-turn prompting and chat-oriented multi-turn baselines adapted to tool-using agents. In multilingual evaluations across six non-English settings, we find that attack success and illicit-task completion do not consistently increase in lower-resource languages, diverging from common chatbot findings. Overall, STING provides a practical way to evaluate and stress-test agent misuse in realistic deployment settings, where interactions are inherently multi-turn and often multilingual.",https://arxiv.org/pdf/2602.16346v1,http://arxiv.org/abs/2602.16346v1,2026_nivya-talokar_helpful-to-a-fault-measuring-illicit-assistance-in-multi-tur_2602.163461.pdf,data\raw\papers\2026_nivya-talokar_helpful-to-a-fault-measuring-illicit-assistance-in-multi-tur_2602.163461.pdf
2602.16298v1,MultiCW: A Large-Scale Balanced Benchmark Dataset for Training Robust Check-Worthiness Detection Models,2026,Martin Hyben,"Martin Hyben, Sebastian Kula, Jan Cegin, Jakub Simko, Ivan Srba, Robert Moro",2026-02-18T09:28:53+00:00,2026-02-18T09:28:53+00:00,cs.CL,"Large Language Models (LLMs) are beginning to reshape how media professionals verify information, yet automated support for detecting check-worthy claims a key step in the fact-checking process remains limited. We introduce the Multi-Check-Worthy (MultiCW) dataset, a balanced multilingual benchmark for check-worthy claim detection spanning 16 languages, 7 topical domains, and 2 writing styles. It consists of 123,722 samples, evenly distributed between noisy (informal) and structured (formal) texts, with balanced representation of check-worthy and non-check-worthy classes across all languages. To probe robustness, we also introduce an equally balanced out-of-distribution evaluation set of 27,761 samples in 4 additional languages. To provide baselines, we benchmark 3 common fine-tuned multilingual transformers against a diverse set of 15 commercial and open LLMs under zero-shot settings. Our findings show that fine-tuned models consistently outperform zero-shot LLMs on claim classification and show strong out-of-distribution generalization across languages, domains, and styles. MultiCW provides a rigorous multilingual resource for advancing automated fact-checking and enables systematic comparisons between fine-tuned models and cutting-edge LLMs on the check-worthy claim detection task.",https://arxiv.org/pdf/2602.16298v1,http://arxiv.org/abs/2602.16298v1,2026_martin-hyben_multicw-a-large-scale-balanced-benchmark-dataset-for-trainin_2602.162981.pdf,data\raw\papers\2026_martin-hyben_multicw-a-large-scale-balanced-benchmark-dataset-for-trainin_2602.162981.pdf
2602.16246v1,Toward Scalable Verifiable Reward: Proxy State-Based Evaluation for Multi-turn Tool-Calling LLM Agents,2026,Yun-Shiuan Chuang,"Yun-Shiuan Chuang, Chaitanya Kulkarni, Alec Chiu, Avinash Thangali, Zijie Pan, Shivani Shekhar, Yirou Ge, Yixi Li, Uma Kona, Linsey Pang, Prakhar Mehrotra",2026-02-18T07:49:47+00:00,2026-02-18T07:49:47+00:00,cs.AI,"Interactive large language model (LLM) agents operating via multi-turn dialogue and multi-step tool calling are increasingly used in production. Benchmarks for these agents must both reliably compare models and yield on-policy training data. Prior agentic benchmarks (e.g., tau-bench, tau2-bench, AppWorld) rely on fully deterministic backends, which are costly to build and iterate. We propose Proxy State-Based Evaluation, an LLM-driven simulation framework that preserves final state-based evaluation without a deterministic database. Specifically, a scenario specifies the user goal, user/system facts, expected final state, and expected agent behavior, and an LLM state tracker infers a structured proxy state from the full interaction trace. LLM judges then verify goal completion and detect tool/user hallucinations against scenario constraints. Empirically, our benchmark produces stable, model-differentiating rankings across families and inference-time reasoning efforts, and its on-/off-policy rollouts provide supervision that transfers to unseen scenarios. Careful scenario specification yields near-zero simulator hallucination rates as supported by ablation studies. The framework also supports sensitivity analyses over user personas. Human-LLM judge agreement exceeds 90%, indicating reliable automated evaluation. Overall, proxy state-based evaluation offers a practical, scalable alternative to deterministic agentic benchmarks for industrial LLM agents.",https://arxiv.org/pdf/2602.16246v1,http://arxiv.org/abs/2602.16246v1,2026_yun-shiuan-chuang_toward-scalable-verifiable-reward-proxy-state-based-evaluati_2602.162461.pdf,data\raw\papers\2026_yun-shiuan-chuang_toward-scalable-verifiable-reward-proxy-state-based-evaluati_2602.162461.pdf
2602.16241v1,Are LLMs Ready to Replace Bangla Annotators?,2026,Md. Najib Hasan,"Md. Najib Hasan, Touseef Hasan, Souvika Sarkar",2026-02-18T07:36:41+00:00,2026-02-18T07:36:41+00:00,"cs.CL, cs.AI","Large Language Models (LLMs) are increasingly used as automated annotators to scale dataset creation, yet their reliability as unbiased annotators--especially for low-resource and identity-sensitive settings--remains poorly understood. In this work, we study the behavior of LLMs as zero-shot annotators for Bangla hate speech, a task where even human agreement is challenging, and annotator bias can have serious downstream consequences. We conduct a systematic benchmark of 17 LLMs using a unified evaluation framework. Our analysis uncovers annotator bias and substantial instability in model judgments. Surprisingly, increased model scale does not guarantee improved annotation quality--smaller, more task-aligned models frequently exhibit more consistent behavior than their larger counterparts. These results highlight important limitations of current LLMs for sensitive annotation tasks in low-resource languages and underscore the need for careful evaluation before deployment.",https://arxiv.org/pdf/2602.16241v1,http://arxiv.org/abs/2602.16241v1,2026_md-najib-hasan_are-llms-ready-to-replace-bangla-annotators_2602.162411.pdf,data\raw\papers\2026_md-najib-hasan_are-llms-ready-to-replace-bangla-annotators_2602.162411.pdf
2602.16189v1,Beyond Learning: A Training-Free Alternative to Model Adaptation,2026,Namkyung Yoon,"Namkyung Yoon, Kyeonghyun Yoo, Wooyong Jung, Sanghong Kim, Hwangnam Kim",2026-02-18T05:17:44+00:00,2026-02-18T05:17:44+00:00,"cs.CL, cs.AI","Despite the continuous research and evolution of language models, they sometimes underperform previous versions. Existing approaches to overcome these challenges are resource-intensive, highlighting the need for alternatives that enable immediate action. We assume that each language model has a local module inside that is suitable for a specific function. First, this work identifies a set of modules showing consistent and local activation changes under an inference workload through activation-based analysis. Subsequently, we transplant an internal module that is properly activated for a specific task into the target model, leading to immediate and measurable functional changes without additional training or fine-tuning. To experimentally demonstrate the effectiveness of the transplant technique, we quantify the relationship between transplant strength and performance improvement under different conditions for two language models. In the cross-generation setting, we find that transplanting activation-selected modules can substantially improve the underperforming model, reaching up to twice the target baseline and achieving gap-based recovery above 100%. Moreover, in transplant experiments between a base model and its instruction-tuned counterpart, transplantation improves the underperforming model toward the stronger baseline, yielding up to about 2.33 times the target baseline with gap-based recovery reaching up to 100% in the best case. These results show that meaningful capacity transfer can be realized through the implantation of highly localized modules implied by language models. Overall, this work provides empirical evidence for task-localized modularity in language models and presents a new research area: model transplantation.",https://arxiv.org/pdf/2602.16189v1,http://arxiv.org/abs/2602.16189v1,2026_namkyung-yoon_beyond-learning-a-training-free-alternative-to-model-adaptat_2602.161891.pdf,data\raw\papers\2026_namkyung-yoon_beyond-learning-a-training-free-alternative-to-model-adaptat_2602.161891.pdf
2602.16188v1,Deep TPC: Temporal-Prior Conditioning for Time Series Forecasting,2026,Filippos Bellos,"Filippos Bellos, NaveenJohn Premkumar, Yannis Avrithis, Nam H. Nguyen, Jason J. Corso",2026-02-18T05:16:29+00:00,2026-02-18T05:16:29+00:00,cs.LG,"LLM-for-time series (TS) methods typically treat time shallowly, injecting positional or prompt-based cues once at the input of a largely frozen decoder, which limits temporal reasoning as this information degrades through the layers. We introduce Temporal-Prior Conditioning (TPC), which elevates time to a first-class modality that conditions the model at multiple depths. TPC attaches a small set of learnable time series tokens to the patch stream; at selected layers these tokens cross-attend to temporal embeddings derived from compact, human-readable temporal descriptors encoded by the same frozen LLM, then feed temporal context back via self-attention. This disentangles time series signal and temporal information while maintaining a low parameter budget. We show that by training only the cross-attention modules and explicitly disentangling time series signal and temporal information, TPC consistently outperforms both full fine-tuning and shallow conditioning strategies, achieving state-of-the-art performance in long-term forecasting across diverse datasets. Code available at: https://github.com/fil-mp/Deep_tpc",https://arxiv.org/pdf/2602.16188v1,http://arxiv.org/abs/2602.16188v1,2026_filippos-bellos_deep-tpc-temporal-prior-conditioning-for-time-series-forecas_2602.161881.pdf,data\raw\papers\2026_filippos-bellos_deep-tpc-temporal-prior-conditioning-for-time-series-forecas_2602.161881.pdf
2602.16162v1,LLMs Exhibit Significantly Lower Uncertainty in Creative Writing Than Professional Writers,2026,Peiqi Sui,Peiqi Sui,2026-02-18T03:19:12+00:00,2026-02-18T03:19:12+00:00,cs.CL,"We argue that uncertainty is a key and understudied limitation of LLMs' performance in creative writing, which is often characterized as trite and cliché-ridden. Literary theory identifies uncertainty as a necessary condition for creative expression, while current alignment strategies steer models away from uncertain outputs to ensure factuality and reduce hallucination. We formalize this tension by quantifying the ""uncertainty gap"" between human-authored stories and model-generated continuations. Through a controlled information-theoretic analysis of 28 LLMs on high-quality storytelling datasets, we demonstrate that human writing consistently exhibits significantly higher uncertainty than model outputs. We find that instruction-tuned and reasoning models exacerbate this trend compared to their base counterparts; furthermore, the gap is more pronounced in creative writing than in functional domains, and strongly correlates to writing quality. Achieving human-level creativity requires new uncertainty-aware alignment paradigms that can distinguish between destructive hallucinations and the constructive ambiguity required for literary richness.",https://arxiv.org/pdf/2602.16162v1,http://arxiv.org/abs/2602.16162v1,2026_peiqi-sui_llms-exhibit-significantly-lower-uncertainty-in-creative-wri_2602.161621.pdf,data\raw\papers\2026_peiqi-sui_llms-exhibit-significantly-lower-uncertainty-in-creative-wri_2602.161621.pdf
2602.16154v1,Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution,2026,Nithin Sivakumaran,"Nithin Sivakumaran, Shoubin Yu, Hyunji Lee, Yue Zhang, Ali Payani, Mohit Bansal, Elias Stengel-Eskin",2026-02-18T02:55:55+00:00,2026-02-18T02:55:55+00:00,"cs.CL, cs.AI","Chain-of-thought (CoT) reasoning sometimes fails to faithfully reflect the true computation of a large language model (LLM), hampering its utility in explaining how LLMs arrive at their answers. Moreover, optimizing for faithfulness and interpretability in reasoning often degrades task performance. To address this tradeoff and improve CoT faithfulness, we propose Reasoning Execution by Multiple Listeners (REMUL), a multi-party reinforcement learning approach. REMUL builds on the hypothesis that reasoning traces which other parties can follow will be more faithful. A speaker model generates a reasoning trace, which is truncated and passed to a pool of listener models who ""execute"" the trace, continuing the trace to an answer. Speakers are rewarded for producing reasoning that is clear to listeners, with additional correctness regularization via masked supervised finetuning to counter the tradeoff between faithfulness and performance. On multiple reasoning benchmarks (BIG-Bench Extra Hard, MuSR, ZebraLogicBench, and FOLIO), REMUL consistently and substantially improves three measures of faithfulness -- hint attribution, early answering area over the curve (AOC), and mistake injection AOC -- while also improving accuracy. Our analysis finds that these gains are robust across training domains, translate to legibility gains, and are associated with shorter and more direct CoTs.",https://arxiv.org/pdf/2602.16154v1,http://arxiv.org/abs/2602.16154v1,2026_nithin-sivakumaran_balancing-faithfulness-and-performance-in-reasoning-via-mult_2602.161541.pdf,data\raw\papers\2026_nithin-sivakumaran_balancing-faithfulness-and-performance-in-reasoning-via-mult_2602.161541.pdf
2602.16111v1,Surrogate-Based Prevalence Measurement for Large-Scale A/B Testing,2026,Zehao Xu,"Zehao Xu, Tony Paek, Kevin O'Sullivan, Attila Dobi",2026-02-18T00:45:46+00:00,2026-02-18T00:45:46+00:00,"stat.AP, cs.AI","Online media platforms often need to measure how frequently users are exposed to specific content attributes in order to evaluate trade-offs in A/B experiments. A direct approach is to sample content, label it using a high-quality rubric (e.g., an expert-reviewed LLM prompt), and estimate impression-weighted prevalence. However, repeatedly running such labeling for every experiment arm and segment is too costly and slow to serve as a default measurement at scale.   We present a scalable \emph{surrogate-based prevalence measurement} framework that decouples expensive labeling from per-experiment evaluation. The framework calibrates a surrogate signal to reference labels offline and then uses only impression logs to estimate prevalence for arbitrary experiment arms and segments. We instantiate this framework using \emph{score bucketing} as the surrogate: we discretize a model score into buckets, estimate bucket-level prevalences from an offline labeled sample, and combine these calibrated bucket level prevalences with the bucket distribution of impressions in each arm to obtain fast, log-based estimates.   Across multiple large-scale A/B tests, we validate that the surrogate estimates closely match the reference estimates for both arm-level prevalence and treatment--control deltas. This enables scalable, low-latency prevalence measurement in experimentation without requiring per-experiment labeling jobs.",https://arxiv.org/pdf/2602.16111v1,http://arxiv.org/abs/2602.16111v1,2026_zehao-xu_surrogate-based-prevalence-measurement-for-large-scale-a-b-t_2602.161111.pdf,data\raw\papers\2026_zehao-xu_surrogate-based-prevalence-measurement-for-large-scale-a-b-t_2602.161111.pdf
2602.16110v1,OmniCT: Towards a Unified Slice-Volume LVLM for Comprehensive CT Analysis,2026,Tianwei Lin,"Tianwei Lin, Zhongwei Qiu, Wenqiao Zhang, Jiang Liu, Yihan Xie, Mingjian Gao, Zhenxuan Fan, Zhaocheng Li, Sijing Li, Zhongle Xie, Peng LU, Yueting Zhuang, Yingda Xia, Ling Zhang, Beng Chin Ooi",2026-02-18T00:42:41+00:00,2026-02-18T00:42:41+00:00,"cs.CV, cs.AI","Computed Tomography (CT) is one of the most widely used and diagnostically information-dense imaging modalities, covering critical organs such as the heart, lungs, liver, and colon. Clinical interpretation relies on both slice-driven local features (e.g., sub-centimeter nodules, lesion boundaries) and volume-driven spatial representations (e.g., tumor infiltration, inter-organ anatomical relations). However, existing Large Vision-Language Models (LVLMs) remain fragmented in CT slice versus volumetric understanding: slice-driven LVLMs show strong generalization but lack cross-slice spatial consistency, while volume-driven LVLMs explicitly capture volumetric semantics but suffer from coarse granularity and poor compatibility with slice inputs. The absence of a unified modeling paradigm constitutes a major bottleneck for the clinical translation of medical LVLMs. We present OmniCT, a powerful unified slice-volume LVLM for CT scenarios, which makes three contributions: (i) Spatial Consistency Enhancement (SCE): volumetric slice composition combined with tri-axial positional embedding that introduces volumetric consistency, and an MoE hybrid projection enables efficient slice-volume adaptation; (ii) Organ-level Semantic Enhancement (OSE): segmentation and ROI localization explicitly align anatomical regions, emphasizing lesion- and organ-level semantics; (iii) MedEval-CT: the largest slice-volume CT dataset and hybrid benchmark integrates comprehensive metrics for unified evaluation. OmniCT consistently outperforms existing methods with a substantial margin across diverse clinical tasks and satisfies both micro-level detail sensitivity and macro-level spatial reasoning. More importantly, it establishes a new paradigm for cross-modal medical imaging understanding.",https://arxiv.org/pdf/2602.16110v1,http://arxiv.org/abs/2602.16110v1,2026_tianwei-lin_omnict-towards-a-unified-slice-volume-lvlm-for-comprehensive_2602.161101.pdf,data\raw\papers\2026_tianwei-lin_omnict-towards-a-unified-slice-volume-lvlm-for-comprehensive_2602.161101.pdf
2602.16093v1,Updating Parametric Knowledge with Context Distillation Retains Post-Training Capabilities,2026,Shankar Padmanabhan,"Shankar Padmanabhan, Mustafa Omer Gul, Tanya Goyal",2026-02-17T23:49:47+00:00,2026-02-17T23:49:47+00:00,"cs.CL, cs.AI","Post-training endows pretrained LLMs with a variety of desirable skills, including instruction-following, reasoning, and others. However, these post-trained LLMs only encode knowledge up to a cut-off date, necessitating continual adaptation. Unfortunately, existing solutions cannot simultaneously learn new knowledge from an adaptation document corpora and mitigate the forgetting of earlier learned capabilities. To address this, we introduce Distillation via Split Contexts (DiSC), a simple context-distillation based approach for continual knowledge adaptation. \methodname~derives student and teacher distributions by conditioning on distinct segments of the training example and minimizes the KL divergence between the shared tokens. This allows us to efficiently apply context-distillation without requiring explicit generation steps during training. We run experiments on four post-trained models and two adaptation domains. Compared to prior finetuning and distillation methods for continual adaptation, DiSC consistently reports the best trade-off between learning new knowledge and mitigating forgetting of previously learned skills like instruction-following, reasoning, and factual knowledge.",https://arxiv.org/pdf/2602.16093v1,http://arxiv.org/abs/2602.16093v1,2026_shankar-padmanabhan_updating-parametric-knowledge-with-context-distillation-reta_2602.160931.pdf,data\raw\papers\2026_shankar-padmanabhan_updating-parametric-knowledge-with-context-distillation-reta_2602.160931.pdf
2602.16085v1,Language Statistics and False Belief Reasoning: Evidence from 41 Open-Weight LMs,2026,Sean Trott,"Sean Trott, Samuel Taylor, Cameron Jones, James A. Michaelov, Pamela D. Rivière",2026-02-17T23:20:08+00:00,2026-02-17T23:20:08+00:00,"cs.CL, cs.AI","Research on mental state reasoning in language models (LMs) has the potential to inform theories of human social cognition--such as the theory that mental state reasoning emerges in part from language exposure--and our understanding of LMs themselves. Yet much published work on LMs relies on a relatively small sample of closed-source LMs, limiting our ability to rigorously test psychological theories and evaluate LM capacities. Here, we replicate and extend published work on the false belief task by assessing LM mental state reasoning behavior across 41 open-weight models (from distinct model families). We find sensitivity to implied knowledge states in 34% of the LMs tested; however, consistent with prior work, none fully ``explain away'' the effect in humans. Larger LMs show increased sensitivity and also exhibit higher psychometric predictive power. Finally, we use LM behavior to generate and test a novel hypothesis about human cognition: both humans and LMs show a bias towards attributing false beliefs when knowledge states are cued using a non-factive verb (``John thinks...'') than when cued indirectly (``John looks in the...''). Unlike the primary effect of knowledge states, where human sensitivity exceeds that of LMs, the magnitude of the human knowledge cue effect falls squarely within the distribution of LM effect sizes-suggesting that distributional statistics of language can in principle account for the latter but not the former in humans. These results demonstrate the value of using larger samples of open-weight LMs to test theories of human cognition and evaluate LM capacities.",https://arxiv.org/pdf/2602.16085v1,http://arxiv.org/abs/2602.16085v1,2026_sean-trott_language-statistics-and-false-belief-reasoning-evidence-from_2602.160851.pdf,data\raw\papers\2026_sean-trott_language-statistics-and-false-belief-reasoning-evidence-from_2602.160851.pdf
2602.16080v1,Surgical Activation Steering via Generative Causal Mediation,2026,Aruna Sankaranarayanan,"Aruna Sankaranarayanan, Amir Zur, Atticus Geiger, Dylan Hadfield-Menell",2026-02-17T23:13:18+00:00,2026-02-17T23:13:18+00:00,"cs.CL, cs.CY, cs.HC, cs.LG","Where should we intervene in a language model (LM) to control behaviors that are diffused across many tokens of a long-form response? We introduce Generative Causal Mediation (GCM), a procedure for selecting model components, e.g., attention heads, to steer a binary concept (e.g., talk in verse vs. talk in prose) from contrastive long-form responses. In GCM, we first construct a dataset of contrasting inputs and responses. Then, we quantify how individual model components mediate the contrastive concept and select the strongest mediators for steering. We evaluate GCM on three tasks--refusal, sycophancy, and style transfer--across three language models. GCM successfully localizes concepts expressed in long-form responses and consistently outperforms correlational probe-based baselines when steering with a sparse set of attention heads. Together, these results demonstrate that GCM provides an effective approach for localizing and controlling the long-form responses of LMs.",https://arxiv.org/pdf/2602.16080v1,http://arxiv.org/abs/2602.16080v1,2026_aruna-sankaranarayanan_surgical-activation-steering-via-generative-causal-mediation_2602.160801.pdf,data\raw\papers\2026_aruna-sankaranarayanan_surgical-activation-steering-via-generative-causal-mediation_2602.160801.pdf
2602.16069v1,The Limits of Long-Context Reasoning in Automated Bug Fixing,2026,Ravi Raju,"Ravi Raju, Mengmeng Ji, Shubhangi Upasani, Bo Li, Urmish Thakker",2026-02-17T22:51:40+00:00,2026-02-17T22:51:40+00:00,"cs.SE, cs.LG","Rapidly increasing context lengths have led to the assumption that large language models (LLMs) can directly reason over entire codebases. Concurrently, recent advances in LLMs have enabled strong performance on software engineering benchmarks, particularly when paired with agentic workflows. In this work, we systematically evaluate whether current LLMs can reliably perform long-context code debugging and patch generation. Using SWE-bench Verified as a controlled experimental setting, we first evaluate state-of-the-art models within an agentic harness (mini-SWE-agent), where performance improves substantially: GPT-5-nano achieves up to a 31\% resolve rate on 100 samples, and open-source models such as Deepseek-R1-0528 obtain competitive results. However, token-level analysis shows that successful agentic trajectories typically remain under 20k tokens, and that longer accumulated contexts correlate with lower success rates, indicating that agentic success primarily arises from task decomposition into short-context steps rather than effective long-context reasoning. To directly test long-context capability, we construct a data pipeline where we artificially inflate the context length of the input by placing the relevant files into the context (ensuring perfect retrieval recall); we then study single-shot patch generation under genuinely long contexts (64k-128k tokens). Despite this setup, performance degrades sharply: Qwen3-Coder-30B-A3B achieves only a 7\% resolve rate at 64k context, while GPT-5-nano solves none of the tasks. Qualitative analysis reveals systematic failure modes, including hallucinated diffs, incorrect file targets, and malformed patch headers. Overall, our findings highlight a significant gap between nominal context length and usable context capacity in current LLMs, and suggest that existing agentic coding benchmarks do not meaningfully evaluate long-context reasoning.",https://arxiv.org/pdf/2602.16069v1,http://arxiv.org/abs/2602.16069v1,2026_ravi-raju_the-limits-of-long-context-reasoning-in-automated-bug-fixing_2602.160691.pdf,data\raw\papers\2026_ravi-raju_the-limits-of-long-context-reasoning-in-automated-bug-fixing_2602.160691.pdf
2602.16053v1,Multi-Objective Alignment of Language Models for Personalized Psychotherapy,2026,Mehrab Beikzadeh,"Mehrab Beikzadeh, Yasaman Asadollah Salmanpour, Ashima Suvarna, Sriram Sankararaman, Matteo Malgaroli, Majid Sarrafzadeh, Saadia Gabriel",2026-02-17T22:08:14+00:00,2026-02-17T22:08:14+00:00,cs.LG,"Mental health disorders affect over 1 billion people worldwide, yet access to care remains limited by workforce shortages and cost constraints. While AI systems show therapeutic promise, current alignment approaches optimize objectives independently, failing to balance patient preferences with clinical safety. We survey 335 individuals with lived mental health experience to collect preference rankings across therapeutic dimensions, then develop a multi-objective alignment framework using direct preference optimization. We train reward models for six criteria -- empathy, safety, active listening, self-motivated change, trust/rapport, and patient autonomy -- and systematically compare multi-objective approaches against single-objective optimization, supervised fine-tuning, and parameter merging. Multi-objective DPO (MODPO) achieves superior balance (77.6% empathy, 62.6% safety) compared to single-objective optimization (93.6% empathy, 47.8% safety), and therapeutic criteria outperform general communication principles by 17.2%. Blinded clinician evaluation confirms MODPO is consistently preferred, with LLM-evaluator agreement comparable to inter-clinician reliability.",https://arxiv.org/pdf/2602.16053v1,http://arxiv.org/abs/2602.16053v1,2026_mehrab-beikzadeh_multi-objective-alignment-of-language-models-for-personalize_2602.160531.pdf,data\raw\papers\2026_mehrab-beikzadeh_multi-objective-alignment-of-language-models-for-personalize_2602.160531.pdf
2602.16038v1,Heuristic Search as Language-Guided Program Optimization,2026,Mingxin Yu,"Mingxin Yu, Ruixiao Yang, Chuchu Fan",2026-02-17T21:45:42+00:00,2026-02-17T21:45:42+00:00,"cs.NE, cs.LG","Large Language Models (LLMs) have advanced Automated Heuristic Design (AHD) in combinatorial optimization (CO) in the past few years. However, existing discovery pipelines often require extensive manual trial-and-error or reliance on domain expertise to adapt to new or complex problems. This stems from tightly coupled internal mechanisms that limit systematic improvement of the LLM-driven design process. To address this challenge, we propose a structured framework for LLM-driven AHD that explicitly decomposes the heuristic discovery process into modular stages: a forward pass for evaluation, a backward pass for analytical feedback, and an update step for program refinement. This separation provides a clear abstraction for iterative refinement and enables principled improvements of individual components. We validate our framework across four diverse real-world CO domains, where it consistently outperforms baselines, achieving up to $0.17$ improvement in QYI on unseen test sets. Finally, we show that several popular AHD methods are restricted instantiations of our framework. By integrating them in our structured pipeline, we can upgrade the components modularly and significantly improve their performance.",https://arxiv.org/pdf/2602.16038v1,http://arxiv.org/abs/2602.16038v1,2026_mingxin-yu_heuristic-search-as-language-guided-program-optimization_2602.160381.pdf,data\raw\papers\2026_mingxin-yu_heuristic-search-as-language-guided-program-optimization_2602.160381.pdf
2602.15997v1,Anatomy of Capability Emergence: Scale-Invariant Representation Collapse and Top-Down Reorganization in Neural Networks,2026,Jayadev Billa,Jayadev Billa,2026-02-17T20:39:02+00:00,2026-02-17T20:39:02+00:00,"cs.LG, cs.AI, cs.CL","Capability emergence during neural network training remains mechanistically opaque. We track five geometric measures across five model scales (405K-85M parameters), 120+ emergence events in eight algorithmic tasks, and three Pythia language models (160M-2.8B). We find: (1) training begins with a universal representation collapse to task-specific floors that are scale-invariant across a 210X parameter range (e.g., modular arithmetic collapses to RANKME ~ 2.0 regardless of model size); (2) collapse propagates top-down through layers (32/32 task X model consistency), contradicting bottom-up feature-building intuition; (3) a geometric hierarchy in which representation geometry leads emergence (75-100% precursor rate for hard tasks), while the local learning coefficient is synchronous (0/24 precursor) and Hessian measures lag. We also delineate prediction limits: geometric measures encode coarse task difficulty but not fine-grained timing (within-class concordance 27%; when task ordering reverses across scales, prediction fails at 26%). On Pythia, global geometric patterns replicate but per-task precursor signals do not -- the precursor relationship requires task-training alignment that naturalistic pre-training does not provide. Our contribution is the geometric anatomy of emergence and its boundary conditions, not a prediction tool.",https://arxiv.org/pdf/2602.15997v1,http://arxiv.org/abs/2602.15997v1,2026_jayadev-billa_anatomy-of-capability-emergence-scale-invariant-representati_2602.159971.pdf,data\raw\papers\2026_jayadev-billa_anatomy-of-capability-emergence-scale-invariant-representati_2602.159971.pdf
2602.15983v1,ReLoop: Structured Modeling and Behavioral Verification for Reliable LLM-Based Optimization,2026,Junbo Jacob Lian,"Junbo Jacob Lian, Yujun Sun, Huiling Chen, Chaoyu Zhang, Chung-Piaw Teo",2026-02-17T20:20:33+00:00,2026-02-17T20:20:33+00:00,"cs.SE, cs.AI, cs.LG, math.OC","Large language models (LLMs) can translate natural language into optimization code, but silent failures pose a critical risk: code that executes and returns solver-feasible solutions may encode semantically incorrect formulations, creating a feasibility-correctness gap of up to 90 percentage points on compositional problems. We introduce ReLoop, addressing silent failures from two complementary directions. Structured generation decomposes code production into a four-stage reasoning chain (understand, formalize, synthesize, verify) that mirrors expert modeling practice, with explicit variable-type reasoning and self-verification to prevent formulation errors at their source. Behavioral verification detects errors that survive generation by testing whether the formulation responds correctly to solver-based parameter perturbation, without requiring ground truth -- an external semantic signal that bypasses the self-consistency problem inherent in LLM-based code review. The two mechanisms are complementary: structured generation dominates on complex compositional problems, while behavioral verification becomes the largest single contributor on problems with localized formulation defects. Together with execution recovery via IIS-enhanced diagnostics, ReLoop raises correctness from 22.6% to 31.1% and execution from 72.1% to 100.0% on the strongest model, with consistent gains across five models spanning three paradigms (foundation, SFT, RL) and three benchmarks. We additionally release RetailOpt-190, 190 compositional retail optimization scenarios targeting the multi-constraint interactions where LLMs most frequently fail.",https://arxiv.org/pdf/2602.15983v1,http://arxiv.org/abs/2602.15983v1,2026_junbo-jacob-lian_reloop-structured-modeling-and-behavioral-verification-for-r_2602.159831.pdf,data\raw\papers\2026_junbo-jacob-lian_reloop-structured-modeling-and-behavioral-verification-for-r_2602.159831.pdf
2602.15950v1,Can Vision-Language Models See Squares? Text-Recognition Mediates Spatial Reasoning Across Three Model Families,2026,Yuval Levental,Yuval Levental,2026-02-17T19:06:19+00:00,2026-02-17T19:06:19+00:00,"cs.CV, cs.LG","We present a simple experiment that exposes a fundamental limitation in vision-language models (VLMs): the inability to accurately localize filled cells in binary grids when those cells lack textual identity. We generate fifteen 15x15 grids with varying density (10.7%-41.8% filled cells) and render each as two image types -- text symbols (. and #) and filled squares without gridlines -- then ask three frontier VLMs (Claude Opus, ChatGPT 5.2, and Gemini 3 Thinking) to transcribe them. In the text-symbol condition, Claude and ChatGPT achieve approximately 91% cell accuracy and 84% F1, while Gemini achieves 84% accuracy and 63% F1. In the filled-squares condition, all three models collapse to 60-73% accuracy and 29-39% F1. Critically, all conditions pass through the same visual encoder -- the text symbols are images, not tokenized text. The text-vs-squares F1 gap ranges from 34 to 54 points across models, demonstrating that VLMs behave as if they possess a high-fidelity text-recognition pathway for spatial reasoning that dramatically outperforms their native visual pathway. Each model exhibits a distinct failure mode in the squares condition -- systematic under-counting (Claude), massive over-counting (ChatGPT), and template hallucination (Gemini) -- but all share the same underlying deficit: severely degraded spatial localization for non-textual visual elements.",https://arxiv.org/pdf/2602.15950v1,http://arxiv.org/abs/2602.15950v1,2026_yuval-levental_can-vision-language-models-see-squares-text-recognition-medi_2602.159501.pdf,data\raw\papers\2026_yuval-levental_can-vision-language-models-see-squares-text-recognition-medi_2602.159501.pdf
2602.15781v1,Neural Scaling Laws for Boosted Jet Tagging,2026,Matthias Vigl,"Matthias Vigl, Nicole Hartman, Michael Kagan, Lukas Heinrich",2026-02-17T18:13:01+00:00,2026-02-17T18:13:01+00:00,"hep-ex, cs.LG, hep-ph, physics.data-an","The success of Large Language Models (LLMs) has established that scaling compute, through joint increases in model capacity and dataset size, is the primary driver of performance in modern machine learning. While machine learning has long been an integral component of High Energy Physics (HEP) data analysis workflows, the compute used to train state-of-the-art HEP models remains orders of magnitude below that of industry foundation models. With scaling laws only beginning to be studied in the field, we investigate neural scaling laws for boosted jet classification using the public JetClass dataset. We derive compute optimal scaling laws and identify an effective performance limit that can be consistently approached through increased compute. We study how data repetition, common in HEP where simulation is expensive, modifies the scaling yielding a quantifiable effective dataset size gain. We then study how the scaling coefficients and asymptotic performance limits vary with the choice of input features and particle multiplicity, demonstrating that increased compute reliably drives performance toward an asymptotic limit, and that more expressive, lower-level features can raise the performance limit and improve results at fixed dataset size.",https://arxiv.org/pdf/2602.15781v1,http://arxiv.org/abs/2602.15781v1,2026_matthias-vigl_neural-scaling-laws-for-boosted-jet-tagging_2602.157811.pdf,data\raw\papers\2026_matthias-vigl_neural-scaling-laws-for-boosted-jet-tagging_2602.157811.pdf
2602.15769v1,ViTaB-A: Evaluating Multimodal Large Language Models on Visual Table Attribution,2026,Yahia Alqurnawi,"Yahia Alqurnawi, Preetom Biswas, Anmol Rao, Tejas Anvekar, Chitta Baral, Vivek Gupta",2026-02-17T18:01:35+00:00,2026-02-17T18:01:35+00:00,cs.CL,"Multimodal Large Language Models (mLLMs) are often used to answer questions in structured data such as tables in Markdown, JSON, and images. While these models can often give correct answers, users also need to know where those answers come from. In this work, we study structured data attribution/citation, which is the ability of the models to point to the specific rows and columns that support an answer. We evaluate several mLLMs across different table formats and prompting strategies. Our results show a clear gap between question answering and evidence attribution. Although question answering accuracy remains moderate, attribution accuracy is much lower, near random for JSON inputs, across all models. We also find that models are more reliable at citing rows than columns, and struggle more with textual formats than images. Finally, we observe notable differences across model families. Overall, our findings show that current mLLMs are unreliable at providing fine-grained, trustworthy attribution for structured data, which limits their usage in applications requiring transparency and traceability.",https://arxiv.org/pdf/2602.15769v1,http://arxiv.org/abs/2602.15769v1,2026_yahia-alqurnawi_vitab-a-evaluating-multimodal-large-language-models-on-visua_2602.157691.pdf,data\raw\papers\2026_yahia-alqurnawi_vitab-a-evaluating-multimodal-large-language-models-on-visua_2602.157691.pdf
2602.15725v1,Recursive Concept Evolution for Compositional Reasoning in Large Language Models,2026,Sarim Chaudhry,Sarim Chaudhry,2026-02-17T17:01:42+00:00,2026-02-17T17:01:42+00:00,"cs.AI, cs.CL, cs.LG","Large language models achieve strong performance on many complex reasoning tasks, yet their accuracy degrades sharply on benchmarks that require compositional reasoning, including ARC-AGI-2, GPQA, MATH, BBH, and HLE. Existing methods improve reasoning by expanding token-level search through chain-of-thought prompting, self-consistency, or reinforcement learning, but they leave the model's latent representation space fixed. When the required abstraction is not already encoded in this space, performance collapses. We propose Recursive Concept Evolution (RCE), a framework that enables pretrained language models to modify their internal representation geometry during inference. RCE introduces dynamically generated low-rank concept subspaces that are spawned when representational inadequacy is detected, selected through a minimum description length criterion, merged when synergistic, and consolidated via constrained optimization to preserve stability. This process allows the model to construct new abstractions rather than recombining existing ones. We integrate RCE with Mistral-7B and evaluate it across compositional reasoning benchmarks. RCE yields 12-18 point gains on ARC-AGI-2, 8-14 point improvements on GPQA and BBH, and consistent reductions in depth-induced error on MATH and HLE.",https://arxiv.org/pdf/2602.15725v1,http://arxiv.org/abs/2602.15725v1,2026_sarim-chaudhry_recursive-concept-evolution-for-compositional-reasoning-in-l_2602.157251.pdf,data\raw\papers\2026_sarim-chaudhry_recursive-concept-evolution-for-compositional-reasoning-in-l_2602.157251.pdf
2602.15724v1,Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation,2026,Shutian Gu,"Shutian Gu, Chengkai Huang, Ruoyu Wang, Lina Yao",2026-02-17T17:00:11+00:00,2026-02-17T17:00:11+00:00,"cs.CV, cs.AI","Vision-and-Language Navigation (VLN) requires an agent to follow natural-language instructions and navigate through previously unseen environments. Recent approaches increasingly employ large language models (LLMs) as high-level navigators due to their flexibility and reasoning capability. However, prompt-based LLM navigation often suffers from inefficient decision-making, as the model must repeatedly interpret instructions from scratch and reason over noisy and verbose navigable candidates at each step. In this paper, we propose a retrieval-augmented framework to improve the efficiency and stability of LLM-based VLN without modifying or fine-tuning the underlying language model. Our approach introduces retrieval at two complementary levels. At the episode level, an instruction-level embedding retriever selects semantically similar successful navigation trajectories as in-context exemplars, providing task-specific priors for instruction grounding. At the step level, an imitation-learned candidate retriever prunes irrelevant navigable directions before LLM inference, reducing action ambiguity and prompt complexity. Both retrieval modules are lightweight, modular, and trained independently of the LLM. We evaluate our method on the Room-to-Room (R2R) benchmark. Experimental results demonstrate consistent improvements in Success Rate, Oracle Success Rate, and SPL on both seen and unseen environments. Ablation studies further show that instruction-level exemplar retrieval and candidate pruning contribute complementary benefits to global guidance and step-wise decision efficiency. These results indicate that retrieval-augmented decision support is an effective and scalable strategy for enhancing LLM-based vision-and-language navigation.",https://arxiv.org/pdf/2602.15724v1,http://arxiv.org/abs/2602.15724v1,2026_shutian-gu_learning-to-retrieve-navigable-candidates-for-efficient-visi_2602.157241.pdf,data\raw\papers\2026_shutian-gu_learning-to-retrieve-navigable-candidates-for-efficient-visi_2602.157241.pdf
2602.15620v2,STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens,2026,Shiqi Liu,"Shiqi Liu, Zeyu He, Guojian Zhan, Letian Tao, Zhilong Zheng, Jiang Wu, Yinuo Wang, Yang Guan, Kehua Sheng, Bo Zhang, Keqiang Li, Jingliang Duan, Shengbo Eben Li",2026-02-17T14:46:48+00:00,2026-02-18T14:13:03+00:00,"cs.CL, cs.AI","Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often suffer from late-stage performance collapse, leading to degraded reasoning quality and unstable training. Our analysis shows that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. We find that training instability can be caused by a tiny fraction of tokens, approximately 0.01\%, which we term \emph{spurious tokens}. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates. To mitigate this instability, we design S2T (silencing spurious tokens) mechanism to efficiently identify spurious tokens through characteristic signals with low probability, low entropy, and positive advantage, and then to suppress their gradient perturbations during optimization. Incorporating this mechanism into a group-based objective, we propose Spurious-Token-Aware Policy Optimization (STAPO), which promotes stable and effective large-scale model refinement. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13\% ($ρ_{\mathrm{T}}$=1.0, top-p=1.0) and 3.69\% ($ρ_{\mathrm{T}}$=0.7, top-p=0.9) over GRPO, 20-Entropy and JustRL.",https://arxiv.org/pdf/2602.15620v2,http://arxiv.org/abs/2602.15620v2,2026_shiqi-liu_stapo-stabilizing-reinforcement-learning-for-llms-by-silenci_2602.156202.pdf,data\raw\papers\2026_shiqi-liu_stapo-stabilizing-reinforcement-learning-for-llms-by-silenci_2602.156202.pdf
2602.15600v1,The geometry of online conversations and the causal antecedents of conflictual discourse,2026,Carlo Santagiustina,"Carlo Santagiustina, Caterina Cruciani",2026-02-17T14:12:03+00:00,2026-02-17T14:12:03+00:00,"cs.SI, cs.AI, econ.EM, stat.AP","This article investigates the causal antecedents of conflictual language and the geometry of interaction in online threaded conversations related to climate change. We employ three annotation dimensions, inferred through LLM prompting and averaging, to capture complementary aspects of discursive conflict (such as stance: agreement vs disagreement; tone: attacking vs respectful; and emotional versus factual framing) and use data from a threaded online forum to examine how these dimensions respond to temporal, conversational, and arborescent structural features of discussions. We show that, as suggested by the literature, longer delays between successive posts in a thread are associated with replies that are, on average, more respectful, whereas longer delays relative to the parent post are associated with slightly less disagreement but more emotional (less factual) language. Second, we characterize alignment with the local conversational environment and find strong convergence both toward the average stance, tone and emotional framing of older sibling posts replying to the same parent and toward those of the parent post itself, with parent post effects generally stronger than sibling effects. We further show that early branch-level responses condition these alignment dynamics, such that parent-child stance alignment is amplified or attenuated depending on whether a branch is initiated in agreement or disagreement with the discussion's root message. These influences are largely additive for civility-related dimensions (attacking vs respectful, disagree vs agree), whereas for emotional versus factual framing there is a significant interaction: alignment with the parent's emotionality is amplified when older siblings are similarly aligned.",https://arxiv.org/pdf/2602.15600v1,http://arxiv.org/abs/2602.15600v1,2026_carlo-santagiustina_the-geometry-of-online-conversations-and-the-causal-antecede_2602.156001.pdf,data\raw\papers\2026_carlo-santagiustina_the-geometry-of-online-conversations-and-the-causal-antecede_2602.156001.pdf
2602.15564v1,Beyond Static Pipelines: Learning Dynamic Workflows for Text-to-SQL,2026,Yihan Wang,"Yihan Wang, Peiyu Liu, Runyu Chen, Wei Xu",2026-02-17T13:24:56+00:00,2026-02-17T13:24:56+00:00,"cs.CL, cs.AI","Text-to-SQL has recently achieved impressive progress, yet remains difficult to apply effectively in real-world scenarios. This gap stems from the reliance on single static workflows, fundamentally limiting scalability to out-of-distribution and long-tail scenarios. Instead of requiring users to select suitable methods through extensive experimentation, we attempt to enable systems to adaptively construct workflows at inference time. Through theoretical and empirical analysis, we demonstrate that optimal dynamic policies consistently outperform the best static workflow, with performance gains fundamentally driven by heterogeneity across candidate workflows. Motivated by this, we propose SquRL, a reinforcement learning framework that enhances LLMs' reasoning capability in adaptive workflow construction. We design a rule-based reward function and introduce two effective training mechanisms: dynamic actor masking to encourage broader exploration, and pseudo rewards to improve training efficiency. Experiments on widely-used Text-to-SQL benchmarks demonstrate that dynamic workflow construction consistently outperforms the best static workflow methods, with especially pronounced gains on complex and out-of-distribution queries. The codes are available at https://github.com/Satissss/SquRL",https://arxiv.org/pdf/2602.15564v1,http://arxiv.org/abs/2602.15564v1,2026_yihan-wang_beyond-static-pipelines-learning-dynamic-workflows-for-text-_2602.155641.pdf,data\raw\papers\2026_yihan-wang_beyond-static-pipelines-learning-dynamic-workflows-for-text-_2602.155641.pdf
2602.15531v1,"GenAI-LA: Generative AI and Learning Analytics Workshop (LAK 2026), April 27--May 1, 2026, Bergen, Norway",2026,Javier Irigoyen,"Javier Irigoyen, Roberto Daza, Aythami Morales, Julian Fierrez, Francisco Jurado, Alvaro Ortigosa, Ruben Tolosana",2026-02-17T12:11:49+00:00,2026-02-17T12:11:49+00:00,"cs.AI, cs.DB","This work introduces EduEVAL-DB, a dataset based on teacher roles designed to support the evaluation and training of automatic pedagogical evaluators and AI tutors for instructional explanations. The dataset comprises 854 explanations corresponding to 139 questions from a curated subset of the ScienceQA benchmark, spanning science, language, and social science across K-12 grade levels. For each question, one human-teacher explanation is provided and six are generated by LLM-simulated teacher roles. These roles are inspired by instructional styles and shortcomings observed in real educational practice and are instantiated via prompt engineering. We further propose a pedagogical risk rubric aligned with established educational standards, operationalizing five complementary risk dimensions: factual correctness, explanatory depth and completeness, focus and relevance, student-level appropriateness, and ideological bias. All explanations are annotated with binary risk labels through a semi-automatic process with expert teacher review. Finally, we present preliminary validation experiments to assess the suitability of EduEVAL-DB for evaluation. We benchmark a state-of-the-art education-oriented model (Gemini 2.5 Pro) against a lightweight local Llama 3.1 8B model and examine whether supervised fine-tuning on EduEVAL-DB supports pedagogical risk detection using models deployable on consumer hardware.",https://arxiv.org/pdf/2602.15531v1,http://arxiv.org/abs/2602.15531v1,2026_javier-irigoyen_genai-la-generative-ai-and-learning-analytics-workshop-lak-2_2602.155311.pdf,data\raw\papers\2026_javier-irigoyen_genai-la-generative-ai-and-learning-analytics-workshop-lak-2_2602.155311.pdf
2602.15521v1,ExpertWeaver: Unlocking the Inherent MoE in Dense LLMs with GLU Activation Patterns,2026,Ziyu Zhao,"Ziyu Zhao, Tong Zhu, Zhi Zhang, Tiantian Fan, Jinluan Yang, Kun Kuang, Zhongyu Wei, Fei Wu, Yu Cheng",2026-02-17T11:50:58+00:00,2026-02-17T11:50:58+00:00,"cs.CL, cs.LG","Mixture-of-Experts (MoE) effectively scales model capacity while preserving computational efficiency through sparse expert activation. However, training high-quality MoEs from scratch is prohibitively expensive. A promising alternative is to convert pretrained dense models into sparse MoEs. Existing dense-to-MoE methods fall into two categories: \textbf{dynamic structural pruning} that converts dense models into MoE architectures with moderate sparsity to balance performance and inference efficiency, and \textbf{downcycling} approaches that use pretrained dense models to initialize highly sparse MoE architectures. However, existing methods break the intrinsic activation patterns within dense models, leading to suboptimal expert construction. In this work, we argue that the Gated Linear Unit (GLU) mechanism provides a natural blueprint for dense-to-MoE conversion. We show that the fine-grained neural-wise activation patterns of GLU reveal a coarse-grained structure, uncovering an inherent MoE architecture composed of consistently activated universal neurons and dynamically activated specialized neurons. Leveraging this discovery, we introduce ExpertWeaver, a training-free framework that partitions neurons according to their activation patterns and constructs shared experts and specialized routed experts with layer-adaptive configurations. Our experiments demonstrate that ExpertWeaver significantly outperforms existing methods, both as a training-free dynamic structural pruning technique and as a downcycling strategy for superior MoE initialization.",https://arxiv.org/pdf/2602.15521v1,http://arxiv.org/abs/2602.15521v1,2026_ziyu-zhao_expertweaver-unlocking-the-inherent-moe-in-dense-llms-with-g_2602.155211.pdf,data\raw\papers\2026_ziyu-zhao_expertweaver-unlocking-the-inherent-moe-in-dense-llms-with-g_2602.155211.pdf
2602.15509v1,Fine-Refine: Iterative Fine-grained Refinement for Mitigating Dialogue Hallucination,2026,Xiangyan Chen,"Xiangyan Chen, Yujian Gan, Matthew Purver",2026-02-17T11:33:23+00:00,2026-02-17T11:33:23+00:00,cs.CL,"The tendency for hallucination in current large language models (LLMs) negatively impacts dialogue systems. Such hallucinations produce factually incorrect responses that may mislead users and undermine system trust. Existing refinement methods for dialogue systems typically operate at the response level, overlooking the fact that a single response may contain multiple verifiable or unverifiable facts. To address this gap, we propose Fine-Refine, a fine-grained refinement framework that decomposes responses into atomic units, verifies each unit using external knowledge, assesses fluency via perplexity, and iteratively corrects granular errors. We evaluate factuality across the HybriDialogue and OpendialKG datasets in terms of factual accuracy (fact score) and coverage (Not Enough Information Proportion), and experiments show that Fine-Refine substantially improves factuality, achieving up to a 7.63-point gain in dialogue fact score, with a small trade-off in dialogue quality.",https://arxiv.org/pdf/2602.15509v1,http://arxiv.org/abs/2602.15509v1,2026_xiangyan-chen_fine-refine-iterative-fine-grained-refinement-for-mitigating_2602.155091.pdf,data\raw\papers\2026_xiangyan-chen_fine-refine-iterative-fine-grained-refinement-for-mitigating_2602.155091.pdf
2602.15508v1,Eco-Amazon: Enriching E-commerce Datasets with Product Carbon Footprint for Sustainable Recommendations,2026,Giuseppe Spillo,"Giuseppe Spillo, Allegra De Filippo, Cataldo Musto, Michela Milano, Giovanni Semeraro",2026-02-17T11:30:11+00:00,2026-02-17T11:30:11+00:00,cs.IR,"In the era of responsible and sustainable AI, information retrieval and recommender systems must expand their scope beyond traditional accuracy metrics to incorporate environmental sustainability. However, this research line is severely limited by the lack of item-level environmental impact data in standard benchmarks. This paper introduces Eco-Amazon, a novel resource designed to bridge this gap. Our resource consists of an enriched version of three widely used Amazon datasets (i.e., Home, Clothing, and Electronics) augmented with Product Carbon Footprint (PCF) metadata. CO2e emission scores were generated using a zero-shot framework that leverages Large Language Models (LLMs) to estimate item-level PCF based on product attributes. Our contribution is three-fold: (i) the release of the Eco-Amazon datasets, enriching item metadata with PCF signals; (ii) the LLM-based PCF estimation script, which allows researchers to enrich any product catalogue and reproduce our results; (iii) a use case demonstrating how PCF estimates can be exploited to promote more sustainable products. By providing these environmental signals, Eco-Amazon enables the community to develop, benchmark, and evaluate the next generation of sustainable retrieval and recommendation models. Our resource is available at https://doi.org/10.5281/zenodo.18549130, while our source code is available at: http://github.com/giuspillo/EcoAmazon/.",https://arxiv.org/pdf/2602.15508v1,http://arxiv.org/abs/2602.15508v1,2026_giuseppe-spillo_eco-amazon-enriching-e-commerce-datasets-with-product-carbon_2602.155081.pdf,data\raw\papers\2026_giuseppe-spillo_eco-amazon-enriching-e-commerce-datasets-with-product-carbon_2602.155081.pdf
2602.15460v1,On the Out-of-Distribution Generalization of Reasoning in Multimodal LLMs for Simple Visual Planning Tasks,2026,Yannic Neuhaus,"Yannic Neuhaus, Nicolas Flammarion, Matthias Hein, Francesco Croce",2026-02-17T09:51:40+00:00,2026-02-17T09:51:40+00:00,"cs.LG, cs.CV","Integrating reasoning in large language models and large vision-language models has recently led to significant improvement of their capabilities. However, the generalization of reasoning models is still vaguely defined and poorly understood. In this work, we present an evaluation framework to rigorously examine how well chain-of-thought (CoT) approaches generalize on a simple planning task. Specifically, we consider a grid-based navigation task in which a model is provided with a map and must output a sequence of moves that guides a player from a start position to a goal while avoiding obstacles. The versatility of the task and its data allows us to fine-tune model variants using different input representations (visual and textual) and CoT reasoning strategies, and systematically evaluate them under both in-distribution (ID) and out-of-distribution (OOD) test conditions. Our experiments show that, while CoT reasoning improves in-distribution generalization across all representations, out-of-distribution generalization (e.g., to larger maps) remains very limited in most cases when controlling for trivial matches with the ID data. Surprisingly, we find that reasoning traces which combine multiple text formats yield the best (and non-trivial) OOD generalization. Finally, purely text-based models consistently outperform those utilizing image-based inputs, including a recently proposed approach relying on latent space reasoning.",https://arxiv.org/pdf/2602.15460v1,http://arxiv.org/abs/2602.15460v1,2026_yannic-neuhaus_on-the-out-of-distribution-generalization-of-reasoning-in-mu_2602.154601.pdf,data\raw\papers\2026_yannic-neuhaus_on-the-out-of-distribution-generalization-of-reasoning-in-mu_2602.154601.pdf
2602.15456v1,"In Agents We Trust, but Who Do Agents Trust? Latent Source Preferences Steer LLM Generations",2026,Mohammad Aflah Khan,"Mohammad Aflah Khan, Mahsa Amani, Soumi Das, Bishwamittra Ghosh, Qinyuan Wu, Krishna P. Gummadi, Manish Gupta, Abhilasha Ravichander",2026-02-17T09:45:22+00:00,2026-02-17T09:45:22+00:00,cs.CL,"Agents based on Large Language Models (LLMs) are increasingly being deployed as interfaces to information on online platforms. These agents filter, prioritize, and synthesize information retrieved from the platforms' back-end databases or via web search. In these scenarios, LLM agents govern the information users receive, by drawing users' attention to particular instances of retrieved information at the expense of others. While much prior work has focused on biases in the information LLMs themselves generate, less attention has been paid to the factors that influence what information LLMs select and present to users. We hypothesize that when information is attributed to specific sources (e.g., particular publishers, journals, or platforms), current LLMs exhibit systematic latent source preferences- that is, they prioritize information from some sources over others. Through controlled experiments on twelve LLMs from six model providers, spanning both synthetic and real-world tasks, we find that several models consistently exhibit strong and predictable source preferences. These preferences are sensitive to contextual framing, can outweigh the influence of content itself, and persist despite explicit prompting to avoid them. They also help explain phenomena such as the observed left-leaning skew in news recommendations in prior work. Our findings advocate for deeper investigation into the origins of these preferences, as well as for mechanisms that provide users with transparency and control over the biases guiding LLM-powered agents.",https://arxiv.org/pdf/2602.15456v1,http://arxiv.org/abs/2602.15456v1,2026_mohammad-aflah-khan_in-agents-we-trust-but-who-do-agents-trust-latent-source-pre_2602.154561.pdf,data\raw\papers\2026_mohammad-aflah-khan_in-agents-we-trust-but-who-do-agents-trust-latent-source-pre_2602.154561.pdf
2602.15449v1,TAROT: Test-driven and Capability-adaptive Curriculum Reinforcement Fine-tuning for Code Generation with Large Language Models,2026,Chansung Park,"Chansung Park, Juyong Jiang, Fan Wang, Sayak Paul, Jiasi Shen, Jing Tang, Jianguo Li",2026-02-17T09:29:18+00:00,2026-02-17T09:29:18+00:00,"cs.CL, cs.LG, cs.SE","Large Language Models (LLMs) are changing the coding paradigm, known as vibe coding, yet synthesizing algorithmically sophisticated and robust code still remains a critical challenge. Incentivizing the deep reasoning capabilities of LLMs is essential to overcoming this hurdle. Reinforcement Fine-Tuning (RFT) has emerged as a promising strategy to address this need. However, most existing approaches overlook the heterogeneous difficulty and granularity inherent in test cases, leading to an imbalanced distribution of reward signals and consequently biased gradient updates during training. To address this, we propose Test-driven and cApability-adaptive cuRriculum reinfOrcement fine-Tuning (TAROT). TAROT systematically constructs, for each problem, a four-tier test suite (basic, intermediate, complex, edge), providing a controlled difficulty landscape for curriculum design and evaluation. Crucially, TAROT decouples curriculum progression from raw reward scores, enabling capability-conditioned evaluation and principled selection from a portfolio of curriculum policies rather than incidental test-case difficulty composition. This design fosters stable optimization and more efficient competency acquisition. Extensive experimental results reveal that the optimal curriculum for RFT in code generation is closely tied to a model's inherent capability, with less capable models achieving greater gains with an easy-to-hard progression, whereas more competent models excel under a hard-first curriculum. TAROT provides a reproducible method that adaptively tailors curriculum design to a model's capability, thereby consistently improving the functional correctness and robustness of the generated code. All code and data are released to foster reproducibility and advance community research at https://github.com/deep-diver/TAROT.",https://arxiv.org/pdf/2602.15449v1,http://arxiv.org/abs/2602.15449v1,2026_chansung-park_tarot-test-driven-and-capability-adaptive-curriculum-reinfor_2602.154491.pdf,data\raw\papers\2026_chansung-park_tarot-test-driven-and-capability-adaptive-curriculum-reinfor_2602.154491.pdf
2602.15378v1,Making Large Language Models Speak Tulu: Structured Prompting for an Extremely Low-Resource Language,2026,Prathamesh Devadiga,"Prathamesh Devadiga, Paras Chopra",2026-02-17T06:20:09+00:00,2026-02-17T06:20:09+00:00,cs.CL,"Can large language models converse in languages virtually absent from their training data? We investigate this question through a case study on Tulu, a Dravidian language with over 2 million speakers but minimal digital presence. Rather than fine-tuning an LLM, we examine whether structured prompts alone can elicit basic conversational ability under controlled prompting. We systematically tackle various challenges posed by absence of training data for Tulu by combining explicit grammar documentation, negative constraints to suppress high-probability tokens from related languages, romanization standardization, and quality-controlled synthetic data generation via self-play. Evaluated on a manually curated held-out set across three LLMs (Gemini 2.0 Flash, GPT-4o, Llama 3.1 70B) and validated by native speakers, our approach reduces vocabulary contamination from 80% to 5% while achieving 85% grammatical accuracy. Cross-model analysis reveals that negative constraints provide consistent improvements (12--18 percentage points), while grammar documentation effects vary by model architecture (8--22 points).",https://arxiv.org/pdf/2602.15378v1,http://arxiv.org/abs/2602.15378v1,2026_prathamesh-devadiga_making-large-language-models-speak-tulu-structured-prompting_2602.153781.pdf,data\raw\papers\2026_prathamesh-devadiga_making-large-language-models-speak-tulu-structured-prompting_2602.153781.pdf
2602.15359v1,Semantics-Aware Denoising: A PLM-Guided Sample Reweighting Strategy for Robust Recommendation,2026,Xikai Yang,"Xikai Yang, Yang Wang, Yilin Li, Sebastian Sun",2026-02-17T04:58:21+00:00,2026-02-17T04:58:21+00:00,cs.IR,"Implicit feedback, such as user clicks, serves as the primary data source for modern recommender systems. However, click interactions inherently contain substantial noise, including accidental clicks, clickbait-induced interactions, and exploratory browsing behaviors that do not reflect genuine user preferences. Training recommendation models with such noisy positive samples leads to degraded prediction accuracy and unreliable recommendations. In this paper, we propose SAID (Semantics-Aware Implicit Denoising), a simple yet effective framework that leverages semantic consistency between user interests and item content to identify and downweight potentially noisy interactions. Our approach constructs textual user interest profiles from historical behaviors and computes semantic similarity with target item descriptions using pre-trained language model (PLM) based text encoders. The similarity scores are then transformed into sample weights that modulate the training loss, effectively reducing the impact of semantically inconsistent clicks. Unlike existing denoising methods that require complex auxiliary networks or multi-stage training procedures, SAID only modifies the loss function while keeping the backbone recommendation model unchanged. Extensive experiments on two real-world datasets demonstrate that SAID consistently improves recommendation performance, achieving up to 2.2% relative improvement in AUC over strong baselines, with particularly notable robustness under high noise conditions.",https://arxiv.org/pdf/2602.15359v1,http://arxiv.org/abs/2602.15359v1,2026_xikai-yang_semantics-aware-denoising-a-plm-guided-sample-reweighting-st_2602.153591.pdf,data\raw\papers\2026_xikai-yang_semantics-aware-denoising-a-plm-guided-sample-reweighting-st_2602.153591.pdf
2602.15353v1,NeuroSymActive: Differentiable Neural-Symbolic Reasoning with Active Exploration for Knowledge Graph Question Answering,2026,Rong Fu,"Rong Fu, Yang Li, Zeyu Zhang, Jiekai Wu, Yaohua Liu, Shuaishuai Cao, Yangchen Zeng, Yuhang Zhang, Xiaojing Du, Chuang Zhao, Kangning Cui, Simon Fong",2026-02-17T04:47:29+00:00,2026-02-17T04:47:29+00:00,"cs.CL, cs.AI","Large pretrained language models and neural reasoning systems have advanced many natural language tasks, yet they remain challenged by knowledge-intensive queries that require precise, structured multi-hop inference. Knowledge graphs provide a compact symbolic substrate for factual grounding, but integrating graph structure with neural models is nontrivial: naively embedding graph facts into prompts leads to inefficiency and fragility, while purely symbolic or search-heavy approaches can be costly in retrievals and lack gradient-based refinement. We introduce NeuroSymActive, a modular framework that combines a differentiable neural-symbolic reasoning layer with an active, value-guided exploration controller for Knowledge Graph Question Answering. The method couples soft-unification style symbolic modules with a neural path evaluator and a Monte-Carlo style exploration policy that prioritizes high-value path expansions. Empirical results on standard KGQA benchmarks show that NeuroSymActive attains strong answer accuracy while reducing the number of expensive graph lookups and model calls compared to common retrieval-augmented baselines.",https://arxiv.org/pdf/2602.15353v1,http://arxiv.org/abs/2602.15353v1,2026_rong-fu_neurosymactive-differentiable-neural-symbolic-reasoning-with_2602.153531.pdf,data\raw\papers\2026_rong-fu_neurosymactive-differentiable-neural-symbolic-reasoning-with_2602.153531.pdf
2602.15338v1,Discovering Implicit Large Language Model Alignment Objectives,2026,Edward Chen,"Edward Chen, Sanmi Koyejo, Carlos Guestrin",2026-02-17T03:58:55+00:00,2026-02-17T03:58:55+00:00,"cs.LG, cs.CL","Large language model (LLM) alignment relies on complex reward signals that often obscure the specific behaviors being incentivized, creating critical risks of misalignment and reward hacking. Existing interpretation methods typically rely on pre-defined rubrics, risking the omission of ""unknown unknowns"", or fail to identify objectives that comprehensively cover and are causal to the model behavior. To address these limitations, we introduce Obj-Disco, a framework that automatically decomposes an alignment reward signal into a sparse, weighted combination of human-interpretable natural language objectives. Our approach utilizes an iterative greedy algorithm to analyze behavioral changes across training checkpoints, identifying and validating candidate objectives that best explain the residual reward signal. Extensive evaluations across diverse tasks, model sizes, and alignment algorithms demonstrate the framework's robustness. Experiments with popular open-source reward models show that the framework consistently captures > 90% of reward behavior, a finding further corroborated by human evaluation. Additionally, a case study on alignment with an open-source reward model reveals that Obj-Disco can successfully identify latent misaligned incentives that emerge alongside intended behaviors. Our work provides a crucial tool for uncovering the implicit objectives in LLM alignment, paving the way for more transparent and safer AI development.",https://arxiv.org/pdf/2602.15338v1,http://arxiv.org/abs/2602.15338v1,2026_edward-chen_discovering-implicit-large-language-model-alignment-objectiv_2602.153381.pdf,data\raw\papers\2026_edward-chen_discovering-implicit-large-language-model-alignment-objectiv_2602.153381.pdf
2602.15332v1,Directional Reasoning Trajectory Change (DRTC): Identifying Critical Trace Segments in Reasoning Models,2026,Waldemar Chang,Waldemar Chang,2026-02-17T03:38:16+00:00,2026-02-17T03:38:16+00:00,cs.LG,"Understanding how language models carry out long-horizon reasoning remains an open challenge. Existing interpretability methods often highlight tokens or spans correlated with an answer, but they rarely reveal where the model makes consequential reasoning turns, which earlier context causally triggers those turns, or whether the highlighted text actually steers the reasoning process. We introduce Directional Reasoning Trajectory Change (DRTC), a process-causal framework for interpreting long-form reasoning from a single on-policy rollout. DRTC detects pivot decision points using uncertainty and distribution-shift signals, then applies receiver-side interventions that preserve the realized rollout without resampling the continuation while blocking information flow from selected earlier chunks only at a pivot. It measures whether each intervention redirects the direction of the model's log-probability trajectory relative to the realized rollout direction, producing a signed per-chunk attribution score. We also compute turning-angle curvature changes on raw logits as a complementary diagnostic and introduce curvature signatures to summarize shared intervention-response geometry. Empirically, directional influence is sharply concentrated across four reasoning models (per-example |DRTC| shares yield Gini 0.50 to 0.58 and top-5 percent mass 0.23 to 0.28), and learned pivots induce stronger intervention magnitudes than matched random spans. In a scaling study on 500 MATH problems with R1-Distill-Qwen-1.5B, learned spans outperform matched random spans (median delta = 0.409, 355 of 500 positive; sign test p = 2.3e-21). Overall, DRTC provides a causally grounded, trajectory-level view of how specific context elements steer reasoning under on-policy dynamics.",https://arxiv.org/pdf/2602.15332v1,http://arxiv.org/abs/2602.15332v1,2026_waldemar-chang_directional-reasoning-trajectory-change-drtc-identifying-cri_2602.153321.pdf,data\raw\papers\2026_waldemar-chang_directional-reasoning-trajectory-change-drtc-identifying-cri_2602.153321.pdf
2602.15327v1,Prescriptive Scaling Reveals the Evolution of Language Model Capabilities,2026,Hanlin Zhang,"Hanlin Zhang, Jikai Jin, Vasilis Syrgkanis, Sham Kakade",2026-02-17T03:13:51+00:00,2026-02-17T03:13:51+00:00,"cs.LG, cs.AI, cs.CL, stat.ML","For deploying foundation models, practitioners increasingly need prescriptive scaling laws: given a pre training compute budget, what downstream accuracy is attainable with contemporary post training practice, and how stable is that mapping as the field evolves? Using large scale observational evaluations with 5k observational and 2k newly sampled data on model performance, we estimate capability boundaries, high conditional quantiles of benchmark scores as a function of log pre training FLOPs, via smoothed quantile regression with a monotone, saturating sigmoid parameterization. We validate the temporal reliability by fitting on earlier model generations and evaluating on later releases. Across various tasks, the estimated boundaries are mostly stable, with the exception of math reasoning that exhibits a consistently advancing boundary over time. We then extend our approach to analyze task dependent saturation and to probe contamination related shifts on math reasoning tasks. Finally, we introduce an efficient algorithm that recovers near full data frontiers using roughly 20% of evaluation budget. Together, our work releases the Proteus 2k, the latest model performance evaluation dataset, and introduces a practical methodology for translating compute budgets into reliable performance expectations and for monitoring when capability boundaries shift across time.",https://arxiv.org/pdf/2602.15327v1,http://arxiv.org/abs/2602.15327v1,2026_hanlin-zhang_prescriptive-scaling-reveals-the-evolution-of-language-model_2602.153271.pdf,data\raw\papers\2026_hanlin-zhang_prescriptive-scaling-reveals-the-evolution-of-language-model_2602.153271.pdf
2602.15323v1,Unforgeable Watermarks for Language Models via Robust Signatures,2026,Huijia Lin,"Huijia Lin, Kameron Shahabi, Min Jae Song",2026-02-17T03:09:06+00:00,2026-02-17T03:09:06+00:00,"cs.CR, cs.AI, cs.LG","Language models now routinely produce text that is difficult to distinguish from human writing, raising the need for robust tools to verify content provenance. Watermarking has emerged as a promising countermeasure, with existing work largely focused on model quality preservation and robust detection. However, current schemes provide limited protection against false attribution. We strengthen the notion of soundness by introducing two novel guarantees: unforgeability and recoverability. Unforgeability prevents adversaries from crafting false positives, texts that are far from any output from the watermarked model but are nonetheless flagged as watermarked. Recoverability provides an additional layer of protection: whenever a watermark is detected, the detector identifies the source text from which the flagged content was derived. Together, these properties strengthen content ownership by linking content exclusively to its generating model, enabling secure attribution and fine-grained traceability. We construct the first undetectable watermarking scheme that is robust, unforgeable, and recoverable with respect to substitutions (i.e., perturbations in Hamming metric). The key technical ingredient is a new cryptographic primitive called robust (or recoverable) digital signatures, which allow verification of messages that are close to signed ones, while preventing forgery of messages that are far from all previously signed messages. We show that any standard digital signature scheme can be boosted to a robust one using property-preserving hash functions (Boyle, LaVigne, and Vaikuntanathan, ITCS 2019).",https://arxiv.org/pdf/2602.15323v1,http://arxiv.org/abs/2602.15323v1,2026_huijia-lin_unforgeable-watermarks-for-language-models-via-robust-signat_2602.153231.pdf,data\raw\papers\2026_huijia-lin_unforgeable-watermarks-for-language-models-via-robust-signat_2602.153231.pdf
2602.15322v1,On Surprising Effectiveness of Masking Updates in Adaptive Optimizers,2026,Taejong Joo,"Taejong Joo, Wenhan Xia, Cheolmin Kim, Ming Zhang, Eugene Ie",2026-02-17T02:57:12+00:00,2026-02-17T02:57:12+00:00,"cs.LG, cs.AI","Training large language models (LLMs) relies almost exclusively on dense adaptive optimizers with increasingly sophisticated preconditioners. We challenge this by showing that randomly masking parameter updates can be highly effective, with a masked variant of RMSProp consistently outperforming recent state-of-the-art optimizers. Our analysis reveals that the random masking induces a curvature-dependent geometric regularization that smooths the optimization trajectory. Motivated by this finding, we introduce Momentum-aligned gradient masking (Magma), which modulates the masked updates using momentum-gradient alignment. Extensive LLM pre-training experiments show that Magma is a simple drop-in replacement for adaptive optimizers with consistent gains and negligible computational overhead. Notably, for the 1B model size, Magma reduces perplexity by over 19\% and 9\% compared to Adam and Muon, respectively.",https://arxiv.org/pdf/2602.15322v1,http://arxiv.org/abs/2602.15322v1,2026_taejong-joo_on-surprising-effectiveness-of-masking-updates-in-adaptive-o_2602.153221.pdf,data\raw\papers\2026_taejong-joo_on-surprising-effectiveness-of-masking-updates-in-adaptive-o_2602.153221.pdf
2602.15312v1,Extracting Consumer Insight from Text: A Large Language Model Approach to Emotion and Evaluation Measurement,2026,Stephan Ludwig,"Stephan Ludwig, Peter J. Danaher, Xiaohao Yang, Yu-Ting Lin, Ehsan Abedin, Dhruv Grewal, Lan Du",2026-02-17T02:33:51+00:00,2026-02-17T02:33:51+00:00,"cs.CL, econ.EM","Accurately measuring consumer emotions and evaluations from unstructured text remains a core challenge for marketing research and practice. This study introduces the Linguistic eXtractor (LX), a fine-tuned, large language model trained on consumer-authored text that also has been labeled with consumers' self-reported ratings of 16 consumption-related emotions and four evaluation constructs: trust, commitment, recommendation, and sentiment. LX consistently outperforms leading models, including GPT-4 Turbo, RoBERTa, and DeepSeek, achieving 81% macro-F1 accuracy on open-ended survey responses and greater than 95% accuracy on third-party-annotated Amazon and Yelp reviews. An application of LX to online retail data, using seemingly unrelated regression, affirms that review-expressed emotions predict product ratings, which in turn predict purchase behavior. Most emotional effects are mediated by product ratings, though some emotions, such as discontent and peacefulness, influence purchase directly, indicating that emotional tone provides meaningful signals beyond star ratings. To support its use, a no-code, cost-free, LX web application is available, enabling scalable analyses of consumer-authored text. In establishing a new methodological foundation for consumer perception measurement, this research demonstrates new methods for leveraging large language models to advance marketing research and practice, thereby achieving validated detection of marketing constructs from consumer data.",https://arxiv.org/pdf/2602.15312v1,http://arxiv.org/abs/2602.15312v1,2026_stephan-ludwig_extracting-consumer-insight-from-text-a-large-language-model_2602.153121.pdf,data\raw\papers\2026_stephan-ludwig_extracting-consumer-insight-from-text-a-large-language-model_2602.153121.pdf
2602.15278v1,Visual Persuasion: What Influences Decisions of Vision-Language Models?,2026,Manuel Cherep,"Manuel Cherep, Pranav M R, Pattie Maes, Nikhil Singh",2026-02-17T00:33:53+00:00,2026-02-17T00:33:53+00:00,"cs.CV, cs.AI","The web is littered with images, once created for human consumption and now increasingly interpreted by agents using vision-language models (VLMs). These agents make visual decisions at scale, deciding what to click, recommend, or buy. Yet, we know little about the structure of their visual preferences. We introduce a framework for studying this by placing VLMs in controlled image-based choice tasks and systematically perturbing their inputs. Our key idea is to treat the agent's decision function as a latent visual utility that can be inferred through revealed preference: choices between systematically edited images. Starting from common images, such as product photos, we propose methods for visual prompt optimization, adapting text optimization methods to iteratively propose and apply visually plausible modifications using an image generation model (such as in composition, lighting, or background). We then evaluate which edits increase selection probability. Through large-scale experiments on frontier VLMs, we demonstrate that optimized edits significantly shift choice probabilities in head-to-head comparisons. We develop an automatic interpretability pipeline to explain these preferences, identifying consistent visual themes that drive selection. We argue that this approach offers a practical and efficient way to surface visual vulnerabilities, safety concerns that might otherwise be discovered implicitly in the wild, supporting more proactive auditing and governance of image-based AI agents.",https://arxiv.org/pdf/2602.15278v1,http://arxiv.org/abs/2602.15278v1,2026_manuel-cherep_visual-persuasion-what-influences-decisions-of-vision-langua_2602.152781.pdf,data\raw\papers\2026_manuel-cherep_visual-persuasion-what-influences-decisions-of-vision-langua_2602.152781.pdf
2602.15222v1,Automatically Finding Reward Model Biases,2026,Atticus Wang,"Atticus Wang, Iván Arcuschin, Arthur Conmy",2026-02-16T22:05:44+00:00,2026-02-16T22:05:44+00:00,"cs.LG, cs.AI","Reward models are central to large language model (LLM) post-training. However, past work has shown that they can reward spurious or undesirable attributes such as length, format, hallucinations, and sycophancy. In this work, we introduce and study the research problem of automatically finding reward model biases in natural language. We offer a simple approach of using an LLM to iteratively propose and refine candidate biases. Our method can recover known biases and surface novel ones: for example, we found that Skywork-V2-8B, a leading open-weight reward model, often mistakenly favors responses with redundant spacing and responses with hallucinated content. In addition, we show evidence that evolutionary iteration outperforms flat best-of-N search, and we validate the recall of our pipeline using synthetically injected biases. We hope our work contributes to further research on improving RMs through automated interpretability methods.",https://arxiv.org/pdf/2602.15222v1,http://arxiv.org/abs/2602.15222v1,2026_atticus-wang_automatically-finding-reward-model-biases_2602.152221.pdf,data\raw\papers\2026_atticus-wang_automatically-finding-reward-model-biases_2602.152221.pdf
2602.15197v1,OpaqueToolsBench: Learning Nuances of Tool Behavior Through Interaction,2026,Skyler Hallinan,"Skyler Hallinan, Thejas Venkatesh, Xiang Ren, Sai Praneeth Karimireddy, Ashwin Paranjape, Yuhao Zhang, Jack Hessel",2026-02-16T21:26:37+00:00,2026-02-16T21:26:37+00:00,"cs.CL, cs.AI","Tool-calling is essential for Large Language Model (LLM) agents to complete real-world tasks. While most existing benchmarks assume simple, perfectly documented tools, real-world tools (e.g., general ""search"" APIs) are often opaque, lacking clear best practices or failure modes. Can LLM agents improve their performance in environments with opaque tools by interacting and subsequently improving documentation? To study this, we create OpaqueToolsBench, a benchmark consisting of three distinct task-oriented environments: general function calling, interactive chess playing, and long-trajectory agentic search. Each environment provides underspecified tools that models must learn to use effectively to complete the task. Results on OpaqueToolsBench suggest existing methods for automatically documenting tools are expensive and unreliable when tools are opaque. To address this, we propose a simple framework, ToolObserver, that iteratively refines tool documentation by observing execution feedback from tool-calling trajectories. Our approach outperforms existing methods on OpaqueToolsBench across datasets, even in relatively hard settings. Furthermore, for test-time tool exploration settings, our method is also efficient, consuming 3.5-7.5x fewer total tokens than the best baseline.",https://arxiv.org/pdf/2602.15197v1,http://arxiv.org/abs/2602.15197v1,2026_skyler-hallinan_opaquetoolsbench-learning-nuances-of-tool-behavior-through-i_2602.151971.pdf,data\raw\papers\2026_skyler-hallinan_opaquetoolsbench-learning-nuances-of-tool-behavior-through-i_2602.151971.pdf
2602.15190v1,AIC CTU@AVerImaTeC: dual-retriever RAG for image-text fact checking,2026,Herbert Ullrich,"Herbert Ullrich, Jan Drchal",2026-02-16T21:00:29+00:00,2026-02-16T21:00:29+00:00,cs.CL,"In this paper, we present our 3rd place system in the AVerImaTeC shared task, which combines our last year's retrieval-augmented generation (RAG) pipeline with a reverse image search (RIS) module. Despite its simplicity, our system delivers competitive performance with a single multimodal LLM call per fact-check at just $0.013 on average using GPT5.1 via OpenAI Batch API. Our system is also easy to reproduce and tweak, consisting of only three decoupled modules - a textual retrieval module based on similarity search, an image retrieval module based on API-accessed RIS, and a generation module using GPT5.1 - which is why we suggest it as an accesible starting point for further experimentation. We publish its code and prompts, as well as our vector stores and insights into the scheme's running costs and directions for further improvement.",https://arxiv.org/pdf/2602.15190v1,http://arxiv.org/abs/2602.15190v1,2026_herbert-ullrich_aic-ctu-averimatec-dual-retriever-rag-for-image-text-fact-ch_2602.151901.pdf,data\raw\papers\2026_herbert-ullrich_aic-ctu-averimatec-dual-retriever-rag-for-image-text-fact-ch_2602.151901.pdf
2602.15028v1,"Long Context, Less Focus: A Scaling Gap in LLMs Revealed through Privacy and Personalization",2026,Shangding Gu,Shangding Gu,2026-02-16T18:59:42+00:00,2026-02-16T18:59:42+00:00,"cs.LG, cs.AI","Large language models (LLMs) are increasingly deployed in privacy-critical and personalization-oriented scenarios, yet the role of context length in shaping privacy leakage and personalization effectiveness remains largely unexplored. We introduce a large-scale benchmark, PAPerBench, to systematically study how increasing context length influences both personalization quality and privacy protection in LLMs. The benchmark comprises approximately 29,000 instances with context lengths ranging from 1K to 256K tokens, yielding a total of 377K evaluation questions. It jointly evaluates personalization performance and privacy risks across diverse scenarios, enabling controlled analysis of long-context model behavior. Extensive evaluations across state-of-the-art LLMs reveal consistent performance degradation in both personalization and privacy as context length increases. We further provide a theoretical analysis of attention dilution under context scaling, explaining this behavior as an inherent limitation of soft attention in fixed-capacity Transformers. The empirical and theoretical findings together suggest a general scaling gap in current models -- long context, less focus. We release the benchmark to support reproducible evaluation and future research on scalable privacy and personalization. Code and data are available at https://github.com/SafeRL-Lab/PAPerBench",https://arxiv.org/pdf/2602.15028v1,http://arxiv.org/abs/2602.15028v1,2026_shangding-gu_long-context-less-focus-a-scaling-gap-in-llms-revealed-throu_2602.150281.pdf,data\raw\papers\2026_shangding-gu_long-context-less-focus-a-scaling-gap-in-llms-revealed-throu_2602.150281.pdf
2602.15019v2,"Hunt Globally: Wide Search AI Agents for Drug Asset Scouting in Investing, Business Development, and Competitive Intelligence",2026,Alisa Vinogradova,"Alisa Vinogradova, Vlad Vinogradov, Luba Greenwood, Ilya Yasny, Dmitry Kobyzev, Shoman Kasbekar, Kong Nguyen, Dmitrii Radkevich, Roman Doronin, Andrey Doronichev",2026-02-16T18:57:49+00:00,2026-02-17T18:58:56+00:00,"cs.AI, cs.IR","Bio-pharmaceutical innovation has shifted: many new drug assets now originate outside the United States and are disclosed primarily via regional, non-English channels. Recent data suggests that over 85% of patent filings originate outside the U.S., with China accounting for nearly half of the global total. A growing share of scholarly output is also non-U.S. Industry estimates put China at 30% of global drug development, spanning 1,200+ novel candidates. In this high-stakes environment, failing to surface ""under-the-radar"" assets creates multi-billion-dollar risk for investors and business development teams, making asset scouting a coverage-critical competition where speed and completeness drive value. Yet today's Deep Research AI agents still lag human experts in achieving high recall discovery across heterogeneous, multilingual sources without hallucination. We propose a benchmarking methodology for drug asset scouting and a tuned, tree-based self-learning Bioptic Agent aimed at complete, non-hallucinated scouting. We construct a challenging completeness benchmark using a multilingual multi-agent pipeline: complex user queries paired with ground-truth assets that are largely outside U.S.-centric radar. To reflect real-deal complexity, we collected screening queries from expert investors, BD, and VC professionals and used them as priors to conditionally generate benchmark queries. For grading, we use LLM-as-judge evaluation calibrated to expert opinions. On this benchmark, our Bioptic Agent achieves 79.7% F1 score, outperforming Claude Opus 4.6 (56.2%), Gemini 3 Pro + Deep Research (50.6%), OpenAI GPT-5.2 Pro (46.6%), Perplexity Deep Research (44.2%), and Exa Websets (26.9%). Performance improves steeply with additional compute, supporting the view that more compute yields better results.",https://arxiv.org/pdf/2602.15019v2,http://arxiv.org/abs/2602.15019v2,2026_alisa-vinogradova_hunt-globally-wide-search-ai-agents-for-drug-asset-scouting-_2602.150192.pdf,data\raw\papers\2026_alisa-vinogradova_hunt-globally-wide-search-ai-agents-for-drug-asset-scouting-_2602.150192.pdf
2602.15013v1,Text Style Transfer with Parameter-efficient LLM Finetuning and Round-trip Translation,2026,Ruoxi Liu,"Ruoxi Liu, Philipp Koehn",2026-02-16T18:52:43+00:00,2026-02-16T18:52:43+00:00,cs.CL,"This paper proposes a novel method for Text Style Transfer (TST) based on parameter-efficient fine-tuning of Large Language Models (LLMs). Addressing the scarcity of parallel corpora that map between styles, the study employs roundtrip translation to synthesize such parallel datasets from monolingual corpora. This approach creates 'neutralized' text devoid of stylistic attributes, essentially creating a shared input style at training-time and inference-time. Experimental results demonstrate consistent superiority of this method over zero-shot prompting and fewshot ICL techniques measured by BLEU scores and style accuracy scores across four investigated domains. Furthermore, the integration of retrieval-augmented generation (RAG) for terminology and name knowledge enhances robustness and stylistic consistency.",https://arxiv.org/pdf/2602.15013v1,http://arxiv.org/abs/2602.15013v1,2026_ruoxi-liu_text-style-transfer-with-parameter-efficient-llm-finetuning-_2602.150131.pdf,data\raw\papers\2026_ruoxi-liu_text-style-transfer-with-parameter-efficient-llm-finetuning-_2602.150131.pdf
2602.15005v1,Learning User Interests via Reasoning and Distillation for Cross-Domain News Recommendation,2026,Mengdan Zhu,"Mengdan Zhu, Yufan Zhao, Tao Di, Yulan Yan, Liang Zhao",2026-02-16T18:45:40+00:00,2026-02-16T18:45:40+00:00,"cs.CL, cs.IR","News recommendation plays a critical role in online news platforms by helping users discover relevant content. Cross-domain news recommendation further requires inferring user's underlying information needs from heterogeneous signals that often extend beyond direct news consumption. A key challenge lies in moving beyond surface-level behaviors to capture deeper, reusable user interests while maintaining scalability in large-scale production systems. In this paper, we present a reinforcement learning framework that trains large language models to generate high-quality lists of interest-driven news search queries from cross-domain user signals. We formulate query-list generation as a policy optimization problem and employ GRPO with multiple reward signals. We systematically study two compute dimensions: inference-time sampling and model capacity, and empirically observe consistent improvements with increased compute that exhibit scaling-like behavior. Finally, we perform on-policy distillation to transfer the learned policy from a large, compute-intensive teacher to a compact student model suitable for scalable deployment. Extensive offline experiments, ablation studies and large-scale online A/B tests in a production news recommendation system demonstrate consistent gains in both interest modeling quality and downstream recommendation performance.",https://arxiv.org/pdf/2602.15005v1,http://arxiv.org/abs/2602.15005v1,2026_mengdan-zhu_learning-user-interests-via-reasoning-and-distillation-for-c_2602.150051.pdf,data\raw\papers\2026_mengdan-zhu_learning-user-interests-via-reasoning-and-distillation-for-c_2602.150051.pdf
2602.14989v1,ThermEval: A Structured Benchmark for Evaluation of Vision-Language Models on Thermal Imagery,2026,Ayush Shrivastava,"Ayush Shrivastava, Kirtan Gangani, Laksh Jain, Mayank Goel, Nipun Batra",2026-02-16T18:16:19+00:00,2026-02-16T18:16:19+00:00,"cs.CV, cs.AI, cs.LG","Vision language models (VLMs) achieve strong performance on RGB imagery, but they do not generalize to thermal images. Thermal sensing plays a critical role in settings where visible light fails, including nighttime surveillance, search and rescue, autonomous driving, and medical screening. Unlike RGB imagery, thermal images encode physical temperature rather than color or texture, requiring perceptual and reasoning capabilities that existing RGB-centric benchmarks do not evaluate. We introduce ThermEval-B, a structured benchmark of approximately 55,000 thermal visual question answering pairs designed to assess the foundational primitives required for thermal vision language understanding. ThermEval-B integrates public datasets with our newly collected ThermEval-D, the first dataset to provide dense per-pixel temperature maps with semantic body-part annotations across diverse indoor and outdoor environments. Evaluating 25 open-source and closed-source VLMs, we find that models consistently fail at temperature-grounded reasoning, degrade under colormap transformations, and default to language priors or fixed responses, with only marginal gains from prompting or supervised fine-tuning. These results demonstrate that thermal understanding requires dedicated evaluation beyond RGB-centric assumptions, positioning ThermEval as a benchmark to drive progress in thermal vision language modeling.",https://arxiv.org/pdf/2602.14989v1,http://arxiv.org/abs/2602.14989v1,2026_ayush-shrivastava_thermeval-a-structured-benchmark-for-evaluation-of-vision-la_2602.149891.pdf,data\raw\papers\2026_ayush-shrivastava_thermeval-a-structured-benchmark-for-evaluation-of-vision-la_2602.149891.pdf
2602.14970v1,Counterfactual Fairness Evaluation of LLM-Based Contact Center Agent Quality Assurance System,2026,Kawin Mayilvaghanan,"Kawin Mayilvaghanan, Siddhant Gupta, Ayush Kumar",2026-02-16T17:56:18+00:00,2026-02-16T17:56:18+00:00,cs.CL,"Large Language Models (LLMs) are increasingly deployed in contact-center Quality Assurance (QA) to automate agent performance evaluation and coaching feedback. While LLMs offer unprecedented scalability and speed, their reliance on web-scale training data raises concerns regarding demographic and behavioral biases that may distort workforce assessment. We present a counterfactual fairness evaluation of LLM-based QA systems across 13 dimensions spanning three categories: Identity, Context, and Behavioral Style. Fairness is quantified using the Counterfactual Flip Rate (CFR), the frequency of binary judgment reversals, and the Mean Absolute Score Difference (MASD), the average shift in coaching or confidence scores across counterfactual pairs. Evaluating 18 LLMs on 3,000 real-world contact center transcripts, we find systematic disparities, with CFR ranging from 5.4% to 13.0% and consistent MASD shifts across confidence, positive, and improvement scores. Larger, more strongly aligned models show lower unfairness, though fairness does not track accuracy. Contextual priming of historical performance induces the most severe degradations (CFR up to 16.4%), while implicit linguistic identity cues remain a persistent bias source. Finally, we analyze the efficacy of fairness-aware prompting, finding that explicit instructions yield only modest improvements in evaluative consistency. Our findings underscore the need for standardized fairness auditing pipelines prior to deploying LLMs in high-stakes workforce evaluation.",https://arxiv.org/pdf/2602.14970v1,http://arxiv.org/abs/2602.14970v1,2026_kawin-mayilvaghanan_counterfactual-fairness-evaluation-of-llm-based-contact-cent_2602.149701.pdf,data\raw\papers\2026_kawin-mayilvaghanan_counterfactual-fairness-evaluation-of-llm-based-contact-cent_2602.149701.pdf
2602.14968v1,PhyScensis: Physics-Augmented LLM Agents for Complex Physical Scene Arrangement,2026,Yian Wang,"Yian Wang, Han Yang, Minghao Guo, Xiaowen Qiu, Tsun-Hsuan Wang, Wojciech Matusik, Joshua B. Tenenbaum, Chuang Gan",2026-02-16T17:55:25+00:00,2026-02-16T17:55:25+00:00,"cs.RO, cs.AI","Automatically generating interactive 3D environments is crucial for scaling up robotic data collection in simulation. While prior work has primarily focused on 3D asset placement, it often overlooks the physical relationships between objects (e.g., contact, support, balance, and containment), which are essential for creating complex and realistic manipulation scenarios such as tabletop arrangements, shelf organization, or box packing. Compared to classical 3D layout generation, producing complex physical scenes introduces additional challenges: (a) higher object density and complexity (e.g., a small shelf may hold dozens of books), (b) richer supporting relationships and compact spatial layouts, and (c) the need to accurately model both spatial placement and physical properties. To address these challenges, we propose PhyScensis, an LLM agent-based framework powered by a physics engine, to produce physically plausible scene configurations with high complexity. Specifically, our framework consists of three main components: an LLM agent iteratively proposes assets with spatial and physical predicates; a solver, equipped with a physics engine, realizes these predicates into a 3D scene; and feedback from the solver informs the agent to refine and enrich the configuration. Moreover, our framework preserves strong controllability over fine-grained textual descriptions and numerical parameters (e.g., relative positions, scene stability), enabled through probabilistic programming for stability and a complementary heuristic that jointly regulates stability and spatial relations. Experimental results show that our method outperforms prior approaches in scene complexity, visual quality, and physical accuracy, offering a unified pipeline for generating complex physical scene layouts for robotic manipulation.",https://arxiv.org/pdf/2602.14968v1,http://arxiv.org/abs/2602.14968v1,2026_yian-wang_physcensis-physics-augmented-llm-agents-for-complex-physical_2602.149681.pdf,data\raw\papers\2026_yian-wang_physcensis-physics-augmented-llm-agents-for-complex-physical_2602.149681.pdf
2602.14869v1,Concept Influence: Leveraging Interpretability to Improve Performance and Efficiency in Training Data Attribution,2026,Matthew Kowal,"Matthew Kowal, Goncalo Paulo, Louis Jaburi, Tom Tseng, Lev E McKinney, Stefan Heimersheim, Aaron David Tucker, Adam Gleave, Kellin Pelrine",2026-02-16T16:02:09+00:00,2026-02-16T16:02:09+00:00,"cs.AI, stat.ML","As large language models are increasingly trained and fine-tuned, practitioners need methods to identify which training data drive specific behaviors, particularly unintended ones. Training Data Attribution (TDA) methods address this by estimating datapoint influence. Existing approaches like influence functions are both computationally expensive and attribute based on single test examples, which can bias results toward syntactic rather than semantic similarity. To address these issues of scalability and influence to abstract behavior, we leverage interpretable structures within the model during the attribution. First, we introduce Concept Influence which attribute model behavior to semantic directions (such as linear probes or sparse autoencoder features) rather than individual test examples. Second, we show that simple probe-based attribution methods are first-order approximations of Concept Influence that achieve comparable performance while being over an order-of-magnitude faster. We empirically validate Concept Influence and approximations across emergent misalignment benchmarks and real post-training datasets, and demonstrate they achieve comparable performance to classical influence functions while being substantially more scalable. More broadly, we show that incorporating interpretable structure within traditional TDA pipelines can enable more scalable, explainable, and better control of model behavior through data.",https://arxiv.org/pdf/2602.14869v1,http://arxiv.org/abs/2602.14869v1,2026_matthew-kowal_concept-influence-leveraging-interpretability-to-improve-per_2602.148691.pdf,data\raw\papers\2026_matthew-kowal_concept-influence-leveraging-interpretability-to-improve-per_2602.148691.pdf
2602.14857v1,World Models for Policy Refinement in StarCraft II,2026,Yixin Zhang,"Yixin Zhang, Ziyi Wang, Yiming Rong, Haoxi Wang, Jinling Jiang, Shuang Xu, Haoran Wu, Shiyu Zhou, Bo Xu",2026-02-16T15:51:59+00:00,2026-02-16T15:51:59+00:00,cs.AI,"Large Language Models (LLMs) have recently shown strong reasoning and generalization capabilities, motivating their use as decision-making policies in complex environments. StarCraft II (SC2), with its massive state-action space and partial observability, is a challenging testbed. However, existing LLM-based SC2 agents primarily focus on improving the policy itself and overlook integrating a learnable, action-conditioned transition model into the decision loop. To bridge this gap, we propose StarWM, the first world model for SC2 that predicts future observations under partial observability. To facilitate learning SC2's hybrid dynamics, we introduce a structured textual representation that factorizes observations into five semantic modules, and construct SC2-Dynamics-50k, the first instruction-tuning dataset for SC2 dynamics prediction. We further develop a multi-dimensional offline evaluation framework for predicted structured observations. Offline results show StarWM's substantial gains over zero-shot baselines, including nearly 60% improvements in resource prediction accuracy and self-side macro-situation consistency. Finally, we propose StarWM-Agent, a world-model-augmented decision system that integrates StarWM into a Generate--Simulate--Refine decision loop for foresight-driven policy refinement. Online evaluation against SC2's built-in AI demonstrates consistent improvements, yielding win-rate gains of 30%, 15%, and 30% against Hard (LV5), Harder (LV6), and VeryHard (LV7), respectively, alongside improved macro-management stability and tactical risk assessment.",https://arxiv.org/pdf/2602.14857v1,http://arxiv.org/abs/2602.14857v1,2026_yixin-zhang_world-models-for-policy-refinement-in-starcraft-ii_2602.148571.pdf,data\raw\papers\2026_yixin-zhang_world-models-for-policy-refinement-in-starcraft-ii_2602.148571.pdf
2602.14812v1,Physical Commonsense Reasoning for Lower-Resourced Languages and Dialects: a Study on Basque,2026,Jaione Bengoetxea,"Jaione Bengoetxea, Itziar Gonzalez-Dios, Rodrigo Agerri",2026-02-16T15:04:35+00:00,2026-02-16T15:04:35+00:00,cs.CL,"Physical commonsense reasoning represents a fundamental capability of human intelligence, enabling individuals to understand their environment, predict future events, and navigate physical spaces. Recent years have witnessed growing interest in reasoning tasks within Natural Language Processing (NLP). However, no prior research has examined the performance of Large Language Models (LLMs) on non-question-answering (non-QA) physical commonsense reasoning tasks in low-resource languages such as Basque. Taking the Italian GITA as a starting point, this paper addresses this gap by presenting BasPhyCo, the first non-QA physical commonsense reasoning dataset for Basque, available in both standard and dialectal variants. We evaluate model performance across three hierarchical levels of commonsense understanding: (1) distinguishing between plausible and implausible narratives (accuracy), (2) identifying the conflicting element that renders a narrative implausible (consistency), and (3) determining the specific physical state that creates the implausibility (verifiability). These tasks were assessed using multiple multilingual LLMs as well as models pretrained specifically for Italian and Basque. Results indicate that, in terms of verifiability, LLMs exhibit limited physical commonsense capabilities in low-resource languages such as Basque, especially when processing dialectal variants.",https://arxiv.org/pdf/2602.14812v1,http://arxiv.org/abs/2602.14812v1,2026_jaione-bengoetxea_physical-commonsense-reasoning-for-lower-resourced-languages_2602.148121.pdf,data\raw\papers\2026_jaione-bengoetxea_physical-commonsense-reasoning-for-lower-resourced-languages_2602.148121.pdf
2602.15909v1,Resp-Agent: An Agent-Based System for Multimodal Respiratory Sound Generation and Disease Diagnosis,2026,Pengfei Zhang,"Pengfei Zhang, Tianxin Xie, Minghao Yang, Li Liu",2026-02-16T14:48:24+00:00,2026-02-16T14:48:24+00:00,"eess.AS, cs.AI, cs.DB, cs.HC, cs.MA, cs.SD","Deep learning-based respiratory auscultation is currently hindered by two fundamental challenges: (i) inherent information loss, as converting signals into spectrograms discards transient acoustic events and clinical context; (ii) limited data availability, exacerbated by severe class imbalance. To bridge these gaps, we present Resp-Agent, an autonomous multimodal system orchestrated by a novel Active Adversarial Curriculum Agent (Thinker-A$^2$CA). Unlike static pipelines, Thinker-A$^2$CA serves as a central controller that actively identifies diagnostic weaknesses and schedules targeted synthesis in a closed loop. To address the representation gap, we introduce a Modality-Weaving Diagnoser that weaves EHR data with audio tokens via Strategic Global Attention and sparse audio anchors, capturing both long-range clinical context and millisecond-level transients. To address the data gap, we design a Flow Matching Generator that adapts a text-only Large Language Model (LLM) via modality injection, decoupling pathological content from acoustic style to synthesize hard-to-diagnose samples. As a foundation for these efforts, we introduce Resp-229k, a benchmark corpus of 229k recordings paired with LLM-distilled clinical narratives. Extensive experiments demonstrate that Resp-Agent consistently outperforms prior approaches across diverse evaluation settings, improving diagnostic robustness under data scarcity and long-tailed class imbalance. Our code and data are available at https://github.com/zpforlove/Resp-Agent.",https://arxiv.org/pdf/2602.15909v1,http://arxiv.org/abs/2602.15909v1,2026_pengfei-zhang_resp-agent-an-agent-based-system-for-multimodal-respiratory-_2602.159091.pdf,data\raw\papers\2026_pengfei-zhang_resp-agent-an-agent-based-system-for-multimodal-respiratory-_2602.159091.pdf
2602.14778v2,A Geometric Analysis of Small-sized Language Model Hallucinations,2026,Emanuele Ricco,"Emanuele Ricco, Elia Onofri, Lorenzo Cima, Stefano Cresci, Roberto Di Pietro",2026-02-16T14:29:55+00:00,2026-02-17T19:50:06+00:00,"cs.CL, cs.AI, cs.CY","Hallucinations -- fluent but factually incorrect responses -- pose a major challenge to the reliability of language models, especially in multi-step or agentic settings.   This work investigates hallucinations in small-sized LLMs through a geometric perspective, starting from the hypothesis that when models generate multiple responses to the same prompt, genuine ones exhibit tighter clustering in the embedding space, we prove this hypothesis and, leveraging this geometrical insight, we also show that it is possible to achieve a consistent level of separability. This latter result is used to introduce a label-efficient propagation method that classifies large collections of responses from just 30-50 annotations, achieving F1 scores above 90%.   Our findings, framing hallucinations from a geometric perspective in the embedding space, complement traditional knowledge-centric and single-response evaluation paradigms, paving the way for further research.",https://arxiv.org/pdf/2602.14778v2,http://arxiv.org/abs/2602.14778v2,2026_emanuele-ricco_a-geometric-analysis-of-small-sized-language-model-hallucina_2602.147782.pdf,data\raw\papers\2026_emanuele-ricco_a-geometric-analysis-of-small-sized-language-model-hallucina_2602.147782.pdf
2602.14763v1,Unlocking Reasoning Capability on Machine Translation in Large Language Models,2026,Sara Rajaee,"Sara Rajaee, Sebastian Vincent, Alexandre Berard, Marzieh Fadaee, Kelly Marchisio, Tom Kocmi",2026-02-16T14:05:59+00:00,2026-02-16T14:05:59+00:00,"cs.CL, cs.AI","Reasoning-oriented large language models (RLMs) achieve strong gains on tasks such as mathematics and coding by generating explicit intermediate reasoning. However, their impact on machine translation (MT) remains underexplored. We systematically evaluate several open- and closed-weights RLMs on the WMT24++ benchmark and find that enabling explicit reasoning consistently degrades translation quality across languages and models. Analysis reveals that MT reasoning traces are highly linear, lacking revision, self-correction and exploration of alternative translations, which limits their usefulness. Furthermore, injecting higher-quality reasoning traces from stronger models does not reliably improve weaker models' performance. To address this mismatch, we propose a structured reasoning framework tailored to translation, based on multi-step drafting, adequacy refinement, fluency improvement, and selective iterative revision. We curate a synthetic dataset of dynamic structured reasoning traces and post-train a large reasoning model on this data. Experiments show significant improvements over standard translation fine-tuning and injected generic reasoning baselines. Our findings demonstrate that reasoning must be task-structured to benefit MT.",https://arxiv.org/pdf/2602.14763v1,http://arxiv.org/abs/2602.14763v1,2026_sara-rajaee_unlocking-reasoning-capability-on-machine-translation-in-lar_2602.147631.pdf,data\raw\papers\2026_sara-rajaee_unlocking-reasoning-capability-on-machine-translation-in-lar_2602.147631.pdf
2602.14759v1,Inner Loop Inference for Pretrained Transformers: Unlocking Latent Capabilities Without Training,2026,Jonathan Lys,"Jonathan Lys, Vincent Gripon, Bastien Pasdeloup, Lukas Mauch, Fabien Cardinaux, Ghouthi Boukli Hacene",2026-02-16T14:04:24+00:00,2026-02-16T14:04:24+00:00,"cs.LG, cs.AI","Deep Learning architectures, and in particular Transformers, are conventionally viewed as a composition of layers. These layers are actually often obtained as the sum of two contributions: a residual path that copies the input and the output of a Transformer block. As a consequence, the inner representations (i.e. the input of these blocks) can be interpreted as iterative refinement of a propagated latent representation. Under this lens, many works suggest that the inner space is shared across layers, meaning that tokens can be decoded at early stages. Mechanistic interpretability even goes further by conjecturing that some layers act as refinement layers. Following this path, we propose inference-time inner looping, which prolongs refinement in pretrained off-the-shelf language models by repeatedly re-applying a selected block range. Across multiple benchmarks, inner looping yields modest but consistent accuracy improvements. Analyses of the resulting latent trajectories suggest more stable state evolution and continued semantic refinement. Overall, our results suggest that additional refinement can be obtained through simple test-time looping, extending computation in frozen pretrained models.",https://arxiv.org/pdf/2602.14759v1,http://arxiv.org/abs/2602.14759v1,2026_jonathan-lys_inner-loop-inference-for-pretrained-transformers-unlocking-l_2602.147591.pdf,data\raw\papers\2026_jonathan-lys_inner-loop-inference-for-pretrained-transformers-unlocking-l_2602.147591.pdf
2602.14749v1,"Cognitive networks reconstruct mindsets about STEM subjects and educational contexts in almost 1000 high-schoolers, University students and LLM-based digital twins",2026,Francesco Gariboldi,"Francesco Gariboldi, Emma Franchino, Edith Haim, Gianluca Lattanzi, Alessandro Grecucci, Massimo Stella",2026-02-16T13:49:21+00:00,2026-02-16T13:49:21+00:00,cs.CL,"Attitudes toward STEM develop from the interaction of conceptual knowledge, educational experiences, and affect. Here we use cognitive network science to reconstruct group mindsets as behavioural forma mentis networks (BFMNs). In this case, nodes are cue words and free associations, edges are empirical associative links, and each concept is annotated with perceived valence. We analyse BFMNs from N = 994 observations spanning high school students, university students, and early-career STEM experts, alongside LLM (GPT-oss) ""digital twins"" prompted to emulate comparable profiles. Focusing also on semantic neighbourhoods (""frames"") around key target concepts (e.g., STEM subjects or educational actors/places), we quantify frames in terms of valence auras, emotional profiles, network overlap (Jaccard similarity), and concreteness relative to null baselines. Across student groups, science and research are consistently framed positively, while their core quantitative subjects (mathematics and statistics) exhibit more negative and anxiety related auras, amplified in higher math-anxiety subgroups, evidencing a STEM-science cognitive and emotional dissonance. High-anxiety frames are also less concrete than chance, suggesting more abstract and decontextualised representations of threatening quantitative domains. Human networks show greater overlapping between mathematics and anxiety than GPT-oss. The results highlight how BFMNs capture cognitive-affective signatures of mindsets towards the target domains and indicate that LLM-based digital twins approximate cultural attitudes but miss key context-sensitive, experience-based components relevant to replicate human educational anxiety.",https://arxiv.org/pdf/2602.14749v1,http://arxiv.org/abs/2602.14749v1,2026_francesco-gariboldi_cognitive-networks-reconstruct-mindsets-about-stem-subjects-_2602.147491.pdf,data\raw\papers\2026_francesco-gariboldi_cognitive-networks-reconstruct-mindsets-about-stem-subjects-_2602.147491.pdf
2602.14743v1,LLMStructBench: Benchmarking Large Language Model Structured Data Extraction,2026,Sönke Tenckhoff,"Sönke Tenckhoff, Mario Koddenbrock, Erik Rodner",2026-02-16T13:37:58+00:00,2026-02-16T13:37:58+00:00,"cs.CL, cs.LG","We present LLMStructBench, a novel benchmark for evaluating Large Language Models (LLMs) on extracting structured data and generating valid JavaScript Object Notation (JSON) outputs from natural-language text. Our open dataset comprises diverse, manually verified parsing scenarios of varying complexity and enables systematic testing across 22 models and five prompting strategies. We further introduce complementary performance metrics that capture both token-level accuracy and document-level validity, facilitating rigorous comparison of model, size, and prompting effects on parsing reliability.   In particular, we show that choosing the right prompting strategy is more important than standard attributes such as model size. This especially ensures structural validity for smaller or less reliable models but increase the number of semantic errors. Our benchmark suite is an step towards future research in the area of LLM applied to parsing or Extract, Transform and Load (ETL) applications.",https://arxiv.org/pdf/2602.14743v1,http://arxiv.org/abs/2602.14743v1,2026_sonke-tenckhoff_llmstructbench-benchmarking-large-language-model-structured-_2602.147431.pdf,data\raw\papers\2026_sonke-tenckhoff_llmstructbench-benchmarking-large-language-model-structured-_2602.147431.pdf
2602.14697v1,Evolutionary System Prompt Learning can Facilitate Reinforcement Learning for LLMs,2026,Lunjun Zhang,"Lunjun Zhang, Ryan Chen, Bradly C. Stadie",2026-02-16T12:34:27+00:00,2026-02-16T12:34:27+00:00,"cs.AI, cs.LG","Building agentic systems that can autonomously self-improve from experience is a longstanding goal of AI. Large language models (LLMs) today primarily self-improve via two mechanisms: self-reflection for context updates, and reinforcement learning (RL) for weight updates. In this work, we propose Evolutionary System Prompt Learning (E-SPL), a method for jointly improving model contexts and model weights. In each RL iteration, E-SPL selects multiple system prompts and runs rollouts with each in parallel. It applies RL updates to model weights conditioned on each system prompt, and evolutionary updates to the system prompt population via LLM-driven mutation and crossover. Each system prompt has a TrueSkill rating for evolutionary selection, updated from relative performance within each RL iteration batch. E-SPL encourages a natural division between declarative knowledge encoded in prompts and procedural knowledge encoded in weights, resulting in improved performance across reasoning and agentic tasks. For instance, in an easy-to-hard (AIME $\rightarrow$ BeyondAIME) generalization setting, E-SPL improves RL success rate from 38.8% $\rightarrow$ 45.1% while also outperforming reflective prompt evolution (40.0%). Overall, our results show that coupling reinforcement learning with system prompt evolution yields consistent gains in sample efficiency and generalization. Code: https://github.com/LunjunZhang/E-SPL",https://arxiv.org/pdf/2602.14697v1,http://arxiv.org/abs/2602.14697v1,2026_lunjun-zhang_evolutionary-system-prompt-learning-can-facilitate-reinforce_2602.146971.pdf,data\raw\papers\2026_lunjun-zhang_evolutionary-system-prompt-learning-can-facilitate-reinforce_2602.146971.pdf
2602.14696v1,A Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesn't),2026,Nihal V. Nayak,"Nihal V. Nayak, Paula Rodriguez-Diaz, Neha Hulkund, Sara Beery, David Alvarez-Melis",2026-02-16T12:33:05+00:00,2026-02-16T12:33:05+00:00,cs.LG,"Instruction fine-tuning of large language models (LLMs) often involves selecting a subset of instruction training data from a large candidate pool, using a small query set from the target task. Despite growing interest, the literature on targeted instruction selection remains fragmented and opaque: methods vary widely in selection budgets, often omit zero-shot baselines, and frequently entangle the contributions of key components. As a result, practitioners lack actionable guidance on selecting instructions for their target tasks. In this work, we aim to bring clarity to this landscape by disentangling and systematically analyzing the two core ingredients: data representation and selection algorithms. Our framework enables controlled comparisons across models, tasks, and budgets. We find that only gradient-based data representations choose subsets whose similarity to the query consistently predicts performance across datasets and models. While no single method dominates, gradient-based representations paired with a greedy round-robin selection algorithm tend to perform best on average at low budgets, but these benefits diminish at larger budgets. Finally, we unify several existing selection algorithms as forms of approximate distance minimization between the selected subset and the query set, and support this view with new generalization bounds. More broadly, our findings provide critical insights and a foundation for more principled data selection in LLM fine-tuning. The code is available at https://github.com/dcml-lab/targeted-instruction-selection.",https://arxiv.org/pdf/2602.14696v1,http://arxiv.org/abs/2602.14696v1,2026_nihal-v-nayak_a-critical-look-at-targeted-instruction-selection-disentangl_2602.146961.pdf,data\raw\papers\2026_nihal-v-nayak_a-critical-look-at-targeted-instruction-selection-disentangl_2602.146961.pdf
2602.14689v1,Exposing the Systematic Vulnerability of Open-Weight Models to Prefill Attacks,2026,Lukas Struppek,"Lukas Struppek, Adam Gleave, Kellin Pelrine",2026-02-16T12:24:21+00:00,2026-02-16T12:24:21+00:00,"cs.CR, cs.AI, cs.CL, cs.LG","As the capabilities of large language models continue to advance, so does their potential for misuse. While closed-source models typically rely on external defenses, open-weight models must primarily depend on internal safeguards to mitigate harmful behavior. Prior red-teaming research has largely focused on input-based jailbreaking and parameter-level manipulations. However, open-weight models also natively support prefilling, which allows an attacker to predefine initial response tokens before generation begins. Despite its potential, this attack vector has received little systematic attention. We present the largest empirical study to date of prefill attacks, evaluating over 20 existing and novel strategies across multiple model families and state-of-the-art open-weight models. Our results show that prefill attacks are consistently effective against all major contemporary open-weight models, revealing a critical and previously underexplored vulnerability with significant implications for deployment. While certain large reasoning models exhibit some robustness against generic prefilling, they remain vulnerable to tailored, model-specific strategies. Our findings underscore the urgent need for model developers to prioritize defenses against prefill attacks in open-weight LLMs.",https://arxiv.org/pdf/2602.14689v1,http://arxiv.org/abs/2602.14689v1,2026_lukas-struppek_exposing-the-systematic-vulnerability-of-open-weight-models-_2602.146891.pdf,data\raw\papers\2026_lukas-struppek_exposing-the-systematic-vulnerability-of-open-weight-models-_2602.146891.pdf
2602.14653v1,Is Information Density Uniform when Utterances are Grounded on Perception and Discourse?,2026,Matteo Gay,"Matteo Gay, Coleman Haley, Mario Giulianelli, Edoardo Ponti",2026-02-16T11:25:00+00:00,2026-02-16T11:25:00+00:00,cs.CL,"The Uniform Information Density (UID) hypothesis posits that speakers are subject to a communicative pressure to distribute information evenly within utterances, minimising surprisal variance. While this hypothesis has been tested empirically, prior studies are limited exclusively to text-only inputs, abstracting away from the perceptual context in which utterances are produced. In this work, we present the first computational study of UID in visually grounded settings. We estimate surprisal using multilingual vision-and-language models over image-caption data in 30 languages and visual storytelling data in 13 languages, together spanning 11 families. We find that grounding on perception consistently smooths the distribution of information, increasing both global and local uniformity across typologically diverse languages compared to text-only settings. In visual narratives, grounding in both image and discourse contexts has additional effects, with the strongest surprisal reductions occurring at the onset of discourse units. Overall, this study takes a first step towards modelling the temporal dynamics of information flow in ecologically plausible, multimodal language use, and finds that grounded language exhibits greater information uniformity, supporting a context-sensitive formulation of UID.",https://arxiv.org/pdf/2602.14653v1,http://arxiv.org/abs/2602.14653v1,2026_matteo-gay_is-information-density-uniform-when-utterances-are-grounded-_2602.146531.pdf,data\raw\papers\2026_matteo-gay_is-information-density-uniform-when-utterances-are-grounded-_2602.146531.pdf
2602.14649v1,GradMAP: Faster Layer Pruning with Gradient Metric and Projection Compensation,2026,Hao Liu,"Hao Liu, Guangyan Li, Wensheng Zhang, Yongqiang Tang",2026-02-16T11:14:02+00:00,2026-02-16T11:14:02+00:00,cs.CL,"Large Language Models (LLMs) exhibit strong reasoning abilities, but their high computational costs limit their practical deployment. Recent studies reveal significant redundancy in LLMs layers, making layer pruning an active research topic. Layer pruning research primarily focuses on two aspects: measuring layer importance and recovering performance after pruning. Unfortunately, the present works fail to simultaneously maintain pruning performance and efficiency. In this study, we propose GradMAP, a faster layer pruning method with \textbf{Grad}ient \textbf{M}etric \textbf{A}nd \textbf{P}rojection compensation, which consists of two stages. In the first stage, we introduce a novel metric based on gradient magnitudes, enabling a global assessment of layer importance. Note that, it requires only a single backward propagation step per pruning decision, substantially enhancing pruning efficiency. In the second stage, we first analyze the layers with the largest mean shift resulting from pruning, and then incorporate a simple yet effective projection compensation matrix to correct this drift in one step. In this way, the degradation of model performance caused by layer pruning is effectively alleviated. Extensive experiments show that GradMAP outperforms previous layer pruning methods in both pruning speed (achieving an average $4\times$ speedup) and performance.",https://arxiv.org/pdf/2602.14649v1,http://arxiv.org/abs/2602.14649v1,2026_hao-liu_gradmap-faster-layer-pruning-with-gradient-metric-and-projec_2602.146491.pdf,data\raw\papers\2026_hao-liu_gradmap-faster-layer-pruning-with-gradient-metric-and-projec_2602.146491.pdf
2602.14612v1,LongAudio-RAG: Event-Grounded Question Answering over Multi-Hour Long Audio,2026,Naveen Vakada,"Naveen Vakada, Kartik Hegde, Arvind Krishna Sridhar, Yinyi Guo, Erik Visser",2026-02-16T10:15:22+00:00,2026-02-16T10:15:22+00:00,"eess.AS, cs.AI, cs.LG","Long-duration audio is increasingly common in industrial and consumer settings, yet reviewing multi-hour recordings is impractical, motivating systems that answer natural-language queries with precise temporal grounding and minimal hallucination. Existing audio-language models show promise, but long-audio question answering remains difficult due to context-length limits. We introduce LongAudio-RAG (LA-RAG), a hybrid framework that grounds Large Language Model (LLM) outputs in retrieved, timestamped acoustic event detections rather than raw audio. Multi-hour streams are converted into structured event records stored in an SQL database, and at inference time the system resolves natural-language time references, classifies intent, retrieves only the relevant events, and generates answers using this constrained evidence. To evaluate performance, we construct a synthetic long-audio benchmark by concatenating recordings with preserved timestamps and generating template-based question-answer pairs for detection, counting, and summarization tasks. Finally, we demonstrate the practicality of our approach by deploying it in a hybrid edge-cloud environment, where the audio grounding model runs on-device on IoT-class hardware while the LLM is hosted on a GPU-backed server. This architecture enables low-latency event extraction at the edge and high-quality language reasoning in the cloud. Experiments show that structured, event-level retrieval significantly improves accuracy compared to vanilla Retrieval-Augmented Generation (RAG) or text-to-SQL approaches.",https://arxiv.org/pdf/2602.14612v1,http://arxiv.org/abs/2602.14612v1,2026_naveen-vakada_longaudio-rag-event-grounded-question-answering-over-multi-h_2602.146121.pdf,data\raw\papers\2026_naveen-vakada_longaudio-rag-event-grounded-question-answering-over-multi-h_2602.146121.pdf
2602.14589v1,MATEO: A Multimodal Benchmark for Temporal Reasoning and Planning in LVLMs,2026,Gabriel Roccabruna,"Gabriel Roccabruna, Olha Khomyn, Giuseppe Riccardi",2026-02-16T09:41:50+00:00,2026-02-16T09:41:50+00:00,"cs.AI, cs.CL, cs.LG","AI agents need to plan to achieve complex goals that involve orchestrating perception, sub-goal decomposition, and execution. These plans consist of ordered steps structured according to a Temporal Execution Order (TEO, a directed acyclic graph that ensures each step executes only after its preconditions are satisfied. Existing research on foundational models' understanding of temporal execution is limited to automatically derived annotations, approximations of the TEO as a linear chain, or text-only inputs. To address this gap, we introduce MATEO (MultimodAl Temporal Execution Order), a benchmark designed to assess and improve the temporal reasoning abilities of Large Vision Language Models (LVLMs) required for real-world planning. We acquire a high-quality professional multimodal recipe corpus, authored through a standardized editorial process that decomposes instructions into discrete steps, each paired with corresponding images. We collect TEO annotations as graphs by designing and using a scalable crowdsourcing pipeline. Using MATEO, we evaluate six state-of-the-art LVLMs across model scales, varying language context, multimodal input structure, and fine-tuning strategies.",https://arxiv.org/pdf/2602.14589v1,http://arxiv.org/abs/2602.14589v1,2026_gabriel-roccabruna_mateo-a-multimodal-benchmark-for-temporal-reasoning-and-plan_2602.145891.pdf,data\raw\papers\2026_gabriel-roccabruna_mateo-a-multimodal-benchmark-for-temporal-reasoning-and-plan_2602.145891.pdf
2602.14564v1,Assessing Large Language Models for Medical QA: Zero-Shot and LLM-as-a-Judge Evaluation,2026,Shefayat E Shams Adib,"Shefayat E Shams Adib, Ahmed Alfey Sani, Ekramul Alam Esham, Ajwad Abrar, Tareque Mohmud Chowdhury",2026-02-16T08:53:23+00:00,2026-02-16T08:53:23+00:00,cs.CL,"Recently, Large Language Models (LLMs) have gained significant traction in medical domain, especially in developing a QA systems to Medical QA systems for enhancing access to healthcare in low-resourced settings. This paper compares five LLMs deployed between April 2024 and August 2025 for medical QA, using the iCliniq dataset, containing 38,000 medical questions and answers of diverse specialties. Our models include Llama-3-8B-Instruct, Llama 3.2 3B, Llama 3.3 70B Instruct, Llama-4-Maverick-17B-128E-Instruct, and GPT-5-mini. We are using a zero-shot evaluation methodology and using BLEU and ROUGE metrics to evaluate performance without specialized fine-tuning. Our results show that larger models like Llama 3.3 70B Instruct outperform smaller models, consistent with observed scaling benefits in clinical tasks. It is notable that, Llama-4-Maverick-17B exhibited more competitive results, thus highlighting evasion efficiency trade-offs relevant for practical deployment. These findings align with advancements in LLM capabilities toward professional-level medical reasoning and reflect the increasing feasibility of LLM-supported QA systems in the real clinical environments. This benchmark aims to serve as a standardized setting for future study to minimize model size, computational resources and to maximize clinical utility in medical NLP applications.",https://arxiv.org/pdf/2602.14564v1,http://arxiv.org/abs/2602.14564v1,2026_shefayat-e-shams-adib_assessing-large-language-models-for-medical-qa-zero-shot-and_2602.145641.pdf,data\raw\papers\2026_shefayat-e-shams-adib_assessing-large-language-models-for-medical-qa-zero-shot-and_2602.145641.pdf
2602.14536v1,Explainable Token-level Noise Filtering for LLM Fine-tuning Datasets,2026,Yuchen Yang,"Yuchen Yang, Wenze Lin, Enhao Huang, Zhixuan Chu, Hongbin Zhou, Lan Tao, Yiming Li, Zhan Qin, Kui Ren",2026-02-16T07:49:33+00:00,2026-02-16T07:49:33+00:00,"cs.CL, cs.AI","Large Language Models (LLMs) have seen remarkable advancements, achieving state-of-the-art results in diverse applications. Fine-tuning, an important step for adapting LLMs to specific downstream tasks, typically involves further training on corresponding datasets. However, a fundamental discrepancy exists between current fine-tuning datasets and the token-level optimization mechanism of LLMs: most datasets are designed at the sentence-level, which introduces token-level noise, causing negative influence to final performance. In this paper, we propose XTF, an explainable token-level noise filtering framework. XTF decomposes the complex and subtle contributions of token-level data to the fine-tuning process into three distinct and explicit attributes (reasoning importance, knowledge novelty, and task relevance), which can be assessed using scoring methods, and then masks the gradients of selected noisy tokens accordingly to optimize the performance of fine-tuned LLMs. We conduct extensive experiments on three representative downstream tasks (math, code and medicine) across 7 mainstream LLMs. The results demonstrate that XTF can significantly improve downstream performance by up to 13.7% compared to regular fine-tuning. Our work highlights the importance of token-level dataset optimization, and demonstrates the potential of strategies based on attribute decomposition for explaining complex training mechanisms.",https://arxiv.org/pdf/2602.14536v1,http://arxiv.org/abs/2602.14536v1,2026_yuchen-yang_explainable-token-level-noise-filtering-for-llm-fine-tuning-_2602.145361.pdf,data\raw\papers\2026_yuchen-yang_explainable-token-level-noise-filtering-for-llm-fine-tuning-_2602.145361.pdf
2602.14529v1,Disentangling Deception and Hallucination Failures in LLMs,2026,Haolang Lu,"Haolang Lu, Hongrui Peng, WeiYe Fu, Guoshun Nan, Xinye Cao, Xingrui Li, Hongcan Guo, Kun Wang",2026-02-16T07:36:49+00:00,2026-02-16T07:36:49+00:00,cs.AI,"Failures in large language models (LLMs) are often analyzed from a behavioral perspective, where incorrect outputs in factual question answering are commonly associated with missing knowledge. In this work, focusing on entity-based factual queries, we suggest that such a view may conflate different failure mechanisms, and propose an internal, mechanism-oriented perspective that separates Knowledge Existence from Behavior Expression. Under this formulation, hallucination and deception correspond to two qualitatively different failure modes that may appear similar at the output level but differ in their underlying mechanisms. To study this distinction, we construct a controlled environment for entity-centric factual questions in which knowledge is preserved while behavioral expression is selectively altered, enabling systematic analysis of four behavioral cases. We analyze these failure modes through representation separability, sparse interpretability, and inference-time activation steering.",https://arxiv.org/pdf/2602.14529v1,http://arxiv.org/abs/2602.14529v1,2026_haolang-lu_disentangling-deception-and-hallucination-failures-in-llms_2602.145291.pdf,data\raw\papers\2026_haolang-lu_disentangling-deception-and-hallucination-failures-in-llms_2602.145291.pdf
2602.14518v1,Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning,2026,Jing Tang,"Jing Tang, Kun Wang, Haolang Lu, Hongjin Chen, KaiTao Chen, Zhongxiang Sun, Qiankun Li, Lingjuan Lyu, Guoshun Nan, Zhigang Zeng",2026-02-16T07:10:44+00:00,2026-02-16T07:10:44+00:00,cs.AI,"Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conflict. Through probing internal representations, we reveal that: (I) Linear Separability: different conflict types are explicitly encoded as linearly separable features rather than entangled; (II) Depth Localization: conflict signals concentrate in mid-to-late layers, indicating a distinct processing stage for conflict encoding; (III) Hierarchical Consistency: aggregating noisy token-level signals along trajectories robustly recovers input-level conflict types; and (IV) Directional Asymmetry: reinforcing the model's implicit source preference under conflict is far easier than enforcing the opposite source. Our findings provide a mechanism-level view of multimodal reasoning under knowledge conflict and enable principled diagnosis and control of long-CoT failures.",https://arxiv.org/pdf/2602.14518v1,http://arxiv.org/abs/2602.14518v1,2026_jing-tang_diagnosing-knowledge-conflict-in-multimodal-long-chain-reaso_2602.145181.pdf,data\raw\papers\2026_jing-tang_diagnosing-knowledge-conflict-in-multimodal-long-chain-reaso_2602.145181.pdf
2602.14492v2,Query as Anchor: Scenario-Adaptive User Representation via Large Language Model,2026,Jiahao Yuan,"Jiahao Yuan, Yike Xu, Jinyong Wen, Baokun Wang, Ziyi Gao, Xiaotong Lin, Yun Liu, Xing Fu, Yu Cheng, Yongchao Liu, Weiqiang Wang, Zhongle Xie",2026-02-16T06:09:31+00:00,2026-02-17T02:44:08+00:00,"cs.CL, cs.IR","Industrial-scale user representation learning requires balancing robust universality with acute task-sensitivity. However, existing paradigms primarily yield static, task-agnostic embeddings that struggle to reconcile the divergent requirements of downstream scenarios within unified vector spaces. Furthermore, heterogeneous multi-source data introduces inherent noise and modality conflicts, degrading representation. We propose Query-as-Anchor, a framework shifting user modeling from static encoding to dynamic, query-aware synthesis. To empower Large Language Models (LLMs) with deep user understanding, we first construct UserU, an industrial-scale pre-training dataset that aligns multi-modal behavioral sequences with user understanding semantics, and our Q-Anchor Embedding architecture integrates hierarchical coarse-to-fine encoders into dual-tower LLMs via joint contrastive-autoregressive optimization for query-aware user representation. To bridge the gap between general pre-training and specialized business logic, we further introduce Cluster-based Soft Prompt Tuning to enforce discriminative latent structures, effectively aligning model attention with scenario-specific modalities. For deployment, anchoring queries at sequence termini enables KV-cache-accelerated inference with negligible incremental latency. Evaluations on 10 Alipay industrial benchmarks show consistent SOTA performance, strong scalability, and efficient deployment. Large-scale online A/B testing in Alipay's production system across two real-world scenarios further validates its practical effectiveness. Our code is prepared for public release and will be available at: https://github.com/JhCircle/Q-Anchor.",https://arxiv.org/pdf/2602.14492v2,http://arxiv.org/abs/2602.14492v2,2026_jiahao-yuan_query-as-anchor-scenario-adaptive-user-representation-via-la_2602.144922.pdf,data\raw\papers\2026_jiahao-yuan_query-as-anchor-scenario-adaptive-user-representation-via-la_2602.144922.pdf
2602.14490v1,Parameter-Efficient Fine-Tuning of LLMs with Mixture of Space Experts,2026,Buze Zhang,"Buze Zhang, Jinkai Tao, Zilang Zeng, Neil He, Ali Maatouk, Menglin Yang, Rex Ying",2026-02-16T06:07:32+00:00,2026-02-16T06:07:32+00:00,"cs.LG, cs.AI, cs.CL, cs.NE","Large Language Models (LLMs) have achieved remarkable progress, with Parameter-Efficient Fine-Tuning (PEFT) emerging as a key technique for downstream task adaptation. However, existing PEFT methods mainly operate in Euclidean space, fundamentally limiting their capacity to capture complex geometric structures inherent in language data. While alternative geometric spaces, like hyperbolic geometries for hierarchical data and spherical manifolds for circular patterns, offer theoretical advantages, forcing representations into a single manifold type ultimately limits expressiveness, even when curvature parameters are learnable. To address this, we propose Mixture of Space (MoS), a unified framework that leverages multiple geometric spaces simultaneously to learn richer, curvature-aware representations. Building on this scheme, we develop MoSLoRA, which extends Low-Rank Adaptation (LoRA) with heterogeneous geometric experts, enabling models to dynamically select or combine appropriate geometric spaces based on input context. Furthermore, to address the computational overhead of frequent manifold switching, we develop a lightweight routing mechanism. Moreover, we provide empirical insights into how curvature optimization impacts training stability and model performance. Our experiments across diverse benchmarks demonstrate that MoSLoRA consistently outperforms strong baselines, achieving up to 5.6% improvement on MATH500 and 15.9% on MAWPS.",https://arxiv.org/pdf/2602.14490v1,http://arxiv.org/abs/2602.14490v1,2026_buze-zhang_parameter-efficient-fine-tuning-of-llms-with-mixture-of-spac_2602.144901.pdf,data\raw\papers\2026_buze-zhang_parameter-efficient-fine-tuning-of-llms-with-mixture-of-spac_2602.144901.pdf
2602.14488v1,BETA-Labeling for Multilingual Dataset Construction in Low-Resource IR,2026,Md. Najib Hasan,"Md. Najib Hasan, Mst. Jannatun Ferdous Rain, Fyad Mohammed, Nazmul Siddique",2026-02-16T06:04:04+00:00,2026-02-16T06:04:04+00:00,"cs.CL, cs.AI","IR in low-resource languages remains limited by the scarcity of high-quality, task-specific annotated datasets. Manual annotation is expensive and difficult to scale, while using large language models (LLMs) as automated annotators introduces concerns about label reliability, bias, and evaluation validity. This work presents a Bangla IR dataset constructed using a BETA-labeling framework involving multiple LLM annotators from diverse model families. The framework incorporates contextual alignment, consistency checks, and majority agreement, followed by human evaluation to verify label quality. Beyond dataset creation, we examine whether IR datasets from other low-resource languages can be effectively reused through one-hop machine translation. Using LLM-based translation across multiple language pairs, we experimented on meaning preservation and task validity between source and translated datasets. Our experiment reveal substantial variation across languages, reflecting language-dependent biases and inconsistent semantic preservation that directly affect the reliability of cross-lingual dataset reuse. Overall, this study highlights both the potential and limitations of LLM-assisted dataset creation for low-resource IR. It provides empirical evidence of the risks associated with cross-lingual dataset reuse and offers practical guidance for constructing more reliable benchmarks and evaluation pipelines in low-resource language settings.",https://arxiv.org/pdf/2602.14488v1,http://arxiv.org/abs/2602.14488v1,2026_md-najib-hasan_beta-labeling-for-multilingual-dataset-construction-in-low-r_2602.144881.pdf,data\raw\papers\2026_md-najib-hasan_beta-labeling-for-multilingual-dataset-construction-in-low-r_2602.144881.pdf
2602.14482v1,TikArt: Aperture-Guided Observation for Fine-Grained Visual Reasoning via Reinforcement Learning,2026,Hao Ding,"Hao Ding, Zhichuan Yang, Weijie Ge, Ziqin Gao, Chaoyi Lu, Lei Zhao",2026-02-16T05:46:47+00:00,2026-02-16T05:46:47+00:00,"cs.CV, cs.AI","We address fine-grained visual reasoning in multimodal large language models (MLLMs), where key evidence may reside in tiny objects, cluttered regions, or subtle markings that are lost under a single global image encoding. We introduce TikArt (Thinking Aperture), an aperture-guided agent that casts multi-step vision-language reasoning as a decision process over regions of interest. TikArt follows a Think-Aperture-Observe loop, alternating between language generation and two aperture actions: Zoom extracts rectangular crops, while Segment invokes SAM2 to obtain mask-based crops for irregular targets. After every action, the model must produce an explicit observation, turning local visual cues into persistent linguistic memory. Built on Qwen3-VL-8B, TikArt optimizes its reasoning policy with AGRPO, a GRPO-style reinforcement learning algorithm with a two-stage curriculum: it warms up segmentation actions and then jointly optimizes visual math, fine-grained VQA, and segmentation, using rewards that couple task success with purposeful aperture use. Experiments on V*, HR-Bench-4K/8K, MME-RealWorld-Lite, MMStar, RefCOCO, and ReasonSeg show consistent gains over the backbone and yield interpretable aperture trajectories for high-resolution reasoning.",https://arxiv.org/pdf/2602.14482v1,http://arxiv.org/abs/2602.14482v1,2026_hao-ding_tikart-aperture-guided-observation-for-fine-grained-visual-r_2602.144821.pdf,data\raw\papers\2026_hao-ding_tikart-aperture-guided-observation-for-fine-grained-visual-r_2602.144821.pdf
2602.14470v1,HyperRAG: Reasoning N-ary Facts over Hypergraphs for Retrieval Augmented Generation,2026,Wen-Sheng Lien,"Wen-Sheng Lien, Yu-Kai Chan, Hao-Lung Hsiao, Bo-Kai Ruan, Meng-Fen Chiang, Chien-An Chen, Yi-Ren Yeh, Hong-Han Shuai",2026-02-16T05:15:55+00:00,2026-02-16T05:15:55+00:00,cs.CL,"Graph-based retrieval-augmented generation (RAG) methods, typically built on knowledge graphs (KGs) with binary relational facts, have shown promise in multi-hop open-domain QA. However, their rigid retrieval schemes and dense similarity search often introduce irrelevant context, increase computational overhead, and limit relational expressiveness. In contrast, n-ary hypergraphs encode higher-order relational facts that capture richer inter-entity dependencies and enable shallower, more efficient reasoning paths. To address this limitation, we propose HyperRAG, a RAG framework tailored for n-ary hypergraphs with two complementary retrieval variants: (i) HyperRetriever learns structural-semantic reasoning over n-ary facts to construct query-conditioned relational chains. It enables accurate factual tracking, adaptive high-order traversal, and interpretable multi-hop reasoning under context constraints. (ii) HyperMemory leverages the LLM's parametric memory to guide beam search, dynamically scoring n-ary facts and entities for query-aware path expansion. Extensive evaluations on WikiTopics (11 closed-domain datasets) and three open-domain QA benchmarks (HotpotQA, MuSiQue, and 2WikiMultiHopQA) validate HyperRAG's effectiveness. HyperRetriever achieves the highest answer accuracy overall, with average gains of 2.95% in MRR and 1.23% in Hits@10 over the strongest baseline. Qualitative analysis further shows that HyperRetriever bridges reasoning gaps through adaptive and interpretable n-ary chain construction, benefiting both open and closed-domain QA.",https://arxiv.org/pdf/2602.14470v1,http://arxiv.org/abs/2602.14470v1,2026_wen-sheng-lien_hyperrag-reasoning-n-ary-facts-over-hypergraphs-for-retrieva_2602.144701.pdf,data\raw\papers\2026_wen-sheng-lien_hyperrag-reasoning-n-ary-facts-over-hypergraphs-for-retrieva_2602.144701.pdf
2602.14466v1,Robust Bias Evaluation with FilBBQ: A Filipino Bias Benchmark for Question-Answering Language Models,2026,Lance Calvin Lim Gamboa,"Lance Calvin Lim Gamboa, Yue Feng, Mark Lee",2026-02-16T05:03:15+00:00,2026-02-16T05:03:15+00:00,cs.CL,"With natural language generation becoming a popular use case for language models, the Bias Benchmark for Question-Answering (BBQ) has grown to be an important benchmark format for evaluating stereotypical associations exhibited by generative models. We expand the linguistic scope of BBQ and construct FilBBQ through a four-phase development process consisting of template categorization, culturally aware translation, new template construction, and prompt generation. These processes resulted in a bias test composed of more than 10,000 prompts which assess whether models demonstrate sexist and homophobic prejudices relevant to the Philippine context. We then apply FilBBQ on models trained in Filipino but do so with a robust evaluation protocol that improves upon the reliability and accuracy of previous BBQ implementations. Specifically, we account for models' response instability by obtaining prompt responses across multiple seeds and averaging the bias scores calculated from these distinctly seeded runs. Our results confirm both the variability of bias scores across different seeds and the presence of sexist and homophobic biases relating to emotion, domesticity, stereotyped queer interests, and polygamy. FilBBQ is available via GitHub.",https://arxiv.org/pdf/2602.14466v1,http://arxiv.org/abs/2602.14466v1,2026_lance-calvin-lim-gamboa_robust-bias-evaluation-with-filbbq-a-filipino-bias-benchmark_2602.144661.pdf,data\raw\papers\2026_lance-calvin-lim-gamboa_robust-bias-evaluation-with-filbbq-a-filipino-bias-benchmark_2602.144661.pdf
2602.14462v1,Silent Inconsistency in Data-Parallel Full Fine-Tuning: Diagnosing Worker-Level Optimization Misalignment,2026,Hong Li,"Hong Li, Zhen Zhou, Honggang Zhang, Yuping Luo, Xinyue Wang, Han Gong, Zhiyuan Liu",2026-02-16T04:42:30+00:00,2026-02-16T04:42:30+00:00,"cs.LG, cs.AI","Data-parallel (DP) training with synchronous all-reduce is a dominant paradigm for full-parameter fine-tuning of large language models (LLMs). While parameter synchronization guarantees numerical equivalence of model weights after each iteration, it does not necessarily imply alignment of worker-level optimization dynamics before gradient aggregation. This paper identifies and studies this latent mismatch, termed \emph{silent inconsistency}, where cross-worker divergence in losses and gradients can remain invisible under conventional aggregated monitoring signals. We propose a lightweight, model-agnostic diagnostic framework that quantifies worker-level consistency using training signals readily available in standard pipelines. Specifically, we introduce three complementary metrics: loss dispersion, gradient-norm dispersion, and gradient-direction consistency measured by inter-worker cosine similarity. The proposed metrics incur negligible overhead and require no modification to model architecture, synchronization mechanisms, or optimization algorithms. We validate the framework by fully fine-tuning the 1B-parameter \texttt{openPangu-Embedded-1B-V1.1} model on the \texttt{tatsu-lab/alpaca} dataset using an 8-NPU DP setup, under controlled perturbations of cross-rank stochasticity. Experimental results show that progressively desynchronized data shuffling and random seeds lead to substantial increases in loss/gradient dispersion and reduced directional alignment, despite smooth globally averaged loss curves. These findings demonstrate that the proposed indicators provide actionable visibility into hidden instability modes in large-scale DP fine-tuning, enabling more reliable diagnosis and configuration assessment.",https://arxiv.org/pdf/2602.14462v1,http://arxiv.org/abs/2602.14462v1,2026_hong-li_silent-inconsistency-in-data-parallel-full-fine-tuning-diagn_2602.144621.pdf,data\raw\papers\2026_hong-li_silent-inconsistency-in-data-parallel-full-fine-tuning-diagn_2602.144621.pdf
2602.14451v1,Precedent-Informed Reasoning: Mitigating Overthinking in Large Reasoning Models via Test-Time Precedent Learning,2026,Qianyue Wang,"Qianyue Wang, Jinwu Hu, Huanxiang Lin, Bolin Chen, Zhiquan Wen, Yaofo Chen, Yu Rong, Mingkui Tan",2026-02-16T04:17:46+00:00,2026-02-16T04:17:46+00:00,"cs.AI, cs.CL","Reasoning in Large Language Models (LLMs) often suffers from inefficient long chain-of-thought traces with redundant self-exploration and validation, which inflate computational costs and even degrade performance. Inspired by human reasoning patterns where people solve new problems by leveraging past related cases to constrain search spaces and reduce trial-and-error, we propose Precedent Informed Reasoning (PIR) transforming LRMs'reasoning paradigm from exhaustive self-exploration to guided learning from precedents. PIR addresses two key challenges: what precedents to adopt and how to utilize them. First, Adaptive Precedent Selection (APS) constructs, for each question and LRM, a compact set of precedents that are both semantically related and informative for the model. It ranks examples by a joint score with semantic similarity and model perplexity, then adapts the amount of precedents to maximize perplexity reduction. Second, Test-time Experience Internalization (TEI) is treated as the test-time learning on precedent-informed instruction, updating lightweight adapters to internalize solution patterns and use them as a prior during subsequent reasoning. Experiments across mathematical reasoning, scientific QA, and code generation demonstrate that PIR consistently shortens reasoning traces while maintaining or improving final accuracy across LLMs, yielding outstanding accuracy-efficiency trade-offs.",https://arxiv.org/pdf/2602.14451v1,http://arxiv.org/abs/2602.14451v1,2026_qianyue-wang_precedent-informed-reasoning-mitigating-overthinking-in-larg_2602.144511.pdf,data\raw\papers\2026_qianyue-wang_precedent-informed-reasoning-mitigating-overthinking-in-larg_2602.144511.pdf
2602.14433v1,Synthetic Reader Panels: Tournament-Based Ideation with LLM Personas for Autonomous Publishing,2026,Fred Zimmerman,Fred Zimmerman,2026-02-16T03:44:54+00:00,2026-02-16T03:44:54+00:00,"cs.CY, cs.AI, cs.CL, cs.HC","We present a system for autonomous book ideation that replaces human focus groups with synthetic reader panels -- diverse collections of LLM-instantiated reader personas that evaluate book concepts through structured tournament competitions. Each persona is defined by demographic attributes (age group, gender, income, education, reading level), behavioral patterns (books per year, genre preferences, discovery methods, price sensitivity), and consistency parameters. Panels are composed per imprint to reflect target demographics, with diversity constraints ensuring representation across age, reading level, and genre affinity. Book concepts compete in single-elimination, double-elimination, round-robin, or Swiss-system tournaments, judged against weighted criteria including market appeal, originality, and execution potential. To reject low-quality LLM evaluations, we implement five automated anti-slop checks (repetitive phrasing, generic framing, circular reasoning, score clustering, audience mismatch). We report results from deployment within a multi-imprint publishing operation managing 6 active imprints and 609 titles in distribution. Three case studies -- a 270-evaluator panel for a children's literacy novel, and two 5-person expert panels for a military memoir and a naval strategy monograph -- demonstrate that synthetic panels produce actionable demographic segmentation, identify structural content issues invisible to homogeneous reviewers, and enable tournament filtering that eliminates low-quality concepts while enriching high-quality survivors from 15% to 62% of the evaluated pool.",https://arxiv.org/pdf/2602.14433v1,http://arxiv.org/abs/2602.14433v1,2026_fred-zimmerman_synthetic-reader-panels-tournament-based-ideation-with-llm-p_2602.144331.pdf,data\raw\papers\2026_fred-zimmerman_synthetic-reader-panels-tournament-based-ideation-with-llm-p_2602.144331.pdf
2602.14428v1,LLM-Guided Knowledge Distillation for Temporal Knowledge Graph Reasoning,2026,Wang Xing,"Wang Xing, Wei Song, Siyu Lin, Chen Wu, Man Wang",2026-02-16T03:27:50+00:00,2026-02-16T03:27:50+00:00,cs.CL,"Temporal knowledge graphs (TKGs) support reasoning over time-evolving facts, yet state-of-the-art models are often computationally heavy and costly to deploy. Existing compression and distillation techniques are largely designed for static graphs; directly applying them to temporal settings may overlook time-dependent interactions and lead to performance degradation. We propose an LLM-assisted distillation framework specifically designed for temporal knowledge graph reasoning. Beyond a conventional high-capacity temporal teacher, we incorporate a large language model as an auxiliary instructor to provide enriched supervision. The LLM supplies broad background knowledge and temporally informed signals, enabling a lightweight student to better model event dynamics without increasing inference-time complexity. Training is conducted by jointly optimizing supervised and distillation objectives, using a staged alignment strategy to progressively integrate guidance from both teachers. Extensive experiments on multiple public TKG benchmarks with diverse backbone architectures demonstrate that the proposed approach consistently improves link prediction performance over strong distillation baselines, while maintaining a compact and efficient student model. The results highlight the potential of large language models as effective teachers for transferring temporal reasoning capability to resource-efficient TKG systems.",https://arxiv.org/pdf/2602.14428v1,http://arxiv.org/abs/2602.14428v1,2026_wang-xing_llm-guided-knowledge-distillation-for-temporal-knowledge-gra_2602.144281.pdf,data\raw\papers\2026_wang-xing_llm-guided-knowledge-distillation-for-temporal-knowledge-gra_2602.144281.pdf
2602.14419v1,WavePhaseNet: A DFT-Based Method for Constructing Semantic Conceptual Hierarchy Structures (SCHS),2026,Kiyotaka Kasubuchi,"Kiyotaka Kasubuchi, Kazuo Fukiya",2026-02-16T03:07:41+00:00,2026-02-16T03:07:41+00:00,cs.CL,"This paper reformulates Transformer/Attention mechanisms in Large Language Models (LLMs) through measure theory and frequency analysis, theoretically demonstrating that hallucination is an inevitable structural limitation. The embedding space functions as a conditional expectation over a σ-algebra, and its failure to be isomorphic to the semantic truth set fundamentally causes logical consistency breakdown. WavePhaseNet Method The authors propose WavePhaseNet, which explicitly constructs a Semantic Conceptual Hierarchy Structure (SCHS) using Discrete Fourier Transform (DFT). By applying DFT along the sequence dimension, semantic information is decomposed into frequency bands: low-frequency components capture global meaning and intent, while high-frequency components represent local syntax and expression. This staged separation enables precise semantic manipulation in diagonalized space. Dimensionality Reduction GPT-4's 24,576-dimensional embedding space exhibits a 1/f spectral structure based on language self-similarity and Zipf's law. Through cumulative energy analysis, the authors derive that approximately 3,000 dimensions constitute the lower bound for ""complete representation."" This demonstrates that reduction from 24,576 to 3,000 dimensions preserves meaning and intent while enabling rigorous reasoning and suppressing hallucination. Cohomological Consistency Control The reduced embedding space, constructed via cohomological regularization over overlapping local windows, allows defining a graph structure and cochain complex. This quantifies inconsistencies among local inferences as coboundary-based losses. Applying harmonic projection based on Hodge theory positions cohomology as a computable regularization principle for controlling semantic consistency, extracting maximally consistent global representations.",https://arxiv.org/pdf/2602.14419v1,http://arxiv.org/abs/2602.14419v1,2026_kiyotaka-kasubuchi_wavephasenet-a-dft-based-method-for-constructing-semantic-co_2602.144191.pdf,data\raw\papers\2026_kiyotaka-kasubuchi_wavephasenet-a-dft-based-method-for-constructing-semantic-co_2602.144191.pdf
2602.14406v1,TruthStance: An Annotated Dataset of Conversations on Truth Social,2026,Fathima Ameen,"Fathima Ameen, Danielle Brown, Manusha Malgareddy, Amanul Haque",2026-02-16T02:25:02+00:00,2026-02-16T02:25:02+00:00,"cs.CL, cs.AI","Argument mining and stance detection are central to understanding how opinions are formed and contested in online discourse. However, most publicly available resources focus on mainstream platforms such as Twitter and Reddit, leaving conversational structure on alt-tech platforms comparatively under-studied. We introduce TruthStance, a large-scale dataset of Truth Social conversation threads spanning 2023-2025, consisting of 24,378 posts and 523,360 comments with reply-tree structure preserved. We provide a human-annotated benchmark of 1,500 instances across argument mining and claim-based stance detection, including inter-annotator agreement, and use it to evaluate large language model (LLM) prompting strategies. Using the best-performing configuration, we release additional LLM-generated labels for 24,352 posts (argument presence) and 107,873 comments (stance to parent), enabling analysis of stance and argumentation patterns across depth, topics, and users. All code and data are released publicly.",https://arxiv.org/pdf/2602.14406v1,http://arxiv.org/abs/2602.14406v1,2026_fathima-ameen_truthstance-an-annotated-dataset-of-conversations-on-truth-s_2602.144061.pdf,data\raw\papers\2026_fathima-ameen_truthstance-an-annotated-dataset-of-conversations-on-truth-s_2602.144061.pdf
2602.14374v1,Differentially Private Retrieval-Augmented Generation,2026,Tingting Tang,"Tingting Tang, James Flemings, Yongqin Wang, Murali Annavaram",2026-02-16T00:52:57+00:00,2026-02-16T00:52:57+00:00,"cs.CR, cs.AI, cs.CL","Retrieval-augmented generation (RAG) is a widely used framework for reducing hallucinations in large language models (LLMs) on domain-specific tasks by retrieving relevant documents from a database to support accurate responses. However, when the database contains sensitive corpora, such as medical records or legal documents, RAG poses serious privacy risks by potentially exposing private information through its outputs. Prior work has demonstrated that one can practically craft adversarial prompts that force an LLM to regurgitate the augmented contexts. A promising direction is to integrate differential privacy (DP), a privacy notion that offers strong formal guarantees, into RAG systems. However, naively applying DP mechanisms into existing systems often leads to significant utility degradation. Particularly for RAG systems, DP can reduce the usefulness of the augmented contexts leading to increase risk of hallucination from the LLMs. Motivated by these challenges, we present DP-KSA, a novel privacy-preserving RAG algorithm that integrates DP using the propose-test-release paradigm. DP-KSA follows from a key observation that most question-answering (QA) queries can be sufficiently answered with a few keywords. Hence, DP-KSA first obtains an ensemble of relevant contexts, each of which will be used to generate a response from an LLM. We utilize these responses to obtain the most frequent keywords in a differentially private manner. Lastly, the keywords are augmented into the prompt for the final output. This approach effectively compresses the semantic space while preserving both utility and privacy. We formally show that DP-KSA provides formal DP guarantees on the generated output with respect to the RAG database. We evaluate DP-KSA on two QA benchmarks using three instruction-tuned LLMs, and our empirical results demonstrate that DP-KSA achieves a strong privacy-utility tradeoff.",https://arxiv.org/pdf/2602.14374v1,http://arxiv.org/abs/2602.14374v1,2026_tingting-tang_differentially-private-retrieval-augmented-generation_2602.143741.pdf,data\raw\papers\2026_tingting-tang_differentially-private-retrieval-augmented-generation_2602.143741.pdf
2602.14367v1,"InnoEval: On Research Idea Evaluation as a Knowledge-Grounded, Multi-Perspective Reasoning Problem",2026,Shuofei Qiao,"Shuofei Qiao, Yunxiang Wei, Xuehai Wang, Bin Wu, Boyang Xue, Ningyu Zhang, Hossein A. Rahmani, Yanshan Wang, Qiang Zhang, Keyan Ding, Jeff Z. Pan, Huajun Chen, Emine Yilmaz",2026-02-16T00:40:31+00:00,2026-02-16T00:40:31+00:00,"cs.CL, cs.AI, cs.IR, cs.LG","The rapid evolution of Large Language Models has catalyzed a surge in scientific idea production, yet this leap has not been accompanied by a matching advance in idea evaluation. The fundamental nature of scientific evaluation needs knowledgeable grounding, collective deliberation, and multi-criteria decision-making. However, existing idea evaluation methods often suffer from narrow knowledge horizons, flattened evaluation dimensions, and the inherent bias in LLM-as-a-Judge. To address these, we regard idea evaluation as a knowledge-grounded, multi-perspective reasoning problem and introduce InnoEval, a deep innovation evaluation framework designed to emulate human-level idea assessment. We apply a heterogeneous deep knowledge search engine that retrieves and grounds dynamic evidence from diverse online sources. We further achieve review consensus with an innovation review board containing reviewers with distinct academic backgrounds, enabling a multi-dimensional decoupled evaluation across multiple metrics. We construct comprehensive datasets derived from authoritative peer-reviewed submissions to benchmark InnoEval. Experiments demonstrate that InnoEval can consistently outperform baselines in point-wise, pair-wise, and group-wise evaluation tasks, exhibiting judgment patterns and consensus highly aligned with human experts.",https://arxiv.org/pdf/2602.14367v1,http://arxiv.org/abs/2602.14367v1,2026_shuofei-qiao_innoeval-on-research-idea-evaluation-as-a-knowledge-grounded_2602.143671.pdf,data\raw\papers\2026_shuofei-qiao_innoeval-on-research-idea-evaluation-as-a-knowledge-grounded_2602.143671.pdf
2602.14279v1,Whom to Query for What: Adaptive Group Elicitation via Multi-Turn LLM Interactions,2026,Ruomeng Ding,"Ruomeng Ding, Tianwei Gao, Thomas P. Zollo, Eitan Bachmat, Richard Zemel, Zhun Deng",2026-02-15T19:05:34+00:00,2026-02-15T19:05:34+00:00,"cs.LG, cs.AI, cs.CL, cs.SI","Eliciting information to reduce uncertainty about latent group-level properties from surveys and other collective assessments requires allocating limited questioning effort under real costs and missing data. Although large language models enable adaptive, multi-turn interactions in natural language, most existing elicitation methods optimize what to ask with a fixed respondent pool, and do not adapt respondent selection or leverage population structure when responses are partial or incomplete. To address this gap, we study adaptive group elicitation, a multi-round setting where an agent adaptively selects both questions and respondents under explicit query and participation budgets. We propose a theoretically grounded framework that combines (i) an LLM-based expected information gain objective for scoring candidate questions with (ii) heterogeneous graph neural network propagation that aggregates observed responses and participant attributes to impute missing responses and guide per-round respondent selection. This closed-loop procedure queries a small, informative subset of individuals while inferring population-level responses via structured similarity. Across three real-world opinion datasets, our method consistently improves population-level response prediction under constrained budgets, including a >12% relative gain on CES at a 10% respondent budget.",https://arxiv.org/pdf/2602.14279v1,http://arxiv.org/abs/2602.14279v1,2026_ruomeng-ding_whom-to-query-for-what-adaptive-group-elicitation-via-multi-_2602.142791.pdf,data\raw\papers\2026_ruomeng-ding_whom-to-query-for-what-adaptive-group-elicitation-via-multi-_2602.142791.pdf
2602.14274v1,Integrating Unstructured Text into Causal Inference: Empirical Evidence from Real Data,2026,Boning Zhou,"Boning Zhou, Ziyu Wang, Han Hong, Haoqi Hu",2026-02-15T18:55:03+00:00,2026-02-15T18:55:03+00:00,"cs.LG, cs.AI","Causal inference, a critical tool for informing business decisions, traditionally relies heavily on structured data. However, in many real-world scenarios, such data can be incomplete or unavailable. This paper presents a framework that leverages transformer-based language models to perform causal inference using unstructured text. We demonstrate the effectiveness of our framework by comparing causal estimates derived from unstructured text against those obtained from structured data across population, group, and individual levels. Our findings show consistent results between the two approaches, validating the potential of unstructured text in causal inference tasks. Our approach extends the applicability of causal inference methods to scenarios where only textual data is available, enabling data-driven business decision-making when structured tabular data is scarce.",https://arxiv.org/pdf/2602.14274v1,http://arxiv.org/abs/2602.14274v1,2026_boning-zhou_integrating-unstructured-text-into-causal-inference-empirica_2602.142741.pdf,data\raw\papers\2026_boning-zhou_integrating-unstructured-text-into-causal-inference-empirica_2602.142741.pdf
2602.14270v1,A Rational Analysis of the Effects of Sycophantic AI,2026,Rafael M. Batista,"Rafael M. Batista, Thomas L. Griffiths",2026-02-15T18:49:19+00:00,2026-02-15T18:49:19+00:00,"cs.CY, cs.AI, cs.HC","People increasingly use large language models (LLMs) to explore ideas, gather information, and make sense of the world. In these interactions, they encounter agents that are overly agreeable. We argue that this sycophancy poses a unique epistemic risk to how individuals come to see the world: unlike hallucinations that introduce falsehoods, sycophancy distorts reality by returning responses that are biased to reinforce existing beliefs. We provide a rational analysis of this phenomenon, showing that when a Bayesian agent is provided with data that are sampled based on a current hypothesis the agent becomes increasingly confident about that hypothesis but does not make any progress towards the truth. We test this prediction using a modified Wason 2-4-6 rule discovery task where participants (N=557) interacted with AI agents providing different types of feedback. Unmodified LLM behavior suppressed discovery and inflated confidence comparably to explicitly sycophantic prompting. By contrast, unbiased sampling from the true distribution yielded discovery rates five times higher. These results reveal how sycophantic AI distorts belief, manufacturing certainty where there should be doubt.",https://arxiv.org/pdf/2602.14270v1,http://arxiv.org/abs/2602.14270v1,2026_rafael-m-batista_a-rational-analysis-of-the-effects-of-sycophantic-ai_2602.142701.pdf,data\raw\papers\2026_rafael-m-batista_a-rational-analysis-of-the-effects-of-sycophantic-ai_2602.142701.pdf
2602.14259v1,Detecting LLM Hallucinations via Embedding Cluster Geometry: A Three-Type Taxonomy with Measurable Signatures,2026,Matic Korun,Matic Korun,2026-02-15T18:14:10+00:00,2026-02-15T18:14:10+00:00,cs.CL,"We propose a geometric taxonomy of large language model hallucinations based on observable signatures in token embedding cluster structure. By analyzing the static embedding spaces of 11 transformer models spanning encoder (BERT, RoBERTa, ELECTRA, DeBERTa, ALBERT, MiniLM, DistilBERT) and decoder (GPT-2) architectures, we identify three operationally distinct hallucination types: Type 1 (center-drift) under weak context, Type 2 (wrong-well convergence) to locally coherent but contextually incorrect cluster regions, and Type 3 (coverage gaps) where no cluster structure exists. We introduce three measurable geometric statistics: α (polarity coupling), \b{eta} (cluster cohesion), and λ_s (radial information gradient). Across all 11 models, polarity structure (α > 0.5) is universal (11/11), cluster cohesion (\b{eta} > 0) is universal (11/11), and the radial information gradient is significant (9/11, p < 0.05). We demonstrate that the two models failing λ_s significance -- ALBERT and MiniLM -- do so for architecturally explicable reasons: factorized embedding compression and distillation-induced isotropy, respectively. These findings establish the geometric prerequisites for type-specific hallucination detection and yield testable predictions about architecture-dependent vulnerability profiles.",https://arxiv.org/pdf/2602.14259v1,http://arxiv.org/abs/2602.14259v1,2026_matic-korun_detecting-llm-hallucinations-via-embedding-cluster-geometry-_2602.142591.pdf,data\raw\papers\2026_matic-korun_detecting-llm-hallucinations-via-embedding-cluster-geometry-_2602.142591.pdf
2602.14224v1,The Interspeech 2026 Audio Reasoning Challenge: Evaluating Reasoning Process Quality for Audio Reasoning Models and Agents,2026,Ziyang Ma,"Ziyang Ma, Ruiyang Xu, Yinghao Ma, Chao-Han Huck Yang, Bohan Li, Jaeyeon Kim, Jin Xu, Jinyu Li, Carlos Busso, Kai Yu, Eng Siong Chng, Xie Chen",2026-02-15T16:38:09+00:00,2026-02-15T16:38:09+00:00,"cs.SD, cs.CL, cs.MM","Recent Large Audio Language Models (LALMs) excel in understanding but often lack transparent reasoning. To address this ""black-box"" limitation, we organized the Audio Reasoning Challenge at Interspeech 2026, the first shared task dedicated to evaluating Chain-of-Thought (CoT) quality in the audio domain. The challenge introduced MMAR-Rubrics, a novel instance-level protocol assessing the factuality and logic of reasoning chains. Featured Single Model and Agent tracks, the competition attracting 156 teams from 18 countries and regions. Results show agent systems currently lead in reasoning quality, utilizing iterative tool orchestration and cross-modal analysis. Besides, single models are rapidly advancing via reinforcement learning and sophisticated data pipeline. We details the challenge design, methodology, and a comprehensive analysis of state-of-the-art systems, providing new insights for explainable audio intelligence.",https://arxiv.org/pdf/2602.14224v1,http://arxiv.org/abs/2602.14224v1,2026_ziyang-ma_the-interspeech-2026-audio-reasoning-challenge-evaluating-re_2602.142241.pdf,data\raw\papers\2026_ziyang-ma_the-interspeech-2026-audio-reasoning-challenge-evaluating-re_2602.142241.pdf
2602.14209v1,MAGE: All-[MASK] Block Already Knows Where to Look in Diffusion LLM,2026,Omin Kwon,"Omin Kwon, Yeonjae Kim, Doyeon Kim, Minseo Kim, Yeonhong Park, Jae W. Lee",2026-02-15T16:07:51+00:00,2026-02-15T16:07:51+00:00,"cs.LG, cs.CL","Block diffusion LLMs are emerging as a promising next paradigm for language generation, but their use of KV caching makes memory access a dominant bottleneck in long-context settings. While dynamic sparse attention has been actively explored, existing methods designed for autoregressive LLMs rely on approximate importance estimation and perform poorly when adapted to block diffusion. This work identifies a key opportunity unique to block diffusion: attention at the first All-[MASK] denoising step reliably predicts important KV entries and budget requirements, enabling MAGE to perform a single exact attention pass per block and reuse it for training-free sparse denoising. Across long-context benchmarks including LongBench and Needle-in-a-Haystack, MAGE achieves near-lossless accuracy with a fraction of the KV budget while delivering up to 3-4x end-to-end speedup, consistently outperforming AR-oriented sparse attention baselines. A lightweight fine-tuning strategy further strengthens [MASK]-guided patterns with minimal cost, requiring only a few hours of training on a single NVIDIA H100 GPU for both 1.5B and 7B models.",https://arxiv.org/pdf/2602.14209v1,http://arxiv.org/abs/2602.14209v1,2026_omin-kwon_mage-all-mask-block-already-knows-where-to-look-in-diffusion_2602.142091.pdf,data\raw\papers\2026_omin-kwon_mage-all-mask-block-already-knows-where-to-look-in-diffusion_2602.142091.pdf
2602.14208v1,"Fast Catch-Up, Late Switching: Optimal Batch Size Scheduling via Functional Scaling Laws",2026,Jinbo Wang,"Jinbo Wang, Binghui Li, Zhanpeng Zhou, Mingze Wang, Yuxuan Sun, Jiaqi Zhang, Xunliang Cai, Lei Wu",2026-02-15T16:06:45+00:00,2026-02-15T16:06:45+00:00,"cs.LG, math.OC, stat.ML","Batch size scheduling (BSS) plays a critical role in large-scale deep learning training, influencing both optimization dynamics and computational efficiency. Yet, its theoretical foundations remain poorly understood. In this work, we show that the functional scaling law (FSL) framework introduced in Li et al. (2025a) provides a principled lens for analyzing BSS. Specifically, we characterize the optimal BSS under a fixed data budget and show that its structure depends sharply on task difficulty. For easy tasks, optimal schedules keep increasing batch size throughout. In contrast, for hard tasks, the optimal schedule maintains small batch sizes for most of training and switches to large batches only in a late stage. To explain the emergence of late switching, we uncover a dynamical mechanism -- the fast catch-up effect -- which also manifests in large language model (LLM) pretraining. After switching from small to large batches, the loss rapidly aligns with the constant large-batch trajectory. Using FSL, we show that this effect stems from rapid forgetting of accumulated gradient noise, with the catch-up speed determined by task difficulty. Crucially, this effect implies that large batches can be safely deferred to late training without sacrificing performance, while substantially reducing data consumption. Finally, extensive LLM pretraining experiments -- covering both Dense and MoE architectures with up to 1.1B parameters and 1T tokens -- validate our theoretical predictions. Across all settings, late-switch schedules consistently outperform constant-batch and early-switch baselines.",https://arxiv.org/pdf/2602.14208v1,http://arxiv.org/abs/2602.14208v1,2026_jinbo-wang_fast-catch-up-late-switching-optimal-batch-size-scheduling-v_2602.142081.pdf,data\raw\papers\2026_jinbo-wang_fast-catch-up-late-switching-optimal-batch-size-scheduling-v_2602.142081.pdf
2602.14201v1,GeoEyes: On-Demand Visual Focusing for Evidence-Grounded Understanding of Ultra-High-Resolution Remote Sensing Imagery,2026,Fengxiang Wang,"Fengxiang Wang, Mingshuo Chen, Yueying Li, Yajie Yang, Yifan Zhang, Long Lan, Xue Yang, Hongda Sun, Yulin Wang, Di Wang, Jun Song, Jing Zhang, Bo Du",2026-02-15T15:50:55+00:00,2026-02-15T15:50:55+00:00,"cs.CV, cs.AI","The ""thinking-with-images"" paradigm enables multimodal large language models (MLLMs) to actively explore visual scenes via zoom-in tools. This is essential for ultra-high-resolution (UHR) remote sensing VQA, where task-relevant cues are sparse and tiny. However, we observe a consistent failure mode in existing zoom-enabled MLLMs: Tool Usage Homogenization, where tool calls collapse into task-agnostic patterns, limiting effective evidence acquisition. To address this, we propose GeoEyes, a staged training framework consisting of (1) a cold-start SFT dataset, UHR Chain-of-Zoom (UHR-CoZ), which covers diverse zooming regimes, and (2) an agentic reinforcement learning method, AdaZoom-GRPO, that explicitly rewards evidence gain and answer improvement during zoom interactions. The resulting model learns on-demand zooming with proper stopping behavior and achieves substantial improvements on UHR remote sensing benchmarks, with 54.23% accuracy on XLRS-Bench.",https://arxiv.org/pdf/2602.14201v1,http://arxiv.org/abs/2602.14201v1,2026_fengxiang-wang_geoeyes-on-demand-visual-focusing-for-evidence-grounded-unde_2602.142011.pdf,data\raw\papers\2026_fengxiang-wang_geoeyes-on-demand-visual-focusing-for-evidence-grounded-unde_2602.142011.pdf
2602.14200v1,TS-Haystack: A Multi-Scale Retrieval Benchmark for Time Series Language Models,2026,Nicolas Zumarraga,"Nicolas Zumarraga, Thomas Kaar, Ning Wang, Maxwell A. Xu, Max Rosenblattl, Markus Kreft, Kevin O'Sullivan, Paul Schmiedmayer, Patrick Langer, Robert Jakob",2026-02-15T15:50:02+00:00,2026-02-15T15:50:02+00:00,cs.LG,"Time Series Language Models (TSLMs) are emerging as unified models for reasoning over continuous signals in natural language. However, long-context retrieval remains a major limitation: existing models are typically trained and evaluated on short sequences, while real-world time-series sensor streams can span millions of datapoints. This mismatch requires precise temporal localization under strict computational constraints, a regime that is not captured by current benchmarks. We introduce TS-Haystack, a long-context temporal retrieval benchmark comprising ten task types across four categories: direct retrieval, temporal reasoning, multi-step reasoning and contextual anomaly. The benchmark uses controlled needle insertion by embedding short activity bouts into longer longitudinal accelerometer recordings, enabling systematic evaluation across context lengths ranging from seconds to 2 hours per sample. We hypothesize that existing TSLM time series encoders overlook temporal granularity as context length increases, creating a task-dependent effect: compression aids classification but impairs retrieval of localized events. Across multiple model and encoding strategies, we observe a consistent divergence between classification and retrieval behavior. Learned latent compression preserves or improves classification accuracy at compression ratios up to 176$\times$, but retrieval performance degrades with context length, incurring in the loss of temporally localized information. These results highlight the importance of architectural designs that decouple sequence length from computational complexity while preserving temporal fidelity.",https://arxiv.org/pdf/2602.14200v1,http://arxiv.org/abs/2602.14200v1,2026_nicolas-zumarraga_ts-haystack-a-multi-scale-retrieval-benchmark-for-time-serie_2602.142001.pdf,data\raw\papers\2026_nicolas-zumarraga_ts-haystack-a-multi-scale-retrieval-benchmark-for-time-serie_2602.142001.pdf
2602.14169v1,Deep Dense Exploration for LLM Reinforcement Learning via Pivot-Driven Resampling,2026,Yiran Guo,"Yiran Guo, Zhongjian Qiao, Yingqi Xie, Jie Liu, Dan Ye, Ruiqing Zhang, Shuang Qiu, Lijie Xu",2026-02-15T14:44:15+00:00,2026-02-15T14:44:15+00:00,"cs.LG, cs.AI, cs.CL","Effective exploration is a key challenge in reinforcement learning for large language models: discovering high-quality trajectories within a limited sampling budget from the vast natural language sequence space. Existing methods face notable limitations: GRPO samples exclusively from the root, saturating high-probability trajectories while leaving deep, error-prone states under-explored. Tree-based methods blindly disperse budgets across trivial or unrecoverable states, causing sampling dilution that fails to uncover rare correct suffixes and destabilizes local baselines. To address this, we propose Deep Dense Exploration (DDE), a strategy that focuses exploration on $\textit{pivots}$-deep, recoverable states within unsuccessful trajectories. We instantiate DDE with DEEP-GRPO, which introduces three key innovations: (1) a lightweight data-driven utility function that automatically balances recoverability and depth bias to identify pivot states; (2) local dense resampling at each pivot to increase the probability of discovering correct subsequent trajectories; and (3) a dual-stream optimization objective that decouples global policy learning from local corrective updates. Experiments on mathematical reasoning benchmarks demonstrate that our method consistently outperforms GRPO, tree-based methods, and other strong baselines.",https://arxiv.org/pdf/2602.14169v1,http://arxiv.org/abs/2602.14169v1,2026_yiran-guo_deep-dense-exploration-for-llm-reinforcement-learning-via-pi_2602.141691.pdf,data\raw\papers\2026_yiran-guo_deep-dense-exploration-for-llm-reinforcement-learning-via-pi_2602.141691.pdf
2602.14158v1,"A Multi-Agent Framework for Medical AI: Leveraging Fine-Tuned GPT, LLaMA, and DeepSeek R1 for Evidence-Based and Bias-Aware Clinical Query Processing",2026,Naeimeh Nourmohammadi,"Naeimeh Nourmohammadi, Md Meem Hossain, The Anh Han, Safina Showkat Ara, Zia Ush Shamszaman",2026-02-15T14:17:27+00:00,2026-02-15T14:17:27+00:00,"cs.CL, cs.AI, cs.MA","Large language models (LLMs) show promise for healthcare question answering, but clinical use is limited by weak verification, insufficient evidence grounding, and unreliable confidence signalling. We propose a multi-agent medical QA framework that combines complementary LLMs with evidence retrieval, uncertainty estimation, and bias checks to improve answer reliability. Our approach has two phases. First, we fine-tune three representative LLM families (GPT, LLaMA, and DeepSeek R1) on MedQuAD-derived medical QA data (20k+ question-answer pairs across multiple NIH domains) and benchmark generation quality. DeepSeek R1 achieves the strongest scores (ROUGE-1 0.536 +- 0.04; ROUGE-2 0.226 +-0.03; BLEU 0.098 -+ 0.018) and substantially outperforms the specialised biomedical baseline BioGPT in zero-shot evaluation. Second, we implement a modular multi-agent pipeline in which a Clinical Reasoning agent (fine-tuned LLaMA) produces structured explanations, an Evidence Retrieval agent queries PubMed to ground responses in recent literature, and a Refinement agent (DeepSeek R1) improves clarity and factual consistency; an optional human validation path is triggered for high-risk or high-uncertainty cases. Safety mechanisms include Monte Carlo dropout and perplexity-based uncertainty scoring, plus lexical and sentiment-based bias detection supported by LIME/SHAP-based analyses. In evaluation, the full system achieves 87% accuracy with relevance around 0.80, and evidence augmentation reduces uncertainty (perplexity 4.13) compared to base responses, with mean end-to-end latency of 36.5 seconds under the reported configuration. Overall, the results indicate that agent specialisation and verification layers can mitigate key single-model limitations and provide a practical, extensible design for evidence-based and bias-aware medical AI.",https://arxiv.org/pdf/2602.14158v1,http://arxiv.org/abs/2602.14158v1,2026_naeimeh-nourmohammadi_a-multi-agent-framework-for-medical-ai-leveraging-fine-tuned_2602.141581.pdf,data\raw\papers\2026_naeimeh-nourmohammadi_a-multi-agent-framework-for-medical-ai-leveraging-fine-tuned_2602.141581.pdf
2602.14143v1,ROAST: Rollout-based On-distribution Activation Steering Technique,2026,Xuanbo Su,"Xuanbo Su, Hao Luo, Yingfang Zhang, Lijun Zhang",2026-02-15T13:30:26+00:00,2026-02-15T13:30:26+00:00,"cs.LG, cs.CL","Activation steering provides parameter-efficient control over large language models (LLMs) at inference time, but many methods rely on off-distribution supervision and discrete masking, leading to brittle interventions. We propose ROAST (Rollout-based On-distribution Activation Steering Technique), which estimates steering directions from the model's own on-distribution rollouts via ROC and avoids hard sparsification via Continuous Soft Scaling (CSS) and Grouped Mean Normalization. Our empirical analysis reveals that while activation magnitude correlates moderately with directional consistency, the variance in magnitude is significant and often disproportionate to semantic quality. This suggests that high-magnitude activations risk dominating the global steering direction if not properly normalized. To address this, ROAST employs grouped normalization to balance contributions across samples, ensuring a more robust estimation of the consensus steering direction. Across models (0.6B to 32B), ROAST consistently improves performance on diverse tasks (e.g., +9.7% on GSM8K for Qwen3-0.6B and +12.1% on TruthfulQA for GLM4-32B), and analyses show that CSS better preserves activation energy.",https://arxiv.org/pdf/2602.14143v1,http://arxiv.org/abs/2602.14143v1,2026_xuanbo-su_roast-rollout-based-on-distribution-activation-steering-tech_2602.141431.pdf,data\raw\papers\2026_xuanbo-su_roast-rollout-based-on-distribution-activation-steering-tech_2602.141431.pdf
2602.14130v1,Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity,2026,Kazuo Yano,"Kazuo Yano, Jonghyeok Lee, Tae Ishitomi, Hironobu Kawaguchi, Akira Koyama, Masakuni Ota, Yuki Ota, Nobuo Sato, Keita Shimada, Sho Takematsu, Ayaka Tobinai, Satomi Tsuji, Kazunori Yanagi, Keiko Yano, Manabu Harada, Yuki Matsuda, Kazunori Matsumoto, Kenichi Matsumura, Hamae Matsuo, Yumi Miyazaki, Kotaro Murai, Tatsuya Ohshita, Marie Seki, Shun Tanoue, Tatsuki Terakado, Yuko Ichimaru, Mirei Saito, Akihiro Otsuka, Koji Ara",2026-02-15T13:02:57+00:00,2026-02-15T13:02:57+00:00,"cs.AI, cs.CL, cs.LG","Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provided with rich context, the space of future generations becomes strongly constrained, and the generation process is effectively governed by near-deterministic dynamics. Recent approaches such as test-time scaling and context adaptation improve performance but do not fundamentally alter this constraint. To address this issue, we propose Algebraic Quantum Intelligence (AQI) as a computational framework that enables systematic expansion of semantic space. AQI is formulated as a noncommutative algebraic structure inspired by quantum theory, allowing properties such as order dependence, interference, and uncertainty to be implemented in a controlled and designable manner. Semantic states are represented as vectors in a Hilbert space, and their evolution is governed by C-values computed from noncommutative operators, thereby ensuring the coexistence and expansion of multiple future semantic possibilities. In this study, we implement AQI by extending a transformer-based LLM with more than 600 specialized operators. We evaluate the resulting system on creative reasoning benchmarks spanning ten domains under an LLM-as-a-judge protocol. The results show that AQI consistently outperforms strong baseline models, yielding statistically significant improvements and reduced cross-domain variance. These findings demonstrate that noncommutative algebraic dynamics can serve as a practical and reproducible foundation for machine creativity. Notably, this architecture has already been deployed in real-world enterprise environments.",https://arxiv.org/pdf/2602.14130v1,http://arxiv.org/abs/2602.14130v1,2026_kazuo-yano_algebraic-quantum-intelligence-a-new-framework-for-reproduci_2602.141301.pdf,data\raw\papers\2026_kazuo-yano_algebraic-quantum-intelligence-a-new-framework-for-reproduci_2602.141301.pdf
2602.14089v1,TabTracer: Monte Carlo Tree Search for Complex Table Reasoning with Large Language Models,2026,Zhizhao Luo,"Zhizhao Luo, Zhaojing Luo, Meihui Zhang, Rui Mao",2026-02-15T10:39:43+00:00,2026-02-15T10:39:43+00:00,"cs.DB, cs.AI","Large language models (LLMs) have emerged as powerful tools for natural language table reasoning, where there are two main categories of methods. Prompt-based approaches rely on language-only inference or one-pass program generation without step-level verification. Agent-based approaches use tools in a closed loop, but verification is often local and backtracking is limited, allowing errors to propagate and increasing cost. Moreover, they rely on chain- or beam-style trajectories that are typically combinatorially redundant, leading to high token costs. In this paper, we propose TabTracer, an agentic framework that coordinates multi-step tool calls over intermediate table states, with explicit state tracking for verification and rollback. First, it enforces step-level verification with typed operations and lightweight numeric and format checks to provide reliable rewards and suppress hallucinations. Second, execution-feedback Monte Carlo Tree Search maintains a search tree of candidate table states and uses backpropagated reflection scores to guide UCB1 selection and rollback via versioned snapshots. Third, it reduces redundancy with budget-aware pruning, deduplication, and state hashing with a monotonicity gate to cut token cost. Comprehensive evaluation on TabFact, WikiTQ, and CRT datasets shows that TabTracer outperforms state-of-the-art baselines by up to 6.7% in accuracy while reducing token consumption by 59--84%.",https://arxiv.org/pdf/2602.14089v1,http://arxiv.org/abs/2602.14089v1,2026_zhizhao-luo_tabtracer-monte-carlo-tree-search-for-complex-table-reasonin_2602.140891.pdf,data\raw\papers\2026_zhizhao-luo_tabtracer-monte-carlo-tree-search-for-complex-table-reasonin_2602.140891.pdf
2602.14080v1,Empty Shelves or Lost Keys? Recall Is the Bottleneck for Parametric Factuality,2026,Nitay Calderon,"Nitay Calderon, Eyal Ben-David, Zorik Gekhman, Eran Ofek, Gal Yona",2026-02-15T10:13:30+00:00,2026-02-15T10:13:30+00:00,"cs.CL, cs.AI","Standard factuality evaluations of LLMs treat all errors alike, obscuring whether failures arise from missing knowledge (empty shelves) or from limited access to encoded facts (lost keys). We propose a behavioral framework that profiles factual knowledge at the level of facts rather than questions, characterizing each fact by whether it is encoded, and then by how accessible it is: cannot be recalled, can be directly recalled, or can only be recalled with inference-time computation (thinking). To support such profiling, we introduce WikiProfile, a new benchmark constructed via an automated pipeline with a prompted LLM grounded in web search. Across 4 million responses from 13 LLMs, we find that encoding is nearly saturated in frontier models on our benchmark, with GPT-5 and Gemini-3 encoding 95--98% of facts. However, recall remains a major bottleneck: many errors previously attributed to missing knowledge instead stem from failures to access it. These failures are systematic and disproportionately affect long-tail facts and reverse questions. Finally, we show that thinking improves recall and can recover a substantial fraction of failures, indicating that future gains may rely less on scaling and more on methods that improve how models utilize what they already encode.",https://arxiv.org/pdf/2602.14080v1,http://arxiv.org/abs/2602.14080v1,2026_nitay-calderon_empty-shelves-or-lost-keys-recall-is-the-bottleneck-for-para_2602.140801.pdf,data\raw\papers\2026_nitay-calderon_empty-shelves-or-lost-keys-recall-is-the-bottleneck-for-para_2602.140801.pdf
2602.14069v1,Open Rubric System: Scaling Reinforcement Learning with Pairwise Adaptive Rubric,2026,Ruipeng Jia,"Ruipeng Jia, Yunyi Yang, Yuxin Wu, Yongbo Gai, Siyuan Tao, Mengyu Zhou, Jianhe Lin, Xiaoxi Jiang, Guanjun Jiang",2026-02-15T09:39:39+00:00,2026-02-15T09:39:39+00:00,cs.CL,"Scalar reward models compress multi-dimensional human preferences into a single opaque score, creating an information bottleneck that often leads to brittleness and reward hacking in open-ended alignment. We argue that robust alignment for non-verifiable tasks is fundamentally a principle generalization problem: reward should not be a learned function internalized into a judge, but an explicit reasoning process executed under inspectable principles. To operationalize this view, we present the Open Rubric System (OpenRS), a plug-and-play, rubrics-based LLM-as-a-Judge framework built around Pairwise Adaptive Meta-Rubrics (PAMR) and lightweight Pointwise Verifiable Rubrics (PVRs), which provide both hard-constraint guardrails and verifiable reward components when ground-truth or programmatic checks are available. OpenRS uses an explicit meta-rubric -- a constitution-like specification that governs how rubrics are instantiated, weighted, and enforced -- and instantiates adaptive rubrics on the fly by conditioning on the semantic differences between two candidate responses. It then performs criterion-wise pairwise comparisons and aggregates criterion-level preferences externally, avoiding pointwise weighted scalarization while improving discriminability in open-ended settings. To keep principles consistent yet editable across various domains, we introduce a two-level meta-rubric refinement pipeline (automated evolutionary refinement for general principles and a reproducible human-in-the-loop procedure for domain principles), complemented with pointwise verifiable rubrics that act as both guardrails against degenerate behaviors and a source of verifiable reward for objective sub-tasks. Finally, we instantiate OpenRS as reward supervision in pairwise RL training.",https://arxiv.org/pdf/2602.14069v1,http://arxiv.org/abs/2602.14069v1,2026_ruipeng-jia_open-rubric-system-scaling-reinforcement-learning-with-pairw_2602.140691.pdf,data\raw\papers\2026_ruipeng-jia_open-rubric-system-scaling-reinforcement-learning-with-pairw_2602.140691.pdf
2602.14044v1,Context Shapes LLMs Retrieval-Augmented Fact-Checking Effectiveness,2026,Pietro Bernardelle,"Pietro Bernardelle, Stefano Civelli, Kevin Roitero, Gianluca Demartini",2026-02-15T08:15:13+00:00,2026-02-15T08:15:13+00:00,cs.CL,"Large language models (LLMs) show strong reasoning abilities across diverse tasks, yet their performance on extended contexts remains inconsistent. While prior research has emphasized mid-context degradation in question answering, this study examines the impact of context in LLM-based fact verification. Using three datasets (HOVER, FEVEROUS, and ClimateFEVER) and five open-source models accross different parameters sizes (7B, 32B and 70B parameters) and model families (Llama-3.1, Qwen2.5 and Qwen3), we evaluate both parametric factual knowledge and the impact of evidence placement across varying context lengths. We find that LLMs exhibit non-trivial parametric knowledge of factual claims and that their verification accuracy generally declines as context length increases. Similarly to what has been shown in previous works, in-context evidence placement plays a critical role with accuracy being consistently higher when relevant evidence appears near the beginning or end of the prompt and lower when placed mid-context. These results underscore the importance of prompt structure in retrieval-augmented fact-checking systems.",https://arxiv.org/pdf/2602.14044v1,http://arxiv.org/abs/2602.14044v1,2026_pietro-bernardelle_context-shapes-llms-retrieval-augmented-fact-checking-effect_2602.140441.pdf,data\raw\papers\2026_pietro-bernardelle_context-shapes-llms-retrieval-augmented-fact-checking-effect_2602.140441.pdf
2602.14043v1,Beyond Static Snapshots: Dynamic Modeling and Forecasting of Group-Level Value Evolution with Large Language Models,2026,Qiankun Pi,"Qiankun Pi, Guixin Su, Jinliang Li, Mayi Xu, Xin Miao, Jiawei Jiang, Ming Zhong, Tieyun Qian",2026-02-15T08:14:55+00:00,2026-02-15T08:14:55+00:00,"cs.SI, cs.AI","Social simulation is critical for mining complex social dynamics and supporting data-driven decision making. LLM-based methods have emerged as powerful tools for this task by leveraging human-like social questionnaire responses to model group behaviors. Existing LLM-based approaches predominantly focus on group-level values at discrete time points, treating them as static snapshots rather than dynamic processes. However, group-level values are not fixed but shaped by long-term social changes. Modeling their dynamics is thus crucial for accurate social evolution prediction--a key challenge in both data mining and social science. This problem remains underexplored due to limited longitudinal data, group heterogeneity, and intricate historical event impacts.   To bridge this gap, we propose a novel framework for group-level dynamic social simulation by integrating historical value trajectories into LLM-based human response modeling. We select China and the U.S. as representative contexts, conducting stratified simulations across four core sociodemographic dimensions (gender, age, education, income). Using the World Values Survey, we construct a multi-wave, group-level longitudinal dataset to capture historical value evolution, and then propose the first event-based prediction method for this task, unifying social events, current value states, and group attributes into a single framework. Evaluations across five LLM families show substantial gains: a maximum 30.88\% improvement on seen questions and 33.97\% on unseen questions over the Vanilla baseline. We further find notable cross-group heterogeneity: U.S. groups are more volatile than Chinese groups, and younger groups in both countries are more sensitive to external changes. These findings advance LLM-based social simulation and provide new insights for social scientists to understand and predict social value changes.",https://arxiv.org/pdf/2602.14043v1,http://arxiv.org/abs/2602.14043v1,2026_qiankun-pi_beyond-static-snapshots-dynamic-modeling-and-forecasting-of-_2602.140431.pdf,data\raw\papers\2026_qiankun-pi_beyond-static-snapshots-dynamic-modeling-and-forecasting-of-_2602.140431.pdf
2602.14035v1,FloCA: Towards Faithful and Logically Consistent Flowchart Reasoning,2026,Jinzi Zou,"Jinzi Zou, Bolin Wang, Liang Li, Shuo Zhang, Nuo Xu, Junzhou Zhao",2026-02-15T07:51:27+00:00,2026-02-15T07:51:27+00:00,cs.AI,"Flowchart-oriented dialogue (FOD) systems aim to guide users through multi-turn decision-making or operational procedures by following a domain-specific flowchart to achieve a task goal. In this work, we formalize flowchart reasoning in FOD as grounding user input to flowchart nodes at each dialogue turn while ensuring node transition is consistent with the correct flowchart path. Despite recent advances of LLMs in task-oriented dialogue systems, adapting them to FOD still faces two limitations: (1) LLMs lack an explicit mechanism to represent and reason over flowchart topology, and (2) they are prone to hallucinations, leading to unfaithful flowchart reasoning. To address these limitations, we propose FloCA, a zero-shot flowchart-oriented conversational agent. FloCA uses an LLM for intent understanding and response generation while delegating flowchart reasoning to an external tool that performs topology-constrained graph execution, ensuring faithful and logically consistent node transitions across dialogue turns. We further introduce an evaluation framework with an LLM-based user simulator and five new metrics covering reasoning accuracy and interaction efficiency. Extensive experiments on FLODIAL and PFDial datasets highlight the bottlenecks of existing LLM-based methods and demonstrate the superiority of FloCA. Our codes are available at https://github.com/Jinzi-Zou/FloCA-flowchart-reasoning.",https://arxiv.org/pdf/2602.14035v1,http://arxiv.org/abs/2602.14035v1,2026_jinzi-zou_floca-towards-faithful-and-logically-consistent-flowchart-re_2602.140351.pdf,data\raw\papers\2026_jinzi-zou_floca-towards-faithful-and-logically-consistent-flowchart-re_2602.140351.pdf
2602.14012v1,From SFT to RL: Demystifying the Post-Training Pipeline for LLM-based Vulnerability Detection,2026,Youpeng Li,"Youpeng Li, Fuxun Yu, Xinda Wang",2026-02-15T06:33:25+00:00,2026-02-15T06:33:25+00:00,"cs.CR, cs.AI, cs.SE","The integration of LLMs into vulnerability detection (VD) has shifted the field toward interpretable and context-aware analysis. While post-training methods have shown promise in general coding tasks, their systematic application to VD remains underexplored. In this paper, we present the first comprehensive investigation into the post-training pipeline for LLM-based VD, spanning from cold-start SFT to off-policy preference optimization and on-policy RL, uncovering how data curation, stage interactions, reward mechanisms, and evaluation protocols collectively dictate the efficacy of model training and assessment. Our study identifies practical guidelines and insights: (1) SFT based on rejection sampling greatly outperforms rationalization-based supervision, which can introduce hallucinations due to ground-truth leakage. (2) While increased SFT epochs constantly benefit preference optimization, excessive SFT inhibits self-exploration during RL, ultimately limiting performance gains. (3) Coarse-grained reward signals often mislead RL, whereas fine-grained root-cause judgments ensure reliable credit assignment. Specification-based rewards offer further benefits but incur significant effort in specification generation. (4) Although filtering extremely hard-to-detect vulnerability samples improves RL training efficiency, the cost of performance loss should be considered in practical applications. (5) Models trained under GRPO significantly outperform those using SFT and preference optimization (i.e., DPO and ORPO), as well as a series of zero-shot SOTA LLMs, underscoring the significant potential of on-policy RL for LLM-based VD. (6) In contrast to binary matching that tends to overestimate performance, LLM-as-a-Judge based on root-cause analysis provides a more robust evaluation protocol, although its accuracy varies across judge models with different levels of security expertise.",https://arxiv.org/pdf/2602.14012v1,http://arxiv.org/abs/2602.14012v1,2026_youpeng-li_from-sft-to-rl-demystifying-the-post-training-pipeline-for-l_2602.140121.pdf,data\raw\papers\2026_youpeng-li_from-sft-to-rl-demystifying-the-post-training-pipeline-for-l_2602.140121.pdf
2602.13980v1,Cognitive Chunking for Soft Prompts: Accelerating Compressor Learning via Block-wise Causal Masking,2026,Guojie Liu,"Guojie Liu, Yiqi Wang, Yanfeng Yang, Wenqi Fan, Songlei Jian, Jianfeng Zhang, Jie Yu",2026-02-15T03:58:13+00:00,2026-02-15T03:58:13+00:00,"cs.AI, cs.LG","Providing extensive context via prompting is vital for leveraging the capabilities of Large Language Models (LLMs). However, lengthy contexts significantly increase inference latency, as the computational cost of self-attention grows quadratically with sequence length. To mitigate this issue, context compression-particularly soft prompt compressio-has emerged as a widely studied solution, which converts long contexts into shorter memory embeddings via a trained compressor. Existing methods typically compress the entire context indiscriminately into a set of memory tokens, requiring the compressor to capture global dependencies and necessitating extensive pre-training data to learn effective patterns. Inspired by the chunking mechanism in human working memory and empirical observations of the spatial specialization of memory embeddings relative to original tokens, we propose Parallelized Iterative Compression (PIC). By simply modifying the Transformer's attention mask, PIC explicitly restricts the receptive field of memory tokens to sequential local chunks, thereby lowering the difficulty of compressor training. Experiments across multiple downstream tasks demonstrate that PIC consistently outperforms competitive baselines, with superiority being particularly pronounced in high compression scenarios (e.g., achieving relative improvements of 29.8\% in F1 score and 40.7\% in EM score on QA tasks at the $64\times$ compression ratio). Furthermore, PIC significantly expedites the training process. Specifically, when training the 16$\times$ compressor, it surpasses the peak performance of the competitive baseline while effectively reducing the training time by approximately 40\%.",https://arxiv.org/pdf/2602.13980v1,http://arxiv.org/abs/2602.13980v1,2026_guojie-liu_cognitive-chunking-for-soft-prompts-accelerating-compressor-_2602.139801.pdf,data\raw\papers\2026_guojie-liu_cognitive-chunking-for-soft-prompts-accelerating-compressor-_2602.139801.pdf
2602.13964v2,HLE-Verified: A Systematic Verification and Structured Revision of Humanity's Last Exam,2026,Weiqi Zhai,"Weiqi Zhai, Zhihai Wang, Jinghang Wang, Boyu Yang, Xiaogang Li, Xiang Xu, Bohan Wang, Peng Wang, Xingzhe Wu, Anfeng Li, Qiyuan Feng, Yuhao Zhou, Shoulin Han, Wenjie Luo, Yiyuan Li, Yaxuan Wang, Ruixian Luo, Guojie Lin, Peiyao Xiao, Chengliang Xu, Ben Wang, Zeyu Wang, Zichao Chen, Jianan Ye, Yijie Hu, Jialong Chen, Zongwen Shen, Yuliang Xu, An Yang, Bowen Yu, Dayiheng Liu, Junyang Lin, Hu Wei, Que Shen, Bing Zhao",2026-02-15T02:50:15+00:00,2026-02-17T12:45:29+00:00,cs.CL,"Humanity's Last Exam (HLE) has become a widely used benchmark for evaluating frontier large language models on challenging, multi-domain questions. However, community-led analyses have raised concerns that HLE contains a non-trivial number of noisy items, which can bias evaluation results and distort cross-model comparisons. To address this challenge, we introduce HLE-Verified, a verified and revised version of HLE with a transparent verification protocol and fine-grained error taxonomy. Our construction follows a two-stage validation-and-repair workflow resulting in a certified benchmark. In Stage I, each item undergoes binary validation of the problem and final answer through domain-expert review and model-based cross-checks, yielding 641 verified items. In Stage II, flawed but fixable items are revised under strict constraints preserving the original evaluation intent, through dual independent expert repairs, model-assisted auditing, and final adjudication, resulting in 1,170 revised-and-certified items. The remaining 689 items are released as a documented uncertain set with explicit uncertainty sources and expertise tags for future refinement. We evaluate seven state-of-the-art language models on HLE and HLE-Verified, observing an average absolute accuracy gain of 7--10 percentage points on HLE-Verified. The improvement is particularly pronounced on items where the original problem statement and/or reference answer is erroneous, with gains of 30--40 percentage points. Our analyses further reveal a strong association between model confidence and the presence of errors in the problem statement or reference answer, supporting the effectiveness of our revisions. Overall, HLE-Verified improves HLE-style evaluations by reducing annotation noise and enabling more faithful measurement of model capabilities. Data is available at: https://github.com/SKYLENAGE-AI/HLE-Verified",https://arxiv.org/pdf/2602.13964v2,http://arxiv.org/abs/2602.13964v2,2026_weiqi-zhai_hle-verified-a-systematic-verification-and-structured-revisi_2602.139642.pdf,data\raw\papers\2026_weiqi-zhai_hle-verified-a-systematic-verification-and-structured-revisi_2602.139642.pdf
